{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PBT_MARL_rock_paper_scissors.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwD3_kI2HDbA",
        "colab_type": "text"
      },
      "source": [
        "#Setup Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyAKAl49kg7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive \n",
        "\n",
        "def run_setup():\n",
        "    drive.mount('/content/gdrive')\n",
        "    %cd \"/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_RockPaperScissorsEnv/\"\n",
        "    !pwd\n",
        "    !ls -l\n",
        "\n",
        "    !pip install tensorflow==2.2.0\n",
        "    !pip install ray[rllib]==0.8.5   \n",
        "\n",
        "    !pip show tensorflow\n",
        "    !pip show ray\n",
        "\n",
        "\n",
        "g_drive_path = \"/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_RockPaperScissorsEnv/\"\n",
        "#run_setup()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BMjPt0fbNSf",
        "colab_type": "text"
      },
      "source": [
        "#Chkpt/restore & log path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQMyQcPpbIai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "local_dir = g_drive_path + \"chkpt/\"\n",
        "chkpt_freq = 10\n",
        "chkpt = 150\n",
        "restore_path = \"{}checkpoint_{}/checkpoint-{}\".format(local_dir, chkpt, chkpt)\n",
        "is_restore = False\n",
        "\n",
        "log_dir = g_drive_path + \"ray_results/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-GBqoxsHBZV",
        "colab_type": "text"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8DRdL7tgKBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "from typing import Dict\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from gym.spaces import Discrete\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.models import ModelCatalog\n",
        "\n",
        "from ray.rllib.policy import Policy\n",
        "\n",
        "from ray.rllib.agents.ppo import ppo\n",
        "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
        "from ray.rllib.agents.ppo import appo\n",
        "from ray.rllib.agents.ppo.appo import APPOTrainer\n",
        "from ray.rllib.agents.ppo import ddppo\n",
        "from ray.rllib.agents.ppo.ddppo import DDPPOTrainer\n",
        "\n",
        "from ray.rllib.env import BaseEnv\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "\n",
        "from ray.rllib.policy.sample_batch import SampleBatch\n",
        "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
        "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
        "\n",
        "from ray.rllib.utils import try_import_tf\n",
        "from ray.tune.logger import pretty_print\n",
        "\n",
        "tf = try_import_tf()\n",
        "\n",
        "ROCK = 0\n",
        "PAPER = 1\n",
        "SCISSORS = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldNcEYhuBIn0",
        "colab_type": "text"
      },
      "source": [
        "# The environment: RockPaperScissorsEnv class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BZtCz51kQw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RockPaperScissorsEnv(MultiAgentEnv):\n",
        "    \"\"\"Two-player environment for rock paper scissors.\n",
        "    The observation is simply the last opponent action.\"\"\"\n",
        "\n",
        "    def __init__(self, _, population_size):\n",
        "        self.population_size = population_size\n",
        "        self.action_space = Discrete(3)\n",
        "        self.observation_space = Discrete(3)\n",
        "        self.player_A = None \n",
        "        self.player_B = None \n",
        "        #self.player_A = \"agt_0\" \n",
        "        #self.player_B = \"agt_1\"  \n",
        "        self.last_move = None\n",
        "        self.num_moves = 0\n",
        "\n",
        "    def reset(self):\n",
        "        g_helper = ray.util.get_actor(\"g_helper\")\n",
        "        agt_i, agt_j = ray.get(g_helper.get_pair.remote())        \n",
        "        self.player_A = agt_i\n",
        "        self.player_B = agt_j\n",
        "        self.last_move = (0, 0)\n",
        "        self.num_moves = 0\n",
        "        return {\n",
        "            self.player_A: self.last_move[1],\n",
        "            self.player_B: self.last_move[0],\n",
        "        }\n",
        "\n",
        "    def step(self, action_dict):\n",
        "        move1 = action_dict[self.player_A]\n",
        "        move2 = action_dict[self.player_B]\n",
        "        self.last_move = (move1, move2)\n",
        "        obs = {\n",
        "            self.player_A: self.last_move[1],\n",
        "            self.player_B: self.last_move[0],\n",
        "        }\n",
        "        \n",
        "        r1, r2 = {\n",
        "            (ROCK, ROCK): (0, 0),\n",
        "            (ROCK, PAPER): (-1, 1),\n",
        "            (ROCK, SCISSORS): (1, -1),\n",
        "            (PAPER, ROCK): (1, -1),\n",
        "            (PAPER, PAPER): (0, 0),\n",
        "            (PAPER, SCISSORS): (-1, 1),\n",
        "            (SCISSORS, ROCK): (-1, 1),\n",
        "            (SCISSORS, PAPER): (1, -1),\n",
        "            (SCISSORS, SCISSORS): (0, 0),\n",
        "        }[move1, move2]\n",
        "        rew = {\n",
        "            self.player_A: r1,\n",
        "            self.player_B: r2,\n",
        "        }\n",
        "        self.num_moves += 1\n",
        "        done = {\n",
        "            \"__all__\": self.num_moves >= 10,\n",
        "        }\n",
        "\n",
        "        #print('obs', obs)\n",
        "\n",
        "        return obs, rew, done, {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSn05vtTBDlu",
        "colab_type": "text"
      },
      "source": [
        "#PBT_MARL class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUC8Ud8X9gOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PBT_MARL:\n",
        "    def __init__(self, population_size, \n",
        "                 K, T_select, \n",
        "                 binomial_n, inherit_prob, \n",
        "                 perturb_prob, perturb_val):\n",
        "        self.population_size = population_size      # num of agents to choose from                \n",
        "        self.K = K      # # step size of Elo rating update given one match result.\n",
        "        self.T_select = T_select      # agt_j selection threshold\n",
        "        # inherit variables\n",
        "        self.binomial_n = binomial_n     # bernoulli is special case of binomial when n=1\n",
        "        self.inherit_prob = inherit_prob     # hyperparameters are either inherited or not independently with probability 0.5        \n",
        "        # mutation variables\n",
        "        self.perturb_prob = perturb_prob     # resample_probability\n",
        "        self.perturb_val = perturb_val      # lower & upper bound for perturbation value          \n",
        "\n",
        "    def _is_eligible(self, agt_i_key):\n",
        "        \"\"\"\n",
        "        If agt_i completed certain training steps > threshold after \n",
        "        last evolution, return true.\n",
        "        \"\"\"\n",
        "        return True\n",
        "\n",
        "    def _is_parent(self, agt_j_key):\n",
        "        \"\"\"\n",
        "        If agt_i completed certain training steps > threshold after \n",
        "        last evolution, return true.\n",
        "        \"\"\"      \n",
        "        return True\n",
        "\n",
        "    def _s_elo(self, rating_i, rating_j):\n",
        "        return 1 / (1 + 10**((rating_j - rating_i) / 400))\n",
        "\n",
        "    def compute_rating(self, prev_rating_i, prev_rating_j, score_i, score_j):\n",
        "        s = (np.sign(score_i - score_j) + 1) / 2\n",
        "        s_elo_val = self._s_elo(prev_rating_i, prev_rating_j)\n",
        "        rating_i = prev_rating_i + self.K * (s - s_elo_val)\n",
        "        rating_j = prev_rating_j + self.K * (s - s_elo_val)\n",
        "\n",
        "        return rating_i, rating_j\n",
        "\n",
        "    def _select_agt_j(self, pol_i_id, population_size, store, T_select):\n",
        "        pol_j_id = np.random.randint(low=0, high=population_size, size=None)\n",
        "        while pol_i_id == pol_j_id:\n",
        "            pol_j_id = np.random.randint(low=0, high=population_size, size=None)\n",
        "\n",
        "        agt_i_key = \"agt_{}\".format(str(pol_i_id))\n",
        "        agt_j_key = \"agt_{}\".format(str(pol_j_id))\n",
        "        rating_i = store[agt_i_key][\"rating\"][-1]\n",
        "        rating_j = store[agt_j_key][\"rating\"][-1]\n",
        "\n",
        "        s_elo_val = self._s_elo(rating_j, rating_i)\n",
        "        print(\"s_elo_val:\", s_elo_val)\n",
        "\n",
        "        if s_elo_val < T_select:\n",
        "            return pol_j_id\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def _inherit(self, trainer, pol_i_id, pol_j_id):\n",
        "        pol_i = \"p_\" + str(pol_i_id)\n",
        "        pol_j = \"p_\" + str(pol_j_id)\n",
        "        print(\"{}_vs_{}\".format(pol_i, pol_j))\n",
        "\n",
        "        # cpy param_j to param_i\n",
        "        self._cp_weight(trainer, pol_j, pol_i)\n",
        "\n",
        "        # inherit hyperparam_j to hyperparam_i\n",
        "        m = np.random.binomial(self.binomial_n, self.inherit_prob, size=1)[0]      # weightage to inherit from agt_i\n",
        "        self._inherit_hyperparameters(trainer, pol_j, pol_i, m)              \n",
        "\n",
        "    def _cp_weight(self, trainer, src, dest):\n",
        "        \"\"\"\n",
        "        Copy weights of source policy to destination policy.\n",
        "        \"\"\"\n",
        "\n",
        "        P0key_P1val = {}\n",
        "        for (k,v), (k2,v2) in zip(trainer.get_policy(dest).get_weights().items(),\n",
        "                                  trainer.get_policy(src).get_weights().items()):\n",
        "            P0key_P1val[k] = v2\n",
        "\n",
        "        trainer.set_weights({dest:P0key_P1val,\n",
        "                             src:trainer.get_policy(src).get_weights()})\n",
        "\n",
        "        for (k,v), (k2,v2) in zip(trainer.get_policy(dest).get_weights().items(),\n",
        "                                  trainer.get_policy(src).get_weights().items()):\n",
        "            assert (v == v2).all()   \n",
        "\n",
        "    def _inherit_hyperparameters(self, trainer, src, dest, m):\n",
        "        src_pol = trainer.get_policy(src)\n",
        "        print(\"src_pol.config['lr']\", src_pol.config[\"lr\"])\n",
        "\n",
        "        dest_pol = trainer.get_policy(dest)\n",
        "        print(\"dest_pol.config['lr']\", dest_pol.config[\"lr\"])\n",
        "\n",
        "        dest_pol.config[\"lr\"] = m * dest_pol.config[\"lr\"] + (1-m) * src_pol.config[\"lr\"]\n",
        "        dest_pol.config[\"gamma\"] = m * dest_pol.config[\"gamma\"] + (1-m) * src_pol.config[\"gamma\"]\n",
        "        print(\"src_pol.config['lr']\", src_pol.config[\"lr\"])\n",
        "        print(\"dest_pol.config['lr']\", dest_pol.config[\"lr\"])      \n",
        "\n",
        "    def _mutate(self, trainer, pol_i_id, store):  \n",
        "        \"\"\"\n",
        "        Don't perturb gamma, just resample when applicable.\n",
        "        \"\"\"\n",
        "        pol_i = \"p_\" + str(pol_i_id)\n",
        "        pol = trainer.get_policy(pol_i)\n",
        "\n",
        "        if random.random() < self.perturb_prob:     # resample\n",
        "            pol.config[\"lr\"] = np.random.uniform(low=0.00001, high=0.1, size=None)\n",
        "            pol.config[\"gamma\"] = np.random.uniform(low=0.9, high=0.999, size=None)     \n",
        "        elif random.random() < 0.5:     # perturb_val = 0.8   \n",
        "            pol.config[\"lr\"] = pol.config[\"lr\"] * self.perturb_val[0]\n",
        "            #pol.config[\"gamma\"] = pol.config[\"gamma\"] * self.perturb_val[0]                     \n",
        "        else:     # perturb_val = 1.2        \n",
        "            pol.config[\"lr\"] = pol.config[\"lr\"] * self.perturb_val[1]\n",
        "            #pol.config[\"gamma\"] = pol.config[\"gamma\"] * self.perturb_val[1]               \n",
        "\n",
        "        # update hyperparameters in storage\n",
        "        key = \"agt_\" + str(pol_i_id)\n",
        "        store[key][\"hyperparameters\"][\"lr\"].append(pol.config[\"lr\"])\n",
        "        store[key][\"hyperparameters\"][\"gamma\"].append(pol.config[\"gamma\"])        \n",
        "\n",
        "    def PBT(self, trainer, store):      \n",
        "        \"\"\"  \n",
        "        For all agents in population, if agt_i is eligible, \n",
        "        select agt_j, (i != j), if agt_j is a parent, \n",
        "        inherit (exploit) & mutate (explore: pertube/resample)    \n",
        "        \"\"\"        \n",
        "        for i in range(self.population_size):\n",
        "            pol_i_id = i\n",
        "            if self._is_eligible(pol_i_id):\n",
        "                pol_j_id = self._select_agt_j(pol_i_id, self.population_size, store, self.T_select)\n",
        "                if pol_j_id is not None:\n",
        "                    if self._is_parent(pol_j_id):\n",
        "                        self._inherit(trainer, pol_i_id, pol_j_id)\n",
        "                        self._mutate(trainer, pol_i_id, store)              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbFNjUZuDtLL",
        "colab_type": "text"
      },
      "source": [
        "#Helper class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIppK-JY71AU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@ray.remote(num_cpus=0.25, num_gpus=0)\n",
        "class Helper:\n",
        "    def __init__(self, population_size, policies):\n",
        "        self.population_size = population_size\n",
        "        self.agt_i, self.agt_j = None, None\n",
        "        self.policies = policies\n",
        "        self.agt_store = self._create_agt_store(population_size, policies)\n",
        "\n",
        "    def set_pair(self):\n",
        "        i, j = np.random.randint(low=0, high=self.population_size, size=2)\n",
        "        while i == j:\n",
        "            j = np.random.randint(low=0, high=self.population_size, size=None)\n",
        "            \n",
        "        self.agt_i = \"agt_\" + str(i)\n",
        "        self.agt_j = \"agt_\" + str(j)\n",
        "        \n",
        "    def get_pair(self):\n",
        "        return self.agt_i, self.agt_j   \n",
        "\n",
        "    def _create_agt_store(self, population_size, policies):\n",
        "        \"\"\"\n",
        "        Storage for stats of agents in the population.\n",
        "        \"\"\"\n",
        "        store = {}\n",
        "        for i in range(0, population_size):\n",
        "            agt_name = \"agt_{}\".format(str(i))\n",
        "            store[agt_name] = {\"hyperparameters\": {\"lr\":[], \n",
        "                                                   \"gamma\":[]}, \n",
        "                               \"score\": [],\n",
        "                               \"rating\": [],\n",
        "                               \"step\": []}      # Steps since last evolved.\n",
        "                                \n",
        "        store = self._init_hyperparameters(store, policies)\n",
        "\n",
        "        return store\n",
        "\n",
        "    def _init_hyperparameters(self, store, policies):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        for key, val in store.items():\n",
        "            _, str_i = key.split(\"_\")\n",
        "            pol_key = \"p_\" + str_i\n",
        "            lr = policies[pol_key][3][\"lr\"]\n",
        "            gamma = policies[pol_key][3][\"gamma\"]\n",
        "            score = 0\n",
        "            #rating = np.random.uniform(low=0.0, high=1.0, size=None)\n",
        "            rating = 0.0\n",
        "            step = 0\n",
        "\n",
        "            store[key][\"hyperparameters\"][\"lr\"].append(lr)\n",
        "            store[key][\"hyperparameters\"][\"gamma\"].append(gamma)\n",
        "            store[key][\"score\"].append(score)\n",
        "            store[key][\"rating\"].append(rating)\n",
        "            store[key][\"step\"].append(step)\n",
        "\n",
        "        return store\n",
        "\n",
        "    def get_agt_store(self):\n",
        "        return self.agt_store         \n",
        "\n",
        "    def update_rating(self, agt_i_key, agt_j_key, rating_i, rating_j, score_i, score_j):\n",
        "        self.agt_store[agt_i_key][\"score\"].append(score_i)\n",
        "        self.agt_store[agt_j_key][\"score\"].append(score_j)\n",
        "        self.agt_store[agt_i_key][\"rating\"].append(rating_i)\n",
        "        self.agt_store[agt_j_key][\"rating\"].append(rating_j)           \n",
        "\n",
        "    def get_rating(self, agt_key):\n",
        "        return self.agt_store[agt_key][\"rating\"]  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B42G5pD1Hoyn",
        "colab_type": "text"
      },
      "source": [
        "#Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwkkAUhXHluq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCallbacks(DefaultCallbacks):\n",
        "    def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                         policies: Dict[str, Policy],\n",
        "                         episode: MultiAgentEpisode, **kwargs):\n",
        "        print(\"on_episode_start {}, _agent_to_policy {}\".format(episode.episode_id, episode._agent_to_policy))            \n",
        "        episode.hist_data[\"episode_id\"] = []        \n",
        "\n",
        "    def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                        episode: MultiAgentEpisode, **kwargs):\n",
        "          \"\"\"\n",
        "          pole_angle = abs(episode.last_observation_for()[2])\n",
        "          raw_angle = abs(episode.last_raw_obs_for()[2])\n",
        "          assert pole_angle == raw_angle\n",
        "          episode.user_data[\"pole_angles\"].append(pole_angle)\n",
        "          \"\"\"\n",
        "          pass\n",
        "\n",
        "    def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
        "                       **kwargs):        \n",
        "        print(\"on_episode_end {}, episode.agent_rewards {}\".format(episode.episode_id, episode.agent_rewards))            \n",
        "        \n",
        "        player_policy = []\n",
        "        score = []\n",
        "        for k,v in episode.agent_rewards.items():\n",
        "            player_policy.append(k)\n",
        "            score.append(v)\n",
        "\n",
        "        pol_i_key = player_policy[0][1]\n",
        "        pol_j_key = player_policy[1][1]\n",
        "        _, str_i = pol_i_key.split(\"_\")\n",
        "        _, str_j = pol_j_key.split(\"_\")\n",
        "        agt_i_key = \"agt_\" + str_i\n",
        "        agt_j_key = \"agt_\" + str_j\n",
        "                \n",
        "        g_helper = ray.util.get_actor(\"g_helper\")     # get global object\n",
        "        prev_rating_i = ray.get(g_helper.get_rating.remote(agt_i_key))\n",
        "        prev_rating_j = ray.get(g_helper.get_rating.remote(agt_j_key))\n",
        "        score_i = score[0]\n",
        "        score_j = score[1]\n",
        "        rating_i, rating_j = l_PBT_MARL.compute_rating(prev_rating_i[0], prev_rating_j[0], score_i, score_j)    \n",
        "        ray.get(g_helper.update_rating.remote(agt_i_key, agt_j_key, rating_i, rating_j, score_i, score_j))          \n",
        "        print(\"on_episode_end ray.get(g_helper.get_agt_store.remote())\", ray.get(g_helper.get_agt_store.remote()))\n",
        "        \n",
        "    def on_sample_end(self, worker: RolloutWorker, samples: SampleBatch, \n",
        "                      **kwargs):\n",
        "        print(\"on_sample_end returned sample batch of size {}\".format(samples.count))\n",
        "\n",
        "    def on_train_result(self, trainer, result: dict, **kwargs):\n",
        "        \"\"\"\n",
        "        #print(\"on_train_result result['hist_stats']\", result[\"hist_stats\"])\n",
        "\n",
        "        print(\"on_train_result result['episodes_this_iter']\", result[\"episodes_this_iter\"])\n",
        "        print(\"on_train_result result['optimizer_steps_this_iter']\", result[\"optimizer_steps_this_iter\"])\n",
        "        print(\"on_train_result result['timesteps_this_iter']\", result[\"timesteps_this_iter\"])\n",
        "        #print(\"on_train_result result['done']\", result[\"done\"])\n",
        "        print(\"on_train_result result['timesteps_total']\", result[\"timesteps_total\"])\n",
        "        print(\"on_train_result result['episodes_total']\", result[\"episodes_total\"])\n",
        "        print(\"on_train_result result['training_iteration']\", result[\"training_iteration\"])\n",
        "\n",
        "        print(\"on_train_result result['info']\", result[\"info\"])        \n",
        "        print(\"on_train_result result['info']['num_steps_trained']\", result[\"info\"][\"num_steps_trained\"])\n",
        "        print(\"on_train_result result['info']['num_steps_sampled']\", result[\"info\"][\"num_steps_sampled\"])\n",
        "        \"\"\"\n",
        "        print(\"trainer.train() result: {} -> {} episodes\".format(trainer, result[\"episodes_this_iter\"]))\n",
        "        # you can mutate the result dict to add new fields to return\n",
        "        result[\"callback_ok\"] = True\n",
        "        print(\"on_train_result result\", result)\n",
        "\n",
        "        \"\"\"\n",
        "        lastest_eps_id = result[\"hist_stats\"][\"episode_id\"][result[\"episodes_this_iter\"]-1]\n",
        "        print(\"on_train_result lastest_eps_id\", lastest_eps_id)\n",
        "        # print newest eps_id in this iter (10 env with 1 worker will have 10 eps in 1 iter)\n",
        "        for i, eps_id in enumerate(result[\"hist_stats\"][\"episode_id\"]):\n",
        "            print(\"on_train_result eps_id\", eps_id)\n",
        "            if i == result[\"episodes_this_iter\"]-1:\n",
        "                break      \n",
        "        \"\"\"\n",
        "\n",
        "        g_helper = ray.util.get_actor(\"g_helper\")     # get global object\n",
        "        agt_store = ray.get(g_helper.get_agt_store.remote())\n",
        "        l_PBT_MARL.PBT(trainer, agt_store)     # perform PBT\n",
        "        print(\"on_train_result agt_store\", agt_store)        \n",
        "        ray.get(g_helper.set_pair.remote())     # set the lastest pair\n",
        "        print(\"on_train_result g_helper.get_pair.remote()\", ray.get(g_helper.get_pair.remote()))      \n",
        "\n",
        "    def on_postprocess_trajectory(\n",
        "            self, worker: RolloutWorker, episode: MultiAgentEpisode,\n",
        "            agent_id: str, policy_id: str, policies: Dict[str, Policy],\n",
        "            postprocessed_batch: SampleBatch,\n",
        "            original_batches: Dict[str, SampleBatch], **kwargs):\n",
        "        print(\"postprocessed {}, {}, {}, {} steps\".format(episode, agent_id, policy_id, postprocessed_batch.count))\n",
        "        \"\"\"\n",
        "        if \"num_batches\" not in episode.custom_metrics:\n",
        "            episode.custom_metrics[\"num_batches\"] = 0\n",
        "        episode.custom_metrics[\"num_batches\"] += 1\n",
        "        \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpcJGyAaBbc2",
        "colab_type": "text"
      },
      "source": [
        "#Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMZ20pVCzxUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_policies(population_size, obs_space, act_space, use_lstm, hyperparameters_range):\n",
        "    \"\"\"\n",
        "    Sample hyper-parameter from the hyper-parameter distribution.\n",
        "    \"\"\"\n",
        "    policies = {}\n",
        "    for i in range(population_size):\n",
        "        pol_key = \"p_\" + str(i)\n",
        "        lr = np.random.uniform(low=hyperparameters_range[\"lr\"][0], high=hyperparameters_range[\"lr\"][1], size=None)\n",
        "        gamma = np.random.uniform(low=hyperparameters_range[\"gamma\"][0], high=hyperparameters_range[\"gamma\"][1], size=None)\n",
        "        policies[pol_key] = (None, obs_space, act_space, {\"model\": {\"use_lstm\": use_lstm},\n",
        "                                                          \"lr\": lr,\n",
        "                                                          \"gamma\": gamma})\n",
        "    return policies\n",
        "\n",
        "def train_policies(population_size):    \n",
        "    train_policies = []\n",
        "    for i in range(population_size):\n",
        "        pol_key = \"p_\" + str(i)\n",
        "        train_policies.append(pol_key)\n",
        "\n",
        "    return policies\n",
        "\n",
        "def select_policy(agent_id):\n",
        "    _, i = agent_id.split(\"_\")\n",
        "    policy = \"p_\" + str(i)\n",
        "    print(\"select_policy {} {}\".format(agent_id , policy))\n",
        "    return policy     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAEySBSfBS5u",
        "colab_type": "text"
      },
      "source": [
        "#Global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trdlnMoHwbfT",
        "colab_type": "code",
        "outputId": "a5727c75-e13c-4e9c-a887-8e3e81659559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "population_size = 6\n",
        "K = 0.1     \n",
        "T_select = 0.6 #0.47\n",
        "binomial_n = 1\n",
        "inherit_prob = 0.5\n",
        "perturb_prob = 0.1\n",
        "perturb_val = [0.8, 1.2]\n",
        "hyperparameters_range = {\"lr\": [0.00001, 0.01], \n",
        "                         \"gamma\": [0.9, 0.999]}\n",
        "\n",
        "register_env(\"RockPaperScissorsEnv\", lambda _: RockPaperScissorsEnv(_, population_size))     # register RockPaperScissorsEnv with RLlib     \n",
        "# get obs & act spaces from dummy CDA env\n",
        "dummy_env = RockPaperScissorsEnv(_, population_size=0)\n",
        "obs_space = dummy_env.observation_space\n",
        "act_space = dummy_env.action_space\n",
        "\n",
        "use_lstm=False\n",
        "policies = init_policies(population_size, obs_space, act_space, use_lstm, hyperparameters_range)\n",
        "train_policies = train_policies(population_size)\n",
        "\n",
        "l_PBT_MARL = PBT_MARL(population_size, \n",
        "                      K, T_select, \n",
        "                      binomial_n, inherit_prob,\n",
        "                      perturb_prob, perturb_val)\n",
        "\n",
        "ray.shutdown()\n",
        "#ray.init(ignore_reinit_error=True, log_to_driver=True, webui_host='127.0.0.1', num_cpus=2)      #start ray\n",
        "ray.init(ignore_reinit_error=True, log_to_driver=True, webui_host='127.0.0.1', num_cpus=2, num_gpus=1)      #start ray\n",
        "print(\"ray.nodes()\", ray.nodes())\n",
        "\n",
        "g_helper = Helper.options(name=\"g_helper\").remote(population_size, policies) \n",
        "ray.get(g_helper.set_pair.remote())\n",
        "\n",
        "num_iters = 5     # num of main training loop"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-10 04:01:21,167\tINFO resource_spec.py:212 -- Starting Ray with 7.13 GiB memory available for workers and up to 3.58 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
            "2020-06-10 04:01:21,580\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32m127.0.0.1:8265\u001b[39m\u001b[22m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ray.nodes() [{'NodeID': '921753200830eebae7bcc7f8461a0e8a90e4bf19', 'Alive': True, 'NodeManagerAddress': '172.28.0.2', 'NodeManagerHostname': '685afc1c1019', 'NodeManagerPort': 57387, 'ObjectManagerPort': 45665, 'ObjectStoreSocketName': '/tmp/ray/session_2020-06-10_04-01-21_166152_4535/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2020-06-10_04-01-21_166152_4535/sockets/raylet', 'Resources': {'node:172.28.0.2': 1.0, 'GPU': 1.0, 'object_store_memory': 50.0, 'memory': 146.0, 'CPU': 2.0}, 'alive': True}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyucHLoqBe5G",
        "colab_type": "text"
      },
      "source": [
        "#Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNWNnavQt9y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_config():\n",
        "    #config = ppo.DEFAULT_CONFIG.copy()\n",
        "    #config = appo.DEFAULT_CONFIG.copy()\n",
        "    config = ddppo.DEFAULT_CONFIG.copy()\n",
        "\n",
        "    config[\"env\"] = RockPaperScissorsEnv\n",
        "    config[\"multiagent\"] = {\"policies_to_train\": train_policies,\n",
        "                            \"policies\": policies,\n",
        "                            \"policy_mapping_fn\": select_policy}        \n",
        "    # Number of CPUs to allocate for the trainer. Note: this only takes effect\n",
        "    # when running in Tune. Otherwise, the trainer runs in the main program.\n",
        "    #config[\"num_cpus_for_driver\"] = 1                          \n",
        "    #config[\"num_gpus\"] = 0.5                                   # trainer only, can be fractional\n",
        "    config[\"num_cpus_per_worker\"] = 0.25                          # if using tune in colab, use 0.5                   \n",
        "    config[\"num_gpus_per_worker\"] = 0.125\n",
        "    config[\"num_workers\"] = 2      \n",
        "    \"\"\"\n",
        "    \"num_envs_per_worker\" vectorized envs\n",
        "\n",
        "    https://docs.ray.io/en/master/rllib-env.html#vectorized\n",
        "\n",
        "    your envs will still be stepped one at a time. \n",
        "    If you would like your envs to be stepped in parallel, \n",
        "    you can set \"remote_worker_envs\": True. \n",
        "    This will create env instances in Ray actors and \n",
        "    step them in parallel. \n",
        "    These remote processes introduce communication overheads, \n",
        "    so this only helps if your env is very expensive to step / reset.\n",
        "    \"\"\"\n",
        "    config[\"num_envs_per_worker\"] = 3\n",
        "    #config[\"remote_worker_envs\"] = False       \n",
        "    #config[\"batch_mode\"] = \"complete_episodes\"              # \"complete_episodes\" or \"truncate_episodes\"    \n",
        "    config[\"rollout_fragment_length\"] = 30                  # let's sample 10 steps per episode which is the same as batch_mode=\"complete_episodes\"\n",
        "    config[\"train_batch_size\"] = -1                         # Training batch size, if applicable \n",
        "                                                            # = total rollout steps of all workers with all envs if this total rollout steps > train_batch_size . \n",
        "                                                            # Should be >= rollout_fragment_length.\n",
        "                                                            # Samples batches will be concatenated together to a batch of this size,\n",
        "                                                            # which is then passed to SGD.\n",
        "                                                            # If batch_mode is \"complete_episodes\", \n",
        "    config[\"sgd_minibatch_size\"] = 10                       # default=128, sgd_minibatch_size, must be <= train_batch_size. (not in appo config)\n",
        "    config[\"num_sgd_iter\"] = 3                              # default=30, number of epochs to execute per train batch.\n",
        "\n",
        "    \"\"\"\n",
        "    # IMPALA config for APPO\n",
        "    config[\"min_iter_time_s\"] = 3 #10\n",
        "    config[\"replay_buffer_num_slots\"] = 10 #100      \n",
        "    config[\"learner_queue_size\"] = 10 #16      \n",
        "    config[\"learner_queue_timeout\"] = 30 #300      \n",
        "    \"\"\"\n",
        "\n",
        "    config[\"callbacks\"] = MyCallbacks\n",
        "    config[\"log_level\"] = \"WARN\"                            # WARN/INFO/DEBUG \n",
        "    config[\"output\"] = log_dir\n",
        "\n",
        "    return config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb_4cEGdBlqf",
        "colab_type": "text"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUB40TYSuDAn",
        "colab_type": "code",
        "outputId": "1d52e628-0790-4ccf-9b85-65d89325b26c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def go_train(config):     \n",
        "    #trainer = ppo.PPOTrainer(config=config, env=\"RockPaperScissorsEnv\")         \n",
        "    #trainer = appo.APPOTrainer(config=config, env=\"RockPaperScissorsEnv\")         \n",
        "    trainer = ddppo.DDPPOTrainer(config=config, env=\"RockPaperScissorsEnv\")         \n",
        "\n",
        "    if is_restore == True:\n",
        "        trainer.restore(restore_path) \n",
        "    \n",
        "    result = None\n",
        "    for i in range(num_iters):\n",
        "        result = trainer.train()       \n",
        "        print(\"training loop = {} of {}\".format(i + 1, num_iters))            \n",
        "        print(pretty_print(result))     # includes result[\"custom_metrics\"]\n",
        "    \"\"\"\n",
        "        if i % chkpt_freq == 0:\n",
        "            checkpoint = trainer.save(local_dir)\n",
        "            print(\"checkpoint saved at\", checkpoint)\n",
        "    \n",
        "    checkpoint = trainer.save(local_dir)\n",
        "    print(\"checkpoint saved at\", checkpoint)\n",
        "    \"\"\"\n",
        "\n",
        "# run everything\n",
        "go_train(get_config())    \n",
        "\n",
        "ray.shutdown()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-10 04:01:24,079\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
            "2020-06-10 04:01:24,124\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "2020-06-10 04:01:36,322\tINFO trainable.py:180 -- _setup took 12.200 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2020-06-10 04:01:36,323\tINFO trainable.py:217 -- Getting current IP.\n",
            "2020-06-10 04:01:36,325\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1574408856, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 221929406, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 651246668, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m /pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m /pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 472991024, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 642268686, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 812934918, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadd3c7b630>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadd3c7b630>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1574408856, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): -2.0, ('agt_2', 'p_2'): 2.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0], 'rating': [0.0, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0], 'rating': [0.0, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1090202846, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadcadb0f98>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadcadb0f98>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 221929406, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): -2.0, ('agt_2', 'p_2'): 2.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0], 'rating': [0.0, -0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0], 'rating': [0.0, -0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 630383423, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadcadb9588>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadcadb9588>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 651246668, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): -1.0, ('agt_2', 'p_2'): 1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf1160940>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf1160940>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 472991024, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 2.0, ('agt_2', 'p_2'): -2.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0], 'rating': [0.0, -0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0], 'rating': [0.0, -0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 903384626, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf1160fd0>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf1160fd0>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 642268686, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): -1.0, ('agt_2', 'p_2'): 1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1988114070, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadcadb9e10>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadcadb9e10>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1090202846, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 2.0, ('agt_2', 'p_2'): -2.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1262240320, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf1169668>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf1169668>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 812934918, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): -2.0, ('agt_2', 'p_2'): 2.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 278824908, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 392585076, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadd3c7b630>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadd3c7b630>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 630383423, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 0.0, ('agt_2', 'p_2'): 0.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1064058479, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadcadb0f98>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadcadb0f98>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1988114070, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 2.0, ('agt_2', 'p_2'): -2.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1191099053, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf11195f8>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf11195f8>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 903384626, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 3.0, ('agt_2', 'p_2'): -3.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 847557659, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce82d6f98>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce82d6f98>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1262240320, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 1.0, ('agt_2', 'p_2'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1791651038, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf1169c50>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf1169c50>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 278824908, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 5.0, ('agt_2', 'p_2'): -5.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadcadb9588>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadcadb9588>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 392585076, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 1.0, ('agt_2', 'p_2'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 249742659, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca560a20>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca560a20>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1064058479, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 1.0, ('agt_2', 'p_2'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 227172298, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadcadb9b00>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadcadb9b00>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1191099053, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): -2.0, ('agt_2', 'p_2'): 2.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1808884822, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_sample_end returned sample batch of size 90\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1681364063, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8299550>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8299550>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 847557659, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): -1.0, ('agt_2', 'p_2'): 1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1135784751, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf11195f8>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf11195f8>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1791651038, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 0.0, ('agt_2', 'p_2'): 0.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 544672254, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce82d6f98>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce82d6f98>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1681364063, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 1.0, ('agt_2', 'p_2'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 592583677, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_sample_end returned sample batch of size 90\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.DDPPO object at 0x7f87490afe10> -> 18 episodes\n",
            "on_train_result result {'episode_reward_max': 0.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 0.0, 'episode_len_mean': 10.0, 'episodes_this_iter': 18, 'policy_reward_min': {'p_0': -2.0, 'p_2': -5.0}, 'policy_reward_max': {'p_0': 5.0, 'p_2': 2.0}, 'policy_reward_mean': {'p_0': 0.3888888888888889, 'p_2': -0.3888888888888889}, 'custom_metrics': {}, 'hist_stats': {'episode_id': [], 'episode_reward': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'episode_lengths': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], 'policy_p_0_reward': [-2.0, -2.0, -1.0, 2.0, 0.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 3.0, 1.0, 5.0, -1.0, 0.0, 1.0], 'policy_p_2_reward': [2.0, 2.0, 1.0, -2.0, 0.0, -2.0, -1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -3.0, -1.0, -5.0, 1.0, 0.0, -1.0]}, 'sampler_perf': {'mean_env_wait_ms': 0.2979155509702621, 'mean_processing_ms': 4.346101514754758, 'mean_inference_ms': 3.601916374698762}, 'off_policy_estimator': {}, 'info': {'num_steps_trained': 180, 'num_steps_sampled': 180, 'sync_weights_up_time': 7.858, 'sync_weights_down_time': 14.673, 'learn_time_ms': 1065.013, 'learner': {'p_0': {'allreduce_latency': 0.005406962500678169, 'cur_kl_coeff': 0.2, 'cur_lr': 0.008907753822295032, 'total_loss': 2.1257931192715964, 'policy_loss': 0.03556920008526908, 'vf_loss': 2.0421192116207547, 'vf_explained_var': -0.2929503, 'kl': 0.24052344924873775, 'entropy': 0.8933159775204129, 'entropy_coeff': 0.0}, 'p_2': {'allreduce_latency': 0.0043370723724365234, 'cur_kl_coeff': 0.2, 'cur_lr': 0.008755722472524574, 'total_loss': 2.7047385242250233, 'policy_loss': 0.13325407707856762, 'vf_loss': 2.5121324724621243, 'vf_explained_var': -0.5284648, 'kl': 0.29675977759891087, 'entropy': 0.8567022085189819, 'entropy_coeff': 0.0}}}, 'optimizer_steps_this_iter': 1, 'timesteps_this_iter': 180, 'done': False, 'timesteps_total': 180, 'episodes_total': 18, 'training_iteration': 1, 'experiment_id': '9eabf5102e0a4e61bedb232b4a8ed663', 'date': '2020-06-10_04-01-37', 'timestamp': 1591761697, 'time_this_iter_s': 1.0912535190582275, 'time_total_s': 1.0912535190582275, 'pid': 4535, 'hostname': '685afc1c1019', 'node_ip': '172.28.0.2', 'config': {'num_workers': 2, 'num_envs_per_worker': 3, 'rollout_fragment_length': 30, 'sample_batch_size': -1, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 90, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_action_dist': None, 'custom_options': {}, 'custom_preprocessor': None}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env_config': {}, 'env': 'RockPaperScissorsEnv', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 5e-05, 'monitor': False, 'log_level': 'WARN', 'callbacks': <class '__main__.MyCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'use_pytorch': True, 'eager': False, 'eager_tracing': False, 'no_eager_on_workers': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'use_exec_api': False, 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_cpus_per_worker': 0.25, 'num_gpus_per_worker': 0.125, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': '/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_RockPaperScissorsEnv/ray_results/', 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'p_0': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008907753822295032, 'gamma': 0.9766025199359999}), 'p_1': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009876705293843066, 'gamma': 0.9969770735748194}), 'p_2': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008755722472524574, 'gamma': 0.9562289739239331}), 'p_3': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009351876078460924, 'gamma': 0.9650823105855924}), 'p_4': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.006340266341324486, 'gamma': 0.9597670369809892}), 'p_5': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.0006377526143306553, 'gamma': 0.9419988557225905})}, 'policy_mapping_fn': <function select_policy at 0x7f87490aeae8>, 'policies_to_train': {'p_0': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008907753822295032, 'gamma': 0.9766025199359999}), 'p_1': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009876705293843066, 'gamma': 0.9969770735748194}), 'p_2': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008755722472524574, 'gamma': 0.9562289739239331}), 'p_3': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009351876078460924, 'gamma': 0.9650823105855924}), 'p_4': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.006340266341324486, 'gamma': 0.9597670369809892}), 'p_5': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.0006377526143306553, 'gamma': 0.9419988557225905})}}, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 10, 'shuffle_sequences': True, 'num_sgd_iter': 3, 'lr_schedule': None, 'vf_share_layers': False, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'simple_optimizer': True, '_fake_gpus': False, 'truncate_episodes': True}, 'time_since_restore': 1.0912535190582275, 'timesteps_since_restore': 180, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': 85.15, 'ram_util_percent': 46.650000000000006}, 'callback_ok': True}\n",
            "s_elo_val: 0.4999280442163407\n",
            "p_0_vs_p_1\n",
            "src_pol.config['lr'] 0.009876705293843066\n",
            "dest_pol.config['lr'] 0.008907753822295032\n",
            "src_pol.config['lr'] 0.009876705293843066\n",
            "dest_pol.config['lr'] 0.009876705293843066\n",
            "s_elo_val: 0.5000719557836594\n",
            "p_1_vs_p_2\n",
            "src_pol.config['lr'] 0.008755722472524574\n",
            "dest_pol.config['lr'] 0.009876705293843066\n",
            "src_pol.config['lr'] 0.008755722472524574\n",
            "dest_pol.config['lr'] 0.009876705293843066\n",
            "s_elo_val: 0.5\n",
            "p_2_vs_p_0\n",
            "src_pol.config['lr'] 0.007901364235074452\n",
            "dest_pol.config['lr'] 0.008755722472524574\n",
            "src_pol.config['lr'] 0.007901364235074452\n",
            "dest_pol.config['lr'] 0.008755722472524574\n",
            "s_elo_val: 0.5\n",
            "p_3_vs_p_4\n",
            "src_pol.config['lr'] 0.006340266341324486\n",
            "dest_pol.config['lr'] 0.009351876078460924\n",
            "src_pol.config['lr'] 0.006340266341324486\n",
            "dest_pol.config['lr'] 0.009351876078460924\n",
            "s_elo_val: 0.5\n",
            "p_4_vs_p_5\n",
            "src_pol.config['lr'] 0.0006377526143306553\n",
            "dest_pol.config['lr'] 0.006340266341324486\n",
            "src_pol.config['lr'] 0.0006377526143306553\n",
            "dest_pol.config['lr'] 0.0006377526143306553\n",
            "s_elo_val: 0.5000719557836594\n",
            "p_5_vs_p_2\n",
            "src_pol.config['lr'] 0.010506866967029488\n",
            "dest_pol.config['lr'] 0.0006377526143306553\n",
            "src_pol.config['lr'] 0.010506866967029488\n",
            "dest_pol.config['lr'] 0.010506866967029488\n",
            "on_train_result agt_store {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032, 0.007901364235074452], 'gamma': [0.9766025199359999, 0.9969770735748194]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066, 0.01185204635261168], 'gamma': [0.9969770735748194, 0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574, 0.010506866967029488], 'gamma': [0.9562289739239331, 0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924, 0.0665998986460843], 'gamma': [0.9650823105855924, 0.9613710712783627]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486, 0.0005102020914645242], 'gamma': [0.9597670369809892, 0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553, 0.008405493573623591], 'gamma': [0.9419988557225905, 0.9562289739239331]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "on_train_result g_helper.get_pair.remote() ('agt_1', 'agt_4')\n",
            "training loop = 1 of 5\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-06-10_04-01-37\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 18\n",
            "experiment_id: 9eabf5102e0a4e61bedb232b4a8ed663\n",
            "hostname: 685afc1c1019\n",
            "info:\n",
            "  learn_time_ms: 1065.013\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.005406962500678169\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.008907753822295032\n",
            "      entropy: 0.8933159775204129\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.24052344924873775\n",
            "      policy_loss: 0.03556920008526908\n",
            "      total_loss: 2.1257931192715964\n",
            "      vf_explained_var: -0.292950302362442\n",
            "      vf_loss: 2.0421192116207547\n",
            "    p_2:\n",
            "      allreduce_latency: 0.0043370723724365234\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.008755722472524574\n",
            "      entropy: 0.8567022085189819\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.29675977759891087\n",
            "      policy_loss: 0.13325407707856762\n",
            "      total_loss: 2.7047385242250233\n",
            "      vf_explained_var: -0.5284647941589355\n",
            "      vf_loss: 2.5121324724621243\n",
            "  num_steps_sampled: 180\n",
            "  num_steps_trained: 180\n",
            "  sync_weights_down_time: 14.673\n",
            "  sync_weights_up_time: 7.858\n",
            "iterations_since_restore: 1\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "optimizer_steps_this_iter: 1\n",
            "perf:\n",
            "  cpu_util_percent: 85.15\n",
            "  ram_util_percent: 46.650000000000006\n",
            "pid: 4535\n",
            "policy_reward_max:\n",
            "  p_0: 5.0\n",
            "  p_2: 2.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.3888888888888889\n",
            "  p_2: -0.3888888888888889\n",
            "policy_reward_min:\n",
            "  p_0: -2.0\n",
            "  p_2: -5.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.2979155509702621\n",
            "  mean_inference_ms: 3.601916374698762\n",
            "  mean_processing_ms: 4.346101514754758\n",
            "time_since_restore: 1.0912535190582275\n",
            "time_this_iter_s: 1.0912535190582275\n",
            "time_total_s: 1.0912535190582275\n",
            "timestamp: 1591761697\n",
            "timesteps_since_restore: 180\n",
            "timesteps_this_iter: 180\n",
            "timesteps_total: 180\n",
            "training_iteration: 1\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca5660f0>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca5660f0>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 249742659, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 4.0, ('agt_2', 'p_2'): -4.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1635102242, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca566fd0>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca566fd0>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 227172298, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 0.0, ('agt_2', 'p_2'): 0.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8235908>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8235908>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1135784751, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 0.0, ('agt_2', 'p_2'): 0.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1060667066, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce823b5c0>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce823b5c0>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 544672254, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 4.0, ('agt_2', 'p_2'): -4.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 818289219, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca520a90>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca520a90>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1808884822, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): -1.0, ('agt_2', 'p_2'): 1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 881723970, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 413401991, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce823b6a0>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce823b6a0>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 592583677, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 1.0, ('agt_2', 'p_2'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1509015798, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca5205f8>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca5205f8>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1635102242, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): -1.0, ('agt_4', 'p_4'): 1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0], 'rating': [0.0, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0], 'rating': [0.0, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1320554290, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadd3c83240>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadd3c83240>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 818289219, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 3.0, ('agt_4', 'p_4'): -3.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 998422681, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca5660f0>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca5660f0>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 881723970, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 1.0, ('agt_4', 'p_4'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce82d6f98>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce82d6f98>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1060667066, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): -2.0, ('agt_4', 'p_4'): 2.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0], 'rating': [0.0, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0], 'rating': [0.0, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1342886426, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8235908>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8235908>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 413401991, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 4.0, ('agt_4', 'p_4'): -4.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0], 'rating': [0.0, -0.05, -0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0], 'rating': [0.0, -0.05, -0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 939141148, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf116a160>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf116a160>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1509015798, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 2.0, ('agt_4', 'p_4'): -2.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 678713470, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 706448347, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca503eb8>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca503eb8>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1320554290, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 3.0, ('agt_4', 'p_4'): -3.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8248fd0>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8248fd0>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1342886426, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 2.0, ('agt_4', 'p_4'): -2.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1628617399, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf11195f8>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf11195f8>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 939141148, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 1.0, ('agt_4', 'p_4'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1916946329, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca5205f8>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca5205f8>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 998422681, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 1.0, ('agt_4', 'p_4'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1201284990, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca516b70>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca516b70>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 706448347, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 0.0, ('agt_4', 'p_4'): 0.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1978307233, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_sample_end returned sample batch of size 90\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1731918183, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8235908>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8235908>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 678713470, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): -2.0, ('agt_4', 'p_4'): 2.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1998036800, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_sample_end returned sample batch of size 90\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.DDPPO object at 0x7f87490afe10> -> 18 episodes\n",
            "on_train_result result {'episode_reward_max': 0.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 0.0, 'episode_len_mean': 10.0, 'episodes_this_iter': 18, 'policy_reward_min': {'p_0': -2.0, 'p_2': -5.0, 'p_1': -2.0, 'p_4': -4.0}, 'policy_reward_max': {'p_0': 5.0, 'p_2': 2.0, 'p_1': 4.0, 'p_4': 2.0}, 'policy_reward_mean': {'p_0': 0.625, 'p_2': -0.625, 'p_1': 1.0, 'p_4': -1.0}, 'custom_metrics': {}, 'hist_stats': {'episode_id': [], 'episode_reward': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'episode_lengths': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], 'policy_p_0_reward': [4.0, 0.0, -1.0, 0.0, 4.0, 1.0, -2.0, -2.0, -1.0, 2.0, 0.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 3.0, 1.0, 5.0, -1.0, 0.0, 1.0], 'policy_p_2_reward': [-4.0, 0.0, 1.0, 0.0, -4.0, -1.0, 2.0, 2.0, 1.0, -2.0, 0.0, -2.0, -1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -3.0, -1.0, -5.0, 1.0, 0.0, -1.0], 'policy_p_1_reward': [-1.0, 3.0, 1.0, 3.0, 1.0, 0.0, -2.0, 4.0, 2.0, 2.0, 1.0, -2.0], 'policy_p_4_reward': [1.0, -3.0, -1.0, -3.0, -1.0, 0.0, 2.0, -4.0, -2.0, -2.0, -1.0, 2.0]}, 'sampler_perf': {'mean_env_wait_ms': 0.23709154835080165, 'mean_processing_ms': 4.4258718198093385, 'mean_inference_ms': 3.5522647915158814}, 'off_policy_estimator': {}, 'info': {'num_steps_trained': 360, 'num_steps_sampled': 360, 'sync_weights_up_time': 6.791, 'sync_weights_down_time': 12.145, 'learn_time_ms': 1060.898, 'learner': {'p_0': {'allreduce_latency': 0.004825512568155925, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 0.008907753822295032, 'total_loss': 1.975239634513855, 'policy_loss': -0.049596777806679405, 'vf_loss': 1.9956187009811401, 'vf_explained_var': -0.042782884, 'kl': 0.14608792463938394, 'entropy': 0.9809234738349915, 'entropy_coeff': 0.0}, 'p_1': {'allreduce_latency': 0.008866190910339355, 'cur_kl_coeff': 0.19999999999999998, 'cur_lr': 0.009876705293843066, 'total_loss': 2.7900850574175515, 'policy_loss': 0.007028368301689625, 'vf_loss': 2.7298144896825156, 'vf_explained_var': -0.14004226, 'kl': 0.266211174428463, 'entropy': 0.6023379961649576, 'entropy_coeff': 0.0}, 'p_2': {'allreduce_latency': 0.003719965616861979, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 0.008755722472524574, 'total_loss': 2.387751579284668, 'policy_loss': 0.01814901332060496, 'vf_loss': 2.3641053438186646, 'vf_explained_var': -0.45671847, 'kl': 0.027485980341831844, 'entropy': 1.0703777472178142, 'entropy_coeff': 0.0}, 'p_4': {'allreduce_latency': 0.005115310351053874, 'cur_kl_coeff': 0.19999999999999998, 'cur_lr': 0.006340266341324486, 'total_loss': 2.80766099691391, 'policy_loss': 0.009998607138792673, 'vf_loss': 2.773405909538269, 'vf_explained_var': -0.31393957, 'kl': 0.12128267685572307, 'entropy': 0.990407278140386, 'entropy_coeff': 0.0}}}, 'optimizer_steps_this_iter': 1, 'timesteps_this_iter': 180, 'done': False, 'timesteps_total': 360, 'episodes_total': 36, 'training_iteration': 2, 'experiment_id': '9eabf5102e0a4e61bedb232b4a8ed663', 'date': '2020-06-10_04-01-38', 'timestamp': 1591761698, 'time_this_iter_s': 1.075605869293213, 'time_total_s': 2.1668593883514404, 'pid': 4535, 'hostname': '685afc1c1019', 'node_ip': '172.28.0.2', 'config': {'num_workers': 2, 'num_envs_per_worker': 3, 'rollout_fragment_length': 30, 'sample_batch_size': -1, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 90, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_action_dist': None, 'custom_options': {}, 'custom_preprocessor': None}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env_config': {}, 'env': 'RockPaperScissorsEnv', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 5e-05, 'monitor': False, 'log_level': 'WARN', 'callbacks': <class '__main__.MyCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'use_pytorch': True, 'eager': False, 'eager_tracing': False, 'no_eager_on_workers': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'use_exec_api': False, 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_cpus_per_worker': 0.25, 'num_gpus_per_worker': 0.125, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': '/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_RockPaperScissorsEnv/ray_results/', 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'p_0': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008907753822295032, 'gamma': 0.9766025199359999}), 'p_1': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009876705293843066, 'gamma': 0.9969770735748194}), 'p_2': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008755722472524574, 'gamma': 0.9562289739239331}), 'p_3': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009351876078460924, 'gamma': 0.9650823105855924}), 'p_4': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.006340266341324486, 'gamma': 0.9597670369809892}), 'p_5': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.0006377526143306553, 'gamma': 0.9419988557225905})}, 'policy_mapping_fn': <function select_policy at 0x7f87490aeae8>, 'policies_to_train': {'p_0': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008907753822295032, 'gamma': 0.9766025199359999}), 'p_1': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009876705293843066, 'gamma': 0.9969770735748194}), 'p_2': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008755722472524574, 'gamma': 0.9562289739239331}), 'p_3': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009351876078460924, 'gamma': 0.9650823105855924}), 'p_4': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.006340266341324486, 'gamma': 0.9597670369809892}), 'p_5': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.0006377526143306553, 'gamma': 0.9419988557225905})}}, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 10, 'shuffle_sequences': True, 'num_sgd_iter': 3, 'lr_schedule': None, 'vf_share_layers': False, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'simple_optimizer': True, '_fake_gpus': False, 'truncate_episodes': True}, 'time_since_restore': 2.1668593883514404, 'timesteps_since_restore': 360, 'iterations_since_restore': 2, 'perf': {'cpu_util_percent': 86.94999999999999, 'ram_util_percent': 46.8}, 'callback_ok': True}\n",
            "s_elo_val: 0.5000719557836594\n",
            "p_0_vs_p_5\n",
            "src_pol.config['lr'] 0.008405493573623591\n",
            "dest_pol.config['lr'] 0.007901364235074452\n",
            "src_pol.config['lr'] 0.008405493573623591\n",
            "dest_pol.config['lr'] 0.007901364235074452\n",
            "s_elo_val: 0.5\n",
            "p_1_vs_p_5\n",
            "src_pol.config['lr'] 0.008405493573623591\n",
            "dest_pol.config['lr'] 0.01185204635261168\n",
            "src_pol.config['lr'] 0.008405493573623591\n",
            "dest_pol.config['lr'] 0.01185204635261168\n",
            "s_elo_val: 0.5000719557836594\n",
            "p_2_vs_p_3\n",
            "src_pol.config['lr'] 0.0665998986460843\n",
            "dest_pol.config['lr'] 0.010506866967029488\n",
            "src_pol.config['lr'] 0.0665998986460843\n",
            "dest_pol.config['lr'] 0.0665998986460843\n",
            "s_elo_val: 0.4999280442163407\n",
            "p_3_vs_p_0\n",
            "src_pol.config['lr'] 0.006321091388059562\n",
            "dest_pol.config['lr'] 0.0665998986460843\n",
            "src_pol.config['lr'] 0.006321091388059562\n",
            "dest_pol.config['lr'] 0.006321091388059562\n",
            "s_elo_val: 0.5\n",
            "p_4_vs_p_1\n",
            "src_pol.config['lr'] 0.009481637082089343\n",
            "dest_pol.config['lr'] 0.0005102020914645242\n",
            "src_pol.config['lr'] 0.009481637082089343\n",
            "dest_pol.config['lr'] 0.0005102020914645242\n",
            "s_elo_val: 0.4999280442163407\n",
            "p_5_vs_p_2\n",
            "src_pol.config['lr'] 0.07991987837530116\n",
            "dest_pol.config['lr'] 0.008405493573623591\n",
            "src_pol.config['lr'] 0.07991987837530116\n",
            "dest_pol.config['lr'] 0.07991987837530116\n",
            "on_train_result agt_store {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032, 0.006321091388059562], 'gamma': [0.9766025199359999, 0.9969770735748194]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066, 0.009481637082089343], 'gamma': [0.9969770735748194, 0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574, 0.07991987837530116], 'gamma': [0.9562289739239331, 0.9613710712783627]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924, 0.00505687311044765], 'gamma': [0.9650823105855924, 0.9969770735748194]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486, 0.000612242509757429], 'gamma': [0.9597670369809892, 0.9419988557225905]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553, 0.06393590270024094], 'gamma': [0.9419988557225905, 0.9613710712783627]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "on_train_result g_helper.get_pair.remote() ('agt_0', 'agt_3')\n",
            "training loop = 2 of 5\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-06-10_04-01-38\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 36\n",
            "experiment_id: 9eabf5102e0a4e61bedb232b4a8ed663\n",
            "hostname: 685afc1c1019\n",
            "info:\n",
            "  learn_time_ms: 1060.898\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.004825512568155925\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.008907753822295032\n",
            "      entropy: 0.9809234738349915\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.14608792463938394\n",
            "      policy_loss: -0.049596777806679405\n",
            "      total_loss: 1.975239634513855\n",
            "      vf_explained_var: -0.04278288409113884\n",
            "      vf_loss: 1.9956187009811401\n",
            "    p_1:\n",
            "      allreduce_latency: 0.008866190910339355\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.009876705293843066\n",
            "      entropy: 0.6023379961649576\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.266211174428463\n",
            "      policy_loss: 0.007028368301689625\n",
            "      total_loss: 2.7900850574175515\n",
            "      vf_explained_var: -0.1400422602891922\n",
            "      vf_loss: 2.7298144896825156\n",
            "    p_2:\n",
            "      allreduce_latency: 0.003719965616861979\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.008755722472524574\n",
            "      entropy: 1.0703777472178142\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.027485980341831844\n",
            "      policy_loss: 0.01814901332060496\n",
            "      total_loss: 2.387751579284668\n",
            "      vf_explained_var: -0.45671847462654114\n",
            "      vf_loss: 2.3641053438186646\n",
            "    p_4:\n",
            "      allreduce_latency: 0.005115310351053874\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.006340266341324486\n",
            "      entropy: 0.990407278140386\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.12128267685572307\n",
            "      policy_loss: 0.009998607138792673\n",
            "      total_loss: 2.80766099691391\n",
            "      vf_explained_var: -0.31393957138061523\n",
            "      vf_loss: 2.773405909538269\n",
            "  num_steps_sampled: 360\n",
            "  num_steps_trained: 360\n",
            "  sync_weights_down_time: 12.145\n",
            "  sync_weights_up_time: 6.791\n",
            "iterations_since_restore: 2\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "optimizer_steps_this_iter: 1\n",
            "perf:\n",
            "  cpu_util_percent: 86.94999999999999\n",
            "  ram_util_percent: 46.8\n",
            "pid: 4535\n",
            "policy_reward_max:\n",
            "  p_0: 5.0\n",
            "  p_1: 4.0\n",
            "  p_2: 2.0\n",
            "  p_4: 2.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.625\n",
            "  p_1: 1.0\n",
            "  p_2: -0.625\n",
            "  p_4: -1.0\n",
            "policy_reward_min:\n",
            "  p_0: -2.0\n",
            "  p_1: -2.0\n",
            "  p_2: -5.0\n",
            "  p_4: -4.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.23709154835080165\n",
            "  mean_inference_ms: 3.5522647915158814\n",
            "  mean_processing_ms: 4.4258718198093385\n",
            "time_since_restore: 2.1668593883514404\n",
            "time_this_iter_s: 1.075605869293213\n",
            "time_total_s: 2.1668593883514404\n",
            "timestamp: 1591761698\n",
            "timesteps_since_restore: 360\n",
            "timesteps_this_iter: 180\n",
            "timesteps_total: 360\n",
            "training_iteration: 2\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca531ef0>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca531ef0>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1916946329, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 4.0, ('agt_4', 'p_4'): -4.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1241031323, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca531f28>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca531f28>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1201284990, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 5.0, ('agt_4', 'p_4'): -5.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1976506518, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca52ea20>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca52ea20>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1978307233, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): -1.0, ('agt_4', 'p_4'): 1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 534391970, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8255d30>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8255d30>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1628617399, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 3.0, ('agt_4', 'p_4'): -3.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 631658518, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8264e48>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8264e48>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1731918183, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 3.0, ('agt_4', 'p_4'): -3.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 806856231, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8264b70>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8264b70>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1998036800, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): -1.0, ('agt_4', 'p_4'): 1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0], 'rating': [0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 875091777, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca52e048>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca52e048>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1241031323, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): -1.0, ('agt_3', 'p_3'): 1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0], 'rating': [0.0, -0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1629459026, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca516b70>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca516b70>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1976506518, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 2.0, ('agt_3', 'p_3'): -2.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8235e80>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8235e80>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 631658518, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 1.0, ('agt_3', 'p_3'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0], 'rating': [0.0, -0.05, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 549767571, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca521f98>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca521f98>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 534391970, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): -1.0, ('agt_3', 'p_3'): 1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1188778369, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0], 'rating': [0.0, -0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1731731783, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8245eb8>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8245eb8>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 806856231, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 1.0, ('agt_3', 'p_3'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1543982618, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf1169b00>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf1169b00>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 875091777, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): -3.0, ('agt_3', 'p_3'): 3.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1956218572, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca566128>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca566128>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1629459026, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 0.0, ('agt_3', 'p_3'): 0.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 802384456, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca521c50>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca521c50>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 549767571, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 1.0, ('agt_3', 'p_3'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 577953865, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca531080>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca531080>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1188778369, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 1.0, ('agt_3', 'p_3'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8245f60>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8245f60>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1731731783, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 3.0, ('agt_3', 'p_3'): -3.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 446601414, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8235e80>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8235e80>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1543982618, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 2.0, ('agt_3', 'p_3'): -2.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 448090689, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1362734203, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_sample_end returned sample batch of size 90\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce82617b8>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce82617b8>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1956218572, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): -5.0, ('agt_3', 'p_3'): 5.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1019132825, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_0 p_0\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_3 p_3\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_sample_end returned sample batch of size 90\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.DDPPO object at 0x7f87490afe10> -> 18 episodes\n",
            "on_train_result result {'episode_reward_max': 0.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 0.0, 'episode_len_mean': 10.0, 'episodes_this_iter': 18, 'policy_reward_min': {'p_1': -2.0, 'p_4': -5.0, 'p_0': -5.0, 'p_3': -3.0, 'p_2': -5.0}, 'policy_reward_max': {'p_1': 5.0, 'p_4': 2.0, 'p_0': 5.0, 'p_3': 5.0, 'p_2': 2.0}, 'policy_reward_mean': {'p_1': 1.3888888888888888, 'p_4': -1.3888888888888888, 'p_0': 0.4444444444444444, 'p_3': -0.08333333333333333, 'p_2': -0.625}, 'custom_metrics': {}, 'hist_stats': {'episode_id': [], 'episode_reward': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'episode_lengths': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], 'policy_p_1_reward': [4.0, 5.0, -1.0, 3.0, 3.0, -1.0, -1.0, 3.0, 1.0, 3.0, 1.0, 0.0, -2.0, 4.0, 2.0, 2.0, 1.0, -2.0], 'policy_p_4_reward': [-4.0, -5.0, 1.0, -3.0, -3.0, 1.0, 1.0, -3.0, -1.0, -3.0, -1.0, 0.0, 2.0, -4.0, -2.0, -2.0, -1.0, 2.0], 'policy_p_0_reward': [-1.0, 2.0, -1.0, 0.0, 1.0, 1.0, 1.0, 1.0, -3.0, 3.0, 2.0, -5.0, -2.0, -2.0, -1.0, 2.0, 0.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 3.0, 1.0, 5.0, -1.0, 0.0, 1.0, 4.0, 0.0, -1.0, 0.0, 4.0, 1.0], 'policy_p_3_reward': [1.0, -2.0, 1.0, 0.0, -1.0, -1.0, -1.0, -1.0, 3.0, -3.0, -2.0, 5.0], 'policy_p_2_reward': [2.0, 2.0, 1.0, -2.0, 0.0, -2.0, -1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -3.0, -1.0, -5.0, 1.0, 0.0, -1.0, -4.0, 0.0, 1.0, 0.0, -4.0, -1.0]}, 'sampler_perf': {'mean_env_wait_ms': 0.20361950405328783, 'mean_processing_ms': 4.455171436329964, 'mean_inference_ms': 3.5526011655520957}, 'off_policy_estimator': {}, 'info': {'num_steps_trained': 540, 'num_steps_sampled': 540, 'sync_weights_up_time': 6.633, 'sync_weights_down_time': 11.2, 'learn_time_ms': 1048.197, 'learner': {'p_0': {'allreduce_latency': 0.005321820576985677, 'cur_kl_coeff': 0.19999999999999998, 'cur_lr': 0.00890775382229503, 'total_loss': 1.8338838517665863, 'policy_loss': 0.003074627990523974, 'vf_loss': 1.8177960912386577, 'vf_explained_var': -0.18223725, 'kl': 0.06506556707123916, 'entropy': 1.038270652294159, 'entropy_coeff': 0.0}, 'p_1': {'allreduce_latency': 0.004776716232299805, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 0.009876705293843066, 'total_loss': 3.0419729550679526, 'policy_loss': -0.033074200774232544, 'vf_loss': 3.065199613571167, 'vf_explained_var': 0.0014640689, 'kl': 0.04923743257919947, 'entropy': 1.0504491726557414, 'entropy_coeff': 0.0}, 'p_3': {'allreduce_latency': 0.003392815589904785, 'cur_kl_coeff': 0.19999999999999998, 'cur_lr': 0.009351876078460926, 'total_loss': 2.5815629760424295, 'policy_loss': 0.0077917480521136895, 'vf_loss': 2.5511687795321145, 'vf_explained_var': -0.40878057, 'kl': 0.1130118469397227, 'entropy': 0.9929754436016083, 'entropy_coeff': 0.0}, 'p_4': {'allreduce_latency': 0.007966200510660807, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 0.006340266341324486, 'total_loss': 3.6575516859690347, 'policy_loss': -0.07380572458108266, 'vf_loss': 3.7181076407432556, 'vf_explained_var': -0.5402384, 'kl': 0.06624943887193997, 'entropy': 1.0389052629470825, 'entropy_coeff': 0.0}}}, 'optimizer_steps_this_iter': 1, 'timesteps_this_iter': 180, 'done': False, 'timesteps_total': 540, 'episodes_total': 54, 'training_iteration': 3, 'experiment_id': '9eabf5102e0a4e61bedb232b4a8ed663', 'date': '2020-06-10_04-01-39', 'timestamp': 1591761699, 'time_this_iter_s': 1.0418241024017334, 'time_total_s': 3.208683490753174, 'pid': 4535, 'hostname': '685afc1c1019', 'node_ip': '172.28.0.2', 'config': {'num_workers': 2, 'num_envs_per_worker': 3, 'rollout_fragment_length': 30, 'sample_batch_size': -1, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 90, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_action_dist': None, 'custom_options': {}, 'custom_preprocessor': None}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env_config': {}, 'env': 'RockPaperScissorsEnv', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 5e-05, 'monitor': False, 'log_level': 'WARN', 'callbacks': <class '__main__.MyCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'use_pytorch': True, 'eager': False, 'eager_tracing': False, 'no_eager_on_workers': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'use_exec_api': False, 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_cpus_per_worker': 0.25, 'num_gpus_per_worker': 0.125, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': '/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_RockPaperScissorsEnv/ray_results/', 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'p_0': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008907753822295032, 'gamma': 0.9766025199359999}), 'p_1': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009876705293843066, 'gamma': 0.9969770735748194}), 'p_2': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008755722472524574, 'gamma': 0.9562289739239331}), 'p_3': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009351876078460924, 'gamma': 0.9650823105855924}), 'p_4': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.006340266341324486, 'gamma': 0.9597670369809892}), 'p_5': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.0006377526143306553, 'gamma': 0.9419988557225905})}, 'policy_mapping_fn': <function select_policy at 0x7f87490aeae8>, 'policies_to_train': {'p_0': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008907753822295032, 'gamma': 0.9766025199359999}), 'p_1': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009876705293843066, 'gamma': 0.9969770735748194}), 'p_2': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008755722472524574, 'gamma': 0.9562289739239331}), 'p_3': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009351876078460924, 'gamma': 0.9650823105855924}), 'p_4': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.006340266341324486, 'gamma': 0.9597670369809892}), 'p_5': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.0006377526143306553, 'gamma': 0.9419988557225905})}}, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 10, 'shuffle_sequences': True, 'num_sgd_iter': 3, 'lr_schedule': None, 'vf_share_layers': False, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'simple_optimizer': True, '_fake_gpus': False, 'truncate_episodes': True}, 'time_since_restore': 3.208683490753174, 'timesteps_since_restore': 540, 'iterations_since_restore': 3, 'perf': {'cpu_util_percent': 85.5, 'ram_util_percent': 46.8}, 'callback_ok': True}\n",
            "s_elo_val: 0.5\n",
            "p_0_vs_p_1\n",
            "src_pol.config['lr'] 0.009481637082089343\n",
            "dest_pol.config['lr'] 0.006321091388059562\n",
            "src_pol.config['lr'] 0.009481637082089343\n",
            "dest_pol.config['lr'] 0.006321091388059562\n",
            "s_elo_val: 0.5\n",
            "p_1_vs_p_4\n",
            "src_pol.config['lr'] 0.000612242509757429\n",
            "dest_pol.config['lr'] 0.009481637082089343\n",
            "src_pol.config['lr'] 0.000612242509757429\n",
            "dest_pol.config['lr'] 0.009481637082089343\n",
            "s_elo_val: 0.5000719557836594\n",
            "p_2_vs_p_5\n",
            "src_pol.config['lr'] 0.06393590270024094\n",
            "dest_pol.config['lr'] 0.07991987837530116\n",
            "src_pol.config['lr'] 0.06393590270024094\n",
            "dest_pol.config['lr'] 0.06393590270024094\n",
            "s_elo_val: 0.5\n",
            "p_3_vs_p_0\n",
            "src_pol.config['lr'] 0.00505687311044765\n",
            "dest_pol.config['lr'] 0.00505687311044765\n",
            "src_pol.config['lr'] 0.00505687311044765\n",
            "dest_pol.config['lr'] 0.00505687311044765\n",
            "s_elo_val: 0.5\n",
            "p_4_vs_p_1\n",
            "src_pol.config['lr'] 0.011377964498507212\n",
            "dest_pol.config['lr'] 0.000612242509757429\n",
            "src_pol.config['lr'] 0.011377964498507212\n",
            "dest_pol.config['lr'] 0.000612242509757429\n",
            "s_elo_val: 0.4999280442163407\n",
            "p_5_vs_p_3\n",
            "src_pol.config['lr'] 0.00404549848835812\n",
            "dest_pol.config['lr'] 0.06393590270024094\n",
            "src_pol.config['lr'] 0.00404549848835812\n",
            "dest_pol.config['lr'] 0.06393590270024094\n",
            "on_train_result agt_store {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032, 0.00505687311044765], 'gamma': [0.9766025199359999, 0.9969770735748194]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066, 0.011377964498507212], 'gamma': [0.9969770735748194, 0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574, 0.07672308324028912], 'gamma': [0.9562289739239331, 0.9613710712783627]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924, 0.00404549848835812], 'gamma': [0.9650823105855924, 0.9969770735748194]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486, 0.0007346910117089148], 'gamma': [0.9597670369809892, 0.9419988557225905]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553, 0.07672308324028912], 'gamma': [0.9419988557225905, 0.9613710712783627]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "on_train_result g_helper.get_pair.remote() ('agt_1', 'agt_2')\n",
            "training loop = 3 of 5\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-06-10_04-01-39\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 54\n",
            "experiment_id: 9eabf5102e0a4e61bedb232b4a8ed663\n",
            "hostname: 685afc1c1019\n",
            "info:\n",
            "  learn_time_ms: 1048.197\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.005321820576985677\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.00890775382229503\n",
            "      entropy: 1.038270652294159\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.06506556707123916\n",
            "      policy_loss: 0.003074627990523974\n",
            "      total_loss: 1.8338838517665863\n",
            "      vf_explained_var: -0.18223725259304047\n",
            "      vf_loss: 1.8177960912386577\n",
            "    p_1:\n",
            "      allreduce_latency: 0.004776716232299805\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.009876705293843066\n",
            "      entropy: 1.0504491726557414\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.04923743257919947\n",
            "      policy_loss: -0.033074200774232544\n",
            "      total_loss: 3.0419729550679526\n",
            "      vf_explained_var: 0.00146406888961792\n",
            "      vf_loss: 3.065199613571167\n",
            "    p_3:\n",
            "      allreduce_latency: 0.003392815589904785\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.009351876078460926\n",
            "      entropy: 0.9929754436016083\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.1130118469397227\n",
            "      policy_loss: 0.0077917480521136895\n",
            "      total_loss: 2.5815629760424295\n",
            "      vf_explained_var: -0.408780574798584\n",
            "      vf_loss: 2.5511687795321145\n",
            "    p_4:\n",
            "      allreduce_latency: 0.007966200510660807\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.006340266341324486\n",
            "      entropy: 1.0389052629470825\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.06624943887193997\n",
            "      policy_loss: -0.07380572458108266\n",
            "      total_loss: 3.6575516859690347\n",
            "      vf_explained_var: -0.5402383804321289\n",
            "      vf_loss: 3.7181076407432556\n",
            "  num_steps_sampled: 540\n",
            "  num_steps_trained: 540\n",
            "  sync_weights_down_time: 11.2\n",
            "  sync_weights_up_time: 6.633\n",
            "iterations_since_restore: 3\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "optimizer_steps_this_iter: 1\n",
            "perf:\n",
            "  cpu_util_percent: 85.5\n",
            "  ram_util_percent: 46.8\n",
            "pid: 4535\n",
            "policy_reward_max:\n",
            "  p_0: 5.0\n",
            "  p_1: 5.0\n",
            "  p_2: 2.0\n",
            "  p_3: 5.0\n",
            "  p_4: 2.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.4444444444444444\n",
            "  p_1: 1.3888888888888888\n",
            "  p_2: -0.625\n",
            "  p_3: -0.08333333333333333\n",
            "  p_4: -1.3888888888888888\n",
            "policy_reward_min:\n",
            "  p_0: -5.0\n",
            "  p_1: -2.0\n",
            "  p_2: -5.0\n",
            "  p_3: -3.0\n",
            "  p_4: -5.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.20361950405328783\n",
            "  mean_inference_ms: 3.5526011655520957\n",
            "  mean_processing_ms: 4.455171436329964\n",
            "time_since_restore: 3.208683490753174\n",
            "time_this_iter_s: 1.0418241024017334\n",
            "time_total_s: 3.208683490753174\n",
            "timestamp: 1591761699\n",
            "timesteps_since_restore: 540\n",
            "timesteps_this_iter: 180\n",
            "timesteps_total: 540\n",
            "training_iteration: 3\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca538e10>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca538e10>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 802384456, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): -2.0, ('agt_3', 'p_3'): 2.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1686629701, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8268940>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8268940>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 446601414, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): -3.0, ('agt_3', 'p_3'): 3.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca530fd0>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca530fd0>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 577953865, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 3.0, ('agt_3', 'p_3'): -3.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1546804677, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca530e48>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca530e48>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1362734203, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 0.0, ('agt_3', 'p_3'): 0.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1608747682, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1209956272, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8269cc0>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8269cc0>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 448090689, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 1.0, ('agt_3', 'p_3'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1972469635, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8269400>, agt_0, p_0, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8269400>, agt_3, p_3, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1019132825, episode.agent_rewards defaultdict(<class 'float'>, {('agt_0', 'p_0'): 1.0, ('agt_3', 'p_3'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1555970236, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca521f98>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca521f98>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1686629701, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 0.0, ('agt_2', 'p_2'): 0.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1176490334, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca538e10>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca538e10>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1546804677, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 6.0, ('agt_2', 'p_2'): -6.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1634982427, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce82617b8>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce82617b8>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1209956272, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 1.0, ('agt_2', 'p_2'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca52e358>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca52e358>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1608747682, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 2.0, ('agt_2', 'p_2'): -2.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1955260295, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1642554376, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8255dd8>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8255dd8>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1972469635, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 2.0, ('agt_2', 'p_2'): -2.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1656368498, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf1169b00>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1cf1169b00>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1555970236, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): -1.0, ('agt_2', 'p_2'): 1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 681885148, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca538d68>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca538d68>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1176490334, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): -3.0, ('agt_2', 'p_2'): 3.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1981431320, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca538e80>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca538e80>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1634982427, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 1.0, ('agt_2', 'p_2'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 504319456, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca538e10>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca538e10>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1955260295, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 0.0, ('agt_2', 'p_2'): 0.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 157470493, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_sample_end returned sample batch of size 90\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8231eb8>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8231eb8>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1642554376, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): -1.0, ('agt_2', 'p_2'): 1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 537652068, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8268ba8>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8268ba8>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1656368498, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 3.0, ('agt_2', 'p_2'): -3.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 913007238, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8269cc0>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8269cc0>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 681885148, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 2.0, ('agt_2', 'p_2'): -2.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 793580596, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_2 p_2\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_sample_end returned sample batch of size 90\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.DDPPO object at 0x7f87490afe10> -> 18 episodes\n",
            "on_train_result result {'episode_reward_max': 0.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 0.0, 'episode_len_mean': 10.0, 'episodes_this_iter': 18, 'policy_reward_min': {'p_0': -5.0, 'p_3': -3.0, 'p_1': -3.0, 'p_2': -6.0, 'p_4': -5.0}, 'policy_reward_max': {'p_0': 5.0, 'p_3': 5.0, 'p_1': 6.0, 'p_2': 3.0, 'p_4': 2.0}, 'policy_reward_mean': {'p_0': 0.38095238095238093, 'p_3': -0.05555555555555555, 'p_1': 1.2333333333333334, 'p_2': -0.75, 'p_4': -1.3888888888888888}, 'custom_metrics': {}, 'hist_stats': {'episode_id': [], 'episode_reward': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'episode_lengths': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], 'policy_p_0_reward': [-2.0, 3.0, 0.0, -3.0, 1.0, 1.0, -2.0, -2.0, -1.0, 2.0, 0.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 3.0, 1.0, 5.0, -1.0, 0.0, 1.0, 4.0, 0.0, -1.0, 0.0, 4.0, 1.0, -1.0, 2.0, -1.0, 0.0, 1.0, 1.0, 1.0, 1.0, -3.0, 3.0, 2.0, -5.0], 'policy_p_3_reward': [2.0, -3.0, 0.0, 3.0, -1.0, -1.0, 1.0, -2.0, 1.0, 0.0, -1.0, -1.0, -1.0, -1.0, 3.0, -3.0, -2.0, 5.0], 'policy_p_1_reward': [0.0, 6.0, 2.0, -3.0, 1.0, 0.0, 1.0, 2.0, -1.0, -1.0, 3.0, 2.0, -1.0, 3.0, 1.0, 3.0, 1.0, 0.0, -2.0, 4.0, 2.0, 2.0, 1.0, -2.0, 4.0, 5.0, -1.0, 3.0, 3.0, -1.0], 'policy_p_2_reward': [0.0, -6.0, -2.0, 3.0, -1.0, 0.0, -1.0, -2.0, 1.0, 1.0, -3.0, -2.0, 2.0, 2.0, 1.0, -2.0, 0.0, -2.0, -1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -3.0, -1.0, -5.0, 1.0, 0.0, -1.0, -4.0, 0.0, 1.0, 0.0, -4.0, -1.0], 'policy_p_4_reward': [1.0, -3.0, -1.0, -3.0, -1.0, 0.0, 2.0, -4.0, -2.0, -2.0, -1.0, 2.0, -4.0, -5.0, 1.0, -3.0, -3.0, 1.0]}, 'sampler_perf': {'mean_env_wait_ms': 0.18161672325140474, 'mean_processing_ms': 4.490514996386107, 'mean_inference_ms': 3.565275328369166}, 'off_policy_estimator': {}, 'info': {'num_steps_trained': 720, 'num_steps_sampled': 720, 'sync_weights_up_time': 6.159, 'sync_weights_down_time': 10.905, 'learn_time_ms': 1045.826, 'learner': {'p_0': {'allreduce_latency': 0.004007577896118164, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 0.008907753822295032, 'total_loss': 3.2768986225128174, 'policy_loss': 0.05195793012777964, 'vf_loss': 3.2117231686909995, 'vf_explained_var': -0.081920266, 'kl': 0.06608770042657852, 'entropy': 0.994611938794454, 'entropy_coeff': 0.0}, 'p_1': {'allreduce_latency': 0.004326939582824707, 'cur_kl_coeff': 0.19999999999999998, 'cur_lr': 0.009876705293843066, 'total_loss': 4.326315085093181, 'policy_loss': -0.03145157794157664, 'vf_loss': 4.339092791080475, 'vf_explained_var': -0.014670104, 'kl': 0.0933691281825304, 'entropy': 0.9961279133955637, 'entropy_coeff': 0.0}, 'p_2': {'allreduce_latency': 0.006044149398803711, 'cur_kl_coeff': 0.19999999999999998, 'cur_lr': 0.008755722472524574, 'total_loss': 4.0810539325078325, 'policy_loss': 0.044855294128259025, 'vf_loss': 4.015428781509399, 'vf_explained_var': -0.10750267, 'kl': 0.10384918128450711, 'entropy': 0.9978473087151846, 'entropy_coeff': 0.0}, 'p_3': {'allreduce_latency': 0.0037734508514404297, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 0.009351876078460924, 'total_loss': 2.865353544553121, 'policy_loss': 0.06102964972766737, 'vf_loss': 2.7928545475006104, 'vf_explained_var': 0.058748286, 'kl': 0.057345278561115265, 'entropy': 1.005924940109253, 'entropy_coeff': 0.0}}}, 'optimizer_steps_this_iter': 1, 'timesteps_this_iter': 180, 'done': False, 'timesteps_total': 720, 'episodes_total': 72, 'training_iteration': 4, 'experiment_id': '9eabf5102e0a4e61bedb232b4a8ed663', 'date': '2020-06-10_04-01-40', 'timestamp': 1591761700, 'time_this_iter_s': 1.0572302341461182, 'time_total_s': 4.265913724899292, 'pid': 4535, 'hostname': '685afc1c1019', 'node_ip': '172.28.0.2', 'config': {'num_workers': 2, 'num_envs_per_worker': 3, 'rollout_fragment_length': 30, 'sample_batch_size': -1, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 90, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_action_dist': None, 'custom_options': {}, 'custom_preprocessor': None}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env_config': {}, 'env': 'RockPaperScissorsEnv', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 5e-05, 'monitor': False, 'log_level': 'WARN', 'callbacks': <class '__main__.MyCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'use_pytorch': True, 'eager': False, 'eager_tracing': False, 'no_eager_on_workers': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'use_exec_api': False, 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_cpus_per_worker': 0.25, 'num_gpus_per_worker': 0.125, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': '/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_RockPaperScissorsEnv/ray_results/', 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'p_0': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008907753822295032, 'gamma': 0.9766025199359999}), 'p_1': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009876705293843066, 'gamma': 0.9969770735748194}), 'p_2': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008755722472524574, 'gamma': 0.9562289739239331}), 'p_3': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009351876078460924, 'gamma': 0.9650823105855924}), 'p_4': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.006340266341324486, 'gamma': 0.9597670369809892}), 'p_5': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.0006377526143306553, 'gamma': 0.9419988557225905})}, 'policy_mapping_fn': <function select_policy at 0x7f87490aeae8>, 'policies_to_train': {'p_0': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008907753822295032, 'gamma': 0.9766025199359999}), 'p_1': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009876705293843066, 'gamma': 0.9969770735748194}), 'p_2': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008755722472524574, 'gamma': 0.9562289739239331}), 'p_3': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009351876078460924, 'gamma': 0.9650823105855924}), 'p_4': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.006340266341324486, 'gamma': 0.9597670369809892}), 'p_5': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.0006377526143306553, 'gamma': 0.9419988557225905})}}, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 10, 'shuffle_sequences': True, 'num_sgd_iter': 3, 'lr_schedule': None, 'vf_share_layers': False, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'simple_optimizer': True, '_fake_gpus': False, 'truncate_episodes': True}, 'time_since_restore': 4.265913724899292, 'timesteps_since_restore': 720, 'iterations_since_restore': 4, 'perf': {'cpu_util_percent': 85.5, 'ram_util_percent': 46.8}, 'callback_ok': True}\n",
            "s_elo_val: 0.5\n",
            "p_0_vs_p_1\n",
            "src_pol.config['lr'] 0.011377964498507212\n",
            "dest_pol.config['lr'] 0.00505687311044765\n",
            "src_pol.config['lr'] 0.011377964498507212\n",
            "dest_pol.config['lr'] 0.011377964498507212\n",
            "s_elo_val: 0.5\n",
            "p_1_vs_p_0\n",
            "src_pol.config['lr'] 0.013653557398208654\n",
            "dest_pol.config['lr'] 0.011377964498507212\n",
            "src_pol.config['lr'] 0.013653557398208654\n",
            "dest_pol.config['lr'] 0.013653557398208654\n",
            "s_elo_val: 0.5\n",
            "p_2_vs_p_1\n",
            "src_pol.config['lr'] 0.010922845918566924\n",
            "dest_pol.config['lr'] 0.07672308324028912\n",
            "src_pol.config['lr'] 0.010922845918566924\n",
            "dest_pol.config['lr'] 0.07672308324028912\n",
            "s_elo_val: 0.4998560884356619\n",
            "p_3_vs_p_4\n",
            "src_pol.config['lr'] 0.0007346910117089148\n",
            "dest_pol.config['lr'] 0.00404549848835812\n",
            "src_pol.config['lr'] 0.0007346910117089148\n",
            "dest_pol.config['lr'] 0.00404549848835812\n",
            "s_elo_val: 0.5000719557836594\n",
            "p_4_vs_p_5\n",
            "src_pol.config['lr'] 0.07672308324028912\n",
            "dest_pol.config['lr'] 0.0007346910117089148\n",
            "src_pol.config['lr'] 0.07672308324028912\n",
            "dest_pol.config['lr'] 0.07672308324028912\n",
            "s_elo_val: 0.4999280442163407\n",
            "p_5_vs_p_4\n",
            "src_pol.config['lr'] 0.0613784665922313\n",
            "dest_pol.config['lr'] 0.07672308324028912\n",
            "src_pol.config['lr'] 0.0613784665922313\n",
            "dest_pol.config['lr'] 0.0613784665922313\n",
            "on_train_result agt_store {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032, 0.013653557398208654], 'gamma': [0.9766025199359999, 0.9969770735748194]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066, 0.010922845918566924], 'gamma': [0.9969770735748194, 0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574, 0.09206769988834694], 'gamma': [0.9562289739239331, 0.9613710712783627]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924, 0.0032363987906864965], 'gamma': [0.9650823105855924, 0.9969770735748194]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486, 0.0613784665922313], 'gamma': [0.9597670369809892, 0.9613710712783627]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553, 0.04910277327378504], 'gamma': [0.9419988557225905, 0.9613710712783627]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "on_train_result g_helper.get_pair.remote() ('agt_4', 'agt_1')\n",
            "training loop = 4 of 5\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-06-10_04-01-40\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 72\n",
            "experiment_id: 9eabf5102e0a4e61bedb232b4a8ed663\n",
            "hostname: 685afc1c1019\n",
            "info:\n",
            "  learn_time_ms: 1045.826\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.004007577896118164\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.008907753822295032\n",
            "      entropy: 0.994611938794454\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.06608770042657852\n",
            "      policy_loss: 0.05195793012777964\n",
            "      total_loss: 3.2768986225128174\n",
            "      vf_explained_var: -0.08192026615142822\n",
            "      vf_loss: 3.2117231686909995\n",
            "    p_1:\n",
            "      allreduce_latency: 0.004326939582824707\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.009876705293843066\n",
            "      entropy: 0.9961279133955637\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0933691281825304\n",
            "      policy_loss: -0.03145157794157664\n",
            "      total_loss: 4.326315085093181\n",
            "      vf_explained_var: -0.014670103788375854\n",
            "      vf_loss: 4.339092791080475\n",
            "    p_2:\n",
            "      allreduce_latency: 0.006044149398803711\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.008755722472524574\n",
            "      entropy: 0.9978473087151846\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.10384918128450711\n",
            "      policy_loss: 0.044855294128259025\n",
            "      total_loss: 4.0810539325078325\n",
            "      vf_explained_var: -0.10750266909599304\n",
            "      vf_loss: 4.015428781509399\n",
            "    p_3:\n",
            "      allreduce_latency: 0.0037734508514404297\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.009351876078460924\n",
            "      entropy: 1.005924940109253\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.057345278561115265\n",
            "      policy_loss: 0.06102964972766737\n",
            "      total_loss: 2.865353544553121\n",
            "      vf_explained_var: 0.058748286217451096\n",
            "      vf_loss: 2.7928545475006104\n",
            "  num_steps_sampled: 720\n",
            "  num_steps_trained: 720\n",
            "  sync_weights_down_time: 10.905\n",
            "  sync_weights_up_time: 6.159\n",
            "iterations_since_restore: 4\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "optimizer_steps_this_iter: 1\n",
            "perf:\n",
            "  cpu_util_percent: 85.5\n",
            "  ram_util_percent: 46.8\n",
            "pid: 4535\n",
            "policy_reward_max:\n",
            "  p_0: 5.0\n",
            "  p_1: 6.0\n",
            "  p_2: 3.0\n",
            "  p_3: 5.0\n",
            "  p_4: 2.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.38095238095238093\n",
            "  p_1: 1.2333333333333334\n",
            "  p_2: -0.75\n",
            "  p_3: -0.05555555555555555\n",
            "  p_4: -1.3888888888888888\n",
            "policy_reward_min:\n",
            "  p_0: -5.0\n",
            "  p_1: -3.0\n",
            "  p_2: -6.0\n",
            "  p_3: -3.0\n",
            "  p_4: -5.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.18161672325140474\n",
            "  mean_inference_ms: 3.565275328369166\n",
            "  mean_processing_ms: 4.490514996386107\n",
            "time_since_restore: 4.265913724899292\n",
            "time_this_iter_s: 1.0572302341461182\n",
            "time_total_s: 4.265913724899292\n",
            "timestamp: 1591761700\n",
            "timesteps_since_restore: 720\n",
            "timesteps_this_iter: 180\n",
            "timesteps_total: 720\n",
            "training_iteration: 4\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca527e80>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca527e80>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1981431320, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 1.0, ('agt_2', 'p_2'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce825e128>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce825e128>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 537652068, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 1.0, ('agt_2', 'p_2'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1649980322, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca541dd8>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca541dd8>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 504319456, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): -1.0, ('agt_2', 'p_2'): 1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 365056972, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca541a20>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca541a20>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 157470493, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 1.0, ('agt_2', 'p_2'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1473591869, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 418770202, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce07bfa58>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce07bfa58>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 913007238, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 1.0, ('agt_2', 'p_2'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1241257670, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce07bfc50>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce07bfc50>, agt_2, p_2, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 793580596, episode.agent_rewards defaultdict(<class 'float'>, {('agt_1', 'p_1'): 0.0, ('agt_2', 'p_2'): 0.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1050739153, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca531f60>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca531f60>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1649980322, episode.agent_rewards defaultdict(<class 'float'>, {('agt_4', 'p_4'): 1.0, ('agt_1', 'p_1'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce82311d0>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce82311d0>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 418770202, episode.agent_rewards defaultdict(<class 'float'>, {('agt_4', 'p_4'): 2.0, ('agt_1', 'p_1'): -2.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0, -2.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0, 2.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1937456130, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca53a2b0>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca53a2b0>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 365056972, episode.agent_rewards defaultdict(<class 'float'>, {('agt_4', 'p_4'): -2.0, ('agt_1', 'p_1'): 2.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0, -2.0, -1.0, -1.0, 2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0, 2.0, 1.0, 1.0, -2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1716910237, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca52e3c8>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca52e3c8>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1473591869, episode.agent_rewards defaultdict(<class 'float'>, {('agt_4', 'p_4'): -1.0, ('agt_1', 'p_1'): 1.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0, -2.0, -1.0, -1.0, 2.0, -3.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 3.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1873969486, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0, -2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0, 2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 306868811, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce825e128>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce825e128>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1241257670, episode.agent_rewards defaultdict(<class 'float'>, {('agt_4', 'p_4'): 1.0, ('agt_1', 'p_1'): -1.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0, -2.0, -1.0, -1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0, 2.0, 1.0, 1.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1553971145, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce07bfa58>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce07bfa58>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1050739153, episode.agent_rewards defaultdict(<class 'float'>, {('agt_4', 'p_4'): 3.0, ('agt_1', 'p_1'): -3.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0, -2.0, -1.0, -1.0, 2.0, -3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 700259184, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca529d68>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca529d68>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1937456130, episode.agent_rewards defaultdict(<class 'float'>, {('agt_4', 'p_4'): 0.0, ('agt_1', 'p_1'): 0.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0, -2.0, -1.0, -1.0, 2.0, -3.0, 1.0, 2.0, 0.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05, -0.05, 0.0], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 3.0, -1.0, -2.0, 0.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05, -0.05, 0.0], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 1377950897, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8269cc0>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8269cc0>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 306868811, episode.agent_rewards defaultdict(<class 'float'>, {('agt_4', 'p_4'): -2.0, ('agt_1', 'p_1'): 2.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0, -2.0, -1.0, -1.0, 2.0, -3.0, 1.0, 2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 3.0, -1.0, -2.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 818089013, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce823deb8>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce823deb8>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 1553971145, episode.agent_rewards defaultdict(<class 'float'>, {('agt_4', 'p_4'): -3.0, ('agt_1', 'p_1'): 3.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca529d30>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca529d30>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1716910237, episode.agent_rewards defaultdict(<class 'float'>, {('agt_4', 'p_4'): -4.0, ('agt_1', 'p_1'): 4.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0, -2.0, -1.0, -1.0, 2.0, -3.0, 1.0, 2.0, 0.0, 3.0, 4.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05, -0.05, 0.0, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 3.0, -1.0, -2.0, 0.0, -3.0, -4.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05, -0.05, 0.0, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 637093256, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca529550>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7fadca529550>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end 1873969486, episode.agent_rewards defaultdict(<class 'float'>, {('agt_4', 'p_4'): 3.0, ('agt_1', 'p_1'): -3.0})\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0, -2.0, -1.0, -1.0, 2.0, -3.0, 1.0, 2.0, 0.0, 3.0, 4.0, 4.0, -3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05, -0.05, 0.0, -0.05, -0.05, -0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 3.0, -1.0, -2.0, 0.0, -3.0, -4.0, -4.0, 3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05, -0.05, 0.0, -0.05, -0.05, -0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_episode_start 5960564, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4591)\u001b[0m on_sample_end returned sample batch of size 90\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0, -2.0, -1.0, -1.0, 2.0, -3.0, 1.0, 2.0, 0.0, 3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05, -0.05, 0.0, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 3.0, -1.0, -2.0, 0.0, -3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05, -0.05, 0.0, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1362184202, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8266320>, agt_1, p_1, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m postprocessed <ray.rllib.evaluation.episode.MultiAgentEpisode object at 0x7f1ce8266320>, agt_4, p_4, 10 steps\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end 700259184, episode.agent_rewards defaultdict(<class 'float'>, {('agt_4', 'p_4'): -4.0, ('agt_1', 'p_1'): 4.0})\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_end ray.get(g_helper.get_agt_store.remote()) {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032], 'gamma': [0.9766025199359999]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066], 'gamma': [0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0, -2.0, -1.0, -1.0, 2.0, -3.0, 1.0, 2.0, 0.0, 3.0, 4.0, 4.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05, -0.05, 0.0, -0.05, -0.05, -0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574], 'gamma': [0.9562289739239331]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924], 'gamma': [0.9650823105855924]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486], 'gamma': [0.9597670369809892]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 3.0, -1.0, -2.0, 0.0, -3.0, -4.0, -4.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05, -0.05, 0.0, -0.05, -0.05, -0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553], 'gamma': [0.9419988557225905]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_episode_start 1086448596, _agent_to_policy {}\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_4 p_4\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m select_policy agt_1 p_1\n",
            "\u001b[2m\u001b[36m(pid=4644)\u001b[0m on_sample_end returned sample batch of size 90\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.DDPPO object at 0x7f87490afe10> -> 18 episodes\n",
            "on_train_result result {'episode_reward_max': 0.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 0.0, 'episode_len_mean': 10.0, 'episodes_this_iter': 18, 'policy_reward_min': {'p_1': -3.0, 'p_2': -6.0, 'p_4': -5.0, 'p_0': -5.0, 'p_3': -3.0}, 'policy_reward_max': {'p_1': 6.0, 'p_2': 3.0, 'p_4': 3.0, 'p_0': 5.0, 'p_3': 5.0}, 'policy_reward_mean': {'p_1': 0.9583333333333334, 'p_2': -0.7142857142857143, 'p_4': -1.0333333333333334, 'p_0': 0.38095238095238093, 'p_3': -0.05555555555555555}, 'custom_metrics': {}, 'hist_stats': {'episode_id': [], 'episode_reward': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'episode_lengths': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], 'policy_p_1_reward': [1.0, -1.0, 1.0, -1.0, 2.0, 1.0, 0.0, 4.0, -3.0, 1.0, 1.0, 0.0, -2.0, -1.0, -3.0, 2.0, 3.0, 4.0, -1.0, 3.0, 1.0, 3.0, 1.0, 0.0, -2.0, 4.0, 2.0, 2.0, 1.0, -2.0, 4.0, 5.0, -1.0, 3.0, 3.0, -1.0, 0.0, 6.0, 2.0, -3.0, 1.0, 0.0, 1.0, 2.0, -1.0, -1.0, 3.0, 2.0], 'policy_p_2_reward': [-1.0, 1.0, -1.0, -1.0, -1.0, 0.0, 2.0, 2.0, 1.0, -2.0, 0.0, -2.0, -1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -3.0, -1.0, -5.0, 1.0, 0.0, -1.0, -4.0, 0.0, 1.0, 0.0, -4.0, -1.0, 0.0, -6.0, -2.0, 3.0, -1.0, 0.0, -1.0, -2.0, 1.0, 1.0, -3.0, -2.0], 'policy_p_4_reward': [1.0, -2.0, -1.0, 0.0, -4.0, 3.0, 2.0, 1.0, 3.0, -2.0, -3.0, -4.0, 1.0, -3.0, -1.0, -3.0, -1.0, 0.0, 2.0, -4.0, -2.0, -2.0, -1.0, 2.0, -4.0, -5.0, 1.0, -3.0, -3.0, 1.0], 'policy_p_0_reward': [-2.0, -2.0, -1.0, 2.0, 0.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 3.0, 1.0, 5.0, -1.0, 0.0, 1.0, 4.0, 0.0, -1.0, 0.0, 4.0, 1.0, -1.0, 2.0, -1.0, 0.0, 1.0, 1.0, 1.0, 1.0, -3.0, 3.0, 2.0, -5.0, -2.0, 3.0, 0.0, -3.0, 1.0, 1.0], 'policy_p_3_reward': [1.0, -2.0, 1.0, 0.0, -1.0, -1.0, -1.0, -1.0, 3.0, -3.0, -2.0, 5.0, 2.0, -3.0, 0.0, 3.0, -1.0, -1.0]}, 'sampler_perf': {'mean_env_wait_ms': 0.16600784790448223, 'mean_processing_ms': 4.5406135808196115, 'mean_inference_ms': 3.581510012295573}, 'off_policy_estimator': {}, 'info': {'num_steps_trained': 900, 'num_steps_sampled': 900, 'sync_weights_up_time': 5.902, 'sync_weights_down_time': 11.039, 'learn_time_ms': 1052.144, 'learner': {'p_1': {'allreduce_latency': 0.0037633048163519967, 'cur_kl_coeff': 0.2, 'cur_lr': 0.009876705293843066, 'total_loss': 3.1037884950637817, 'policy_loss': 0.08401356264948845, 'vf_loss': 2.9974718358781605, 'vf_explained_var': -0.06289044, 'kl': 0.1115152310166094, 'entropy': 0.5852200587590536, 'entropy_coeff': 0.0}, 'p_2': {'allreduce_latency': 0.006133635838826497, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 0.008755722472524574, 'total_loss': 1.9805156389872234, 'policy_loss': 0.048892332861820854, 'vf_loss': 1.9003667831420898, 'vf_explained_var': -0.34553373, 'kl': 0.1562828520933787, 'entropy': 0.8468038439750671, 'entropy_coeff': 0.0}, 'p_4': {'allreduce_latency': 0.00467371940612793, 'cur_kl_coeff': 0.19999999999999998, 'cur_lr': 0.006340266341324486, 'total_loss': 3.3840961853663125, 'policy_loss': 0.07952229709674914, 'vf_loss': 3.2758182485898337, 'vf_explained_var': -0.24376446, 'kl': 0.14377869168917337, 'entropy': 0.9205128848552704, 'entropy_coeff': 0.0}}}, 'optimizer_steps_this_iter': 1, 'timesteps_this_iter': 180, 'done': False, 'timesteps_total': 900, 'episodes_total': 90, 'training_iteration': 5, 'experiment_id': '9eabf5102e0a4e61bedb232b4a8ed663', 'date': '2020-06-10_04-01-41', 'timestamp': 1591761701, 'time_this_iter_s': 1.0972306728363037, 'time_total_s': 5.363144397735596, 'pid': 4535, 'hostname': '685afc1c1019', 'node_ip': '172.28.0.2', 'config': {'num_workers': 2, 'num_envs_per_worker': 3, 'rollout_fragment_length': 30, 'sample_batch_size': -1, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 90, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_action_dist': None, 'custom_options': {}, 'custom_preprocessor': None}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env_config': {}, 'env': 'RockPaperScissorsEnv', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 5e-05, 'monitor': False, 'log_level': 'WARN', 'callbacks': <class '__main__.MyCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'use_pytorch': True, 'eager': False, 'eager_tracing': False, 'no_eager_on_workers': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'use_exec_api': False, 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_cpus_per_worker': 0.25, 'num_gpus_per_worker': 0.125, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': '/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_RockPaperScissorsEnv/ray_results/', 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'p_0': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008907753822295032, 'gamma': 0.9766025199359999}), 'p_1': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009876705293843066, 'gamma': 0.9969770735748194}), 'p_2': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008755722472524574, 'gamma': 0.9562289739239331}), 'p_3': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009351876078460924, 'gamma': 0.9650823105855924}), 'p_4': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.006340266341324486, 'gamma': 0.9597670369809892}), 'p_5': (<class 'ray.rllib.policy.torch_policy_template.PPOTorchPolicy'>, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.0006377526143306553, 'gamma': 0.9419988557225905})}, 'policy_mapping_fn': <function select_policy at 0x7f87490aeae8>, 'policies_to_train': {'p_0': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008907753822295032, 'gamma': 0.9766025199359999}), 'p_1': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009876705293843066, 'gamma': 0.9969770735748194}), 'p_2': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.008755722472524574, 'gamma': 0.9562289739239331}), 'p_3': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.009351876078460924, 'gamma': 0.9650823105855924}), 'p_4': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.006340266341324486, 'gamma': 0.9597670369809892}), 'p_5': (None, Discrete(3), Discrete(3), {'model': {'use_lstm': False}, 'lr': 0.0006377526143306553, 'gamma': 0.9419988557225905})}}, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 10, 'shuffle_sequences': True, 'num_sgd_iter': 3, 'lr_schedule': None, 'vf_share_layers': False, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'simple_optimizer': True, '_fake_gpus': False, 'truncate_episodes': True}, 'time_since_restore': 5.363144397735596, 'timesteps_since_restore': 900, 'iterations_since_restore': 5, 'perf': {'cpu_util_percent': 85.45, 'ram_util_percent': 46.8}, 'callback_ok': True}\n",
            "s_elo_val: 0.5\n",
            "p_0_vs_p_3\n",
            "src_pol.config['lr'] 0.0032363987906864965\n",
            "dest_pol.config['lr'] 0.013653557398208654\n",
            "src_pol.config['lr'] 0.0032363987906864965\n",
            "dest_pol.config['lr'] 0.0032363987906864965\n",
            "s_elo_val: 0.5\n",
            "p_1_vs_p_3\n",
            "src_pol.config['lr'] 0.0032363987906864965\n",
            "dest_pol.config['lr'] 0.010922845918566924\n",
            "src_pol.config['lr'] 0.0032363987906864965\n",
            "dest_pol.config['lr'] 0.010922845918566924\n",
            "s_elo_val: 0.5\n",
            "p_2_vs_p_1\n",
            "src_pol.config['lr'] 0.00873827673485354\n",
            "dest_pol.config['lr'] 0.09206769988834694\n",
            "src_pol.config['lr'] 0.00873827673485354\n",
            "dest_pol.config['lr'] 0.09206769988834694\n",
            "s_elo_val: 0.5\n",
            "p_3_vs_p_1\n",
            "src_pol.config['lr'] 0.00873827673485354\n",
            "dest_pol.config['lr'] 0.0032363987906864965\n",
            "src_pol.config['lr'] 0.00873827673485354\n",
            "dest_pol.config['lr'] 0.00873827673485354\n",
            "s_elo_val: 0.5\n",
            "p_4_vs_p_1\n",
            "src_pol.config['lr'] 0.00873827673485354\n",
            "dest_pol.config['lr'] 0.0613784665922313\n",
            "src_pol.config['lr'] 0.00873827673485354\n",
            "dest_pol.config['lr'] 0.0613784665922313\n",
            "s_elo_val: 0.5000719557836594\n",
            "p_5_vs_p_1\n",
            "src_pol.config['lr'] 0.00873827673485354\n",
            "dest_pol.config['lr'] 0.04910277327378504\n",
            "src_pol.config['lr'] 0.00873827673485354\n",
            "dest_pol.config['lr'] 0.04910277327378504\n",
            "on_train_result agt_store {'agt_0': {'hyperparameters': {'lr': [0.008907753822295032, 0.0025891190325491972], 'gamma': [0.9766025199359999, 0.9969770735748194]}, 'score': [0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, -2.0, -1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -3.0, 0.0, 3.0, 1.0, 2.0, 1.0, -5.0, -2.0, -3.0, 3.0, 0.0, 1.0, 1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_1': {'hyperparameters': {'lr': [0.009876705293843066, 0.00873827673485354], 'gamma': [0.9969770735748194, 0.9969770735748194]}, 'score': [0, -2.0, -1.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, -2.0, 1.0, 0.0, 4.0, 3.0, 5.0, 3.0, -1.0, -1.0, 0.0, 1.0, 6.0, 2.0, 2.0, -1.0, -3.0, 1.0, -1.0, 0.0, 3.0, 2.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0, -2.0, -1.0, -1.0, 2.0, -3.0, 1.0, 2.0, 0.0, 3.0, 4.0, 4.0, -3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05, -0.05, 0.0, -0.05, -0.05, -0.05, 0.05], 'step': [0]}, 'agt_2': {'hyperparameters': {'lr': [0.008755722472524574, 0.11048123986601632], 'gamma': [0.9562289739239331, 0.9613710712783627]}, 'score': [0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 0.0, -3.0, -2.0, -1.0, -5.0, -1.0, -1.0, 2.0, 1.0, 0.0, -1.0, 0.0, -4.0, 0.0, -4.0, -1.0, 1.0, 0.0, -1.0, -6.0, -2.0, -2.0, 1.0, 3.0, -1.0, 1.0, 0.0, -3.0, -2.0, -1.0, -1.0, -1.0, 1.0, 0.0, -1.0], 'rating': [0.0, -0.05, 0.05, -0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.0, 0.05, 0.0, 0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.0, 0.05], 'step': [0]}, 'agt_3': {'hyperparameters': {'lr': [0.009351876078460924, 0.010485932081824249], 'gamma': [0.9650823105855924, 0.9969770735748194]}, 'score': [0, 1.0, -1.0, -2.0, -1.0, 1.0, 3.0, 0.0, -3.0, -1.0, -2.0, -1.0, 5.0, 2.0, 3.0, -3.0, 0.0, -1.0, -1.0], 'rating': [0.0, -0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, -0.05, 0.05, 0.0, 0.05, 0.05], 'step': [0]}, 'agt_4': {'hyperparameters': {'lr': [0.006340266341324486, 0.022229762316246113], 'gamma': [0.9597670369809892, 0.9272791368236576]}, 'score': [0, 2.0, 1.0, -4.0, -3.0, -2.0, -1.0, -2.0, -1.0, -3.0, 2.0, -1.0, 0.0, -4.0, -3.0, -5.0, -3.0, 1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 3.0, -1.0, -2.0, 0.0, -3.0, -4.0, -4.0, 3.0], 'rating': [0.0, -0.05, -0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -0.05, 0.05, 0.0, 0.05, 0.05, 0.05, 0.05, -0.05, -0.05, 0.05, 0.05, 0.05, -0.05, 0.05, -0.05, -0.05, 0.0, -0.05, -0.05, -0.05, 0.05], 'step': [0]}, 'agt_5': {'hyperparameters': {'lr': [0.0006377526143306553, 0.058923327928542046], 'gamma': [0.9419988557225905, 0.9613710712783627]}, 'score': [0], 'rating': [0.0], 'step': [0]}}\n",
            "on_train_result g_helper.get_pair.remote() ('agt_4', 'agt_5')\n",
            "training loop = 5 of 5\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-06-10_04-01-41\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 90\n",
            "experiment_id: 9eabf5102e0a4e61bedb232b4a8ed663\n",
            "hostname: 685afc1c1019\n",
            "info:\n",
            "  learn_time_ms: 1052.144\n",
            "  learner:\n",
            "    p_1:\n",
            "      allreduce_latency: 0.0037633048163519967\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.009876705293843066\n",
            "      entropy: 0.5852200587590536\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.1115152310166094\n",
            "      policy_loss: 0.08401356264948845\n",
            "      total_loss: 3.1037884950637817\n",
            "      vf_explained_var: -0.0628904402256012\n",
            "      vf_loss: 2.9974718358781605\n",
            "    p_2:\n",
            "      allreduce_latency: 0.006133635838826497\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.008755722472524574\n",
            "      entropy: 0.8468038439750671\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.1562828520933787\n",
            "      policy_loss: 0.048892332861820854\n",
            "      total_loss: 1.9805156389872234\n",
            "      vf_explained_var: -0.34553372859954834\n",
            "      vf_loss: 1.9003667831420898\n",
            "    p_4:\n",
            "      allreduce_latency: 0.00467371940612793\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.006340266341324486\n",
            "      entropy: 0.9205128848552704\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.14377869168917337\n",
            "      policy_loss: 0.07952229709674914\n",
            "      total_loss: 3.3840961853663125\n",
            "      vf_explained_var: -0.2437644600868225\n",
            "      vf_loss: 3.2758182485898337\n",
            "  num_steps_sampled: 900\n",
            "  num_steps_trained: 900\n",
            "  sync_weights_down_time: 11.039\n",
            "  sync_weights_up_time: 5.902\n",
            "iterations_since_restore: 5\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "optimizer_steps_this_iter: 1\n",
            "perf:\n",
            "  cpu_util_percent: 85.45\n",
            "  ram_util_percent: 46.8\n",
            "pid: 4535\n",
            "policy_reward_max:\n",
            "  p_0: 5.0\n",
            "  p_1: 6.0\n",
            "  p_2: 3.0\n",
            "  p_3: 5.0\n",
            "  p_4: 3.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.38095238095238093\n",
            "  p_1: 0.9583333333333334\n",
            "  p_2: -0.7142857142857143\n",
            "  p_3: -0.05555555555555555\n",
            "  p_4: -1.0333333333333334\n",
            "policy_reward_min:\n",
            "  p_0: -5.0\n",
            "  p_1: -3.0\n",
            "  p_2: -6.0\n",
            "  p_3: -3.0\n",
            "  p_4: -5.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.16600784790448223\n",
            "  mean_inference_ms: 3.581510012295573\n",
            "  mean_processing_ms: 4.5406135808196115\n",
            "time_since_restore: 5.363144397735596\n",
            "time_this_iter_s: 1.0972306728363037\n",
            "time_total_s: 5.363144397735596\n",
            "timestamp: 1591761701\n",
            "timesteps_since_restore: 900\n",
            "timesteps_this_iter: 180\n",
            "timesteps_total: 900\n",
            "training_iteration: 5\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}