{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PBT_MARL_water_down.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwD3_kI2HDbA",
        "colab_type": "text"
      },
      "source": [
        "#Setup Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyAKAl49kg7I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca885406-9dc2-4513-e26d-5cebe46165ed"
      },
      "source": [
        "from google.colab import drive \n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "%cd \"/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down/\"\n",
        "!pwd\n",
        "!ls -l\n",
        "\n",
        "# Install if you haven't done so.\n",
        "#!pip install tensorflow==2.3.0\n",
        "!pip install lz4\n",
        "!pip install 'ray[tune]'\n",
        "!pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl\n",
        "#!pip install ray[rllib]==0.8.6\n",
        "\n",
        "#!pip show tensorflow\n",
        "#!pip show ray\n",
        "#!cat /etc/os-release\n",
        "\n",
        "#!rm -rf ~/ray_results/DDPPO_*\n",
        "#!rm -rf ~/ray_results/PPO_*"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down\n",
            "/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down\n",
            "total 374\n",
            "drwx------ 14 root root   4096 Aug 24 15:12 chkpt\n",
            "-rw-------  1 root root   3139 Jun 11 03:59 Helper.py\n",
            "-rw-------  1 root root      1 Jun 11 03:59 __init__.py\n",
            "-rw-------  1 root root   1072 Jun 10 04:55 LICENSE\n",
            "-rw-------  1 root root   6543 Aug 24 15:06 PBT_MARL.py\n",
            "-rw-------  1 root root   9014 Jun 10 17:23 pbt_marl_water_down_cpu_only.py\n",
            "-rw-------  1 root root 341977 Aug 24 15:12 PBT_MARL_water_down.ipynb\n",
            "drwx------  2 root root   4096 Aug 24 15:06 __pycache__\n",
            "drwx------  2 root root   4096 Aug 24 15:12 ray_results\n",
            "-rw-------  1 root root   3710 Aug  4 08:18 README.md\n",
            "-rw-------  1 root root   2056 Aug  4 08:04 RockPaperScissorsEnv.py\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: ray[tune] in /usr/local/lib/python3.6/dist-packages (0.9.0.dev0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (3.6.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (0.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (3.13)\n",
            "Requirement already satisfied: redis<3.5.0,>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (2.23.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (2.6.0)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (3.12.4)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (0.7.10)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (2.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (3.0.12)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (0.3.3)\n",
            "Requirement already satisfied: aioredis in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (1.0.0)\n",
            "Requirement already satisfied: gpustat in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (0.6.0)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (1.31.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (7.1.2)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (0.5.4)\n",
            "Requirement already satisfied: tabulate; extra == \"tune\" in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (0.8.7)\n",
            "Requirement already satisfied: tensorboardX; extra == \"tune\" in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (2.1)\n",
            "Requirement already satisfied: pandas; extra == \"tune\" in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (1.0.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[tune]) (19.3.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[tune]) (3.0.4)\n",
            "Requirement already satisfied: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[tune]) (4.7.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[tune]) (3.7.4.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[tune]) (1.5.1)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[tune]) (1.1.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[tune]) (3.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ray[tune]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ray[tune]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ray[tune]) (2020.6.20)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray[tune]) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray[tune]) (49.2.0)\n",
            "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from opencensus->ray[tune]) (1.16.0)\n",
            "Requirement already satisfied: opencensus-context==0.1.1 in /usr/local/lib/python3.6/dist-packages (from opencensus->ray[tune]) (0.1.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray[tune]) (4.6.3)\n",
            "Requirement already satisfied: hiredis in /usr/local/lib/python3.6/dist-packages (from aioredis->ray[tune]) (1.1.0)\n",
            "Requirement already satisfied: blessings>=1.6 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray[tune]) (1.7)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from gpustat->ray[tune]) (5.4.8)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray[tune]) (7.352.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas; extra == \"tune\"->ray[tune]) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas; extra == \"tune\"->ray[tune]) (2018.9)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (1.17.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (1.52.0)\n",
            "Requirement already satisfied: contextvars; python_version >= \"3.6\" and python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from opencensus-context==0.1.1->opencensus->ray[tune]) (2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (4.1.1)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars; python_version >= \"3.6\" and python_version < \"3.7\"->opencensus-context==0.1.1->opencensus->ray[tune]) (0.14)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (0.4.8)\n",
            "Collecting ray==0.9.0.dev0\n",
            "  Using cached https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.28.1 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (1.31.0)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: google in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: py-spy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: opencensus in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.7.10)\n",
            "Requirement already satisfied, skipping upgrade: prometheus-client>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: gpustat in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: redis<3.5.0,>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: colorama in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.4.3)\n",
            "Requirement already satisfied, skipping upgrade: colorful in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.5.4)\n",
            "Requirement already satisfied, skipping upgrade: aioredis in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: aiohttp in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.6.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from grpcio>=1.28.1->ray==0.9.0.dev0) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray==0.9.0.dev0) (4.6.3)\n",
            "Requirement already satisfied, skipping upgrade: opencensus-context==0.1.1 in /usr/local/lib/python3.6/dist-packages (from opencensus->ray==0.9.0.dev0) (0.1.1)\n",
            "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from opencensus->ray==0.9.0.dev0) (1.16.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray==0.9.0.dev0) (49.2.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ray==0.9.0.dev0) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ray==0.9.0.dev0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ray==0.9.0.dev0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->ray==0.9.0.dev0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: blessings>=1.6 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray==0.9.0.dev0) (1.7)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray==0.9.0.dev0) (7.352.0)\n",
            "Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.6/dist-packages (from gpustat->ray==0.9.0.dev0) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: async-timeout in /usr/local/lib/python3.6/dist-packages (from aioredis->ray==0.9.0.dev0) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: hiredis in /usr/local/lib/python3.6/dist-packages (from aioredis->ray==0.9.0.dev0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (3.7.4.2)\n",
            "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (1.5.1)\n",
            "Requirement already satisfied, skipping upgrade: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (4.7.6)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (19.3.0)\n",
            "Requirement already satisfied, skipping upgrade: contextvars; python_version >= \"3.6\" and python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from opencensus-context==0.1.1->opencensus->ray==0.9.0.dev0) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray==0.9.0.dev0) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray==0.9.0.dev0) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray==0.9.0.dev0) (1.52.0)\n",
            "Requirement already satisfied, skipping upgrade: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars; python_version >= \"3.6\" and python_version < \"3.7\"->opencensus-context==0.1.1->opencensus->ray==0.9.0.dev0) (0.14)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray==0.9.0.dev0) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray==0.9.0.dev0) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray==0.9.0.dev0) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray==0.9.0.dev0) (0.4.8)\n",
            "Installing collected packages: ray\n",
            "  Found existing installation: ray 0.9.0.dev0\n",
            "    Uninstalling ray-0.9.0.dev0:\n",
            "      Successfully uninstalled ray-0.9.0.dev0\n",
            "Successfully installed ray-0.9.0.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BMjPt0fbNSf",
        "colab_type": "text"
      },
      "source": [
        "#Chkpt/restore & log path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQMyQcPpbIai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g_drive_path = \"/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down/\"\n",
        "\n",
        "local_dir = g_drive_path + \"chkpt/\"\n",
        "chkpt_freq = 10\n",
        "chkpt = 150\n",
        "restore_path = \"{}checkpoint_{}/checkpoint-{}\".format(local_dir, chkpt, chkpt)\n",
        "is_restore = False\n",
        "\n",
        "log_dir = g_drive_path + \"ray_results/\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-GBqoxsHBZV",
        "colab_type": "text"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8DRdL7tgKBr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2898b69c-d35c-43d2-bbea-f10bc172561c"
      },
      "source": [
        "from collections import defaultdict\n",
        "from typing import Dict\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from gym.spaces import Discrete\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune.logger import pretty_print\n",
        "\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.models import ModelCatalog\n",
        "\n",
        "from ray.rllib.policy import Policy\n",
        "from ray.rllib.policy.torch_policy import LearningRateSchedule, EntropyCoeffSchedule\n",
        "\n",
        "from ray.rllib.agents.ppo.ppo_torch_policy import PPOTorchPolicy, KLCoeffMixin, ValueNetworkMixin\n",
        "from ray.rllib.agents.ppo import ppo\n",
        "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
        "from ray.rllib.agents.ppo import appo\n",
        "from ray.rllib.agents.ppo.appo import APPOTrainer\n",
        "from ray.rllib.agents.ppo import ddppo\n",
        "from ray.rllib.agents.ppo.ddppo import DDPPOTrainer\n",
        "\n",
        "from ray.rllib.env import BaseEnv\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "\n",
        "from ray.rllib.policy.sample_batch import SampleBatch\n",
        "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
        "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
        "\n",
        "from ray.rllib.utils.schedules import ConstantSchedule\n",
        "from ray.rllib.utils import try_import_tf\n",
        "tf = try_import_tf()\n",
        "\n",
        "from RockPaperScissorsEnv import RockPaperScissorsEnv\n",
        "from Helper import Helper\n",
        "from PBT_MARL import PBT_MARL"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFRqbhqAunwM",
        "colab_type": "text"
      },
      "source": [
        "#Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBp3zwiEuqM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"#Callbacks\"\"\"\n",
        "\n",
        "class MyCallbacks(DefaultCallbacks):\n",
        "    def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                         policies: Dict[str, Policy],\n",
        "                         episode: MultiAgentEpisode, **kwargs):\n",
        "        #print(\"on_episode_start {}, _agent_to_policy {}\".format(episode.episode_id, episode._agent_to_policy))\n",
        "        #episode.hist_data[\"episode_id\"] = []\n",
        "        pass\n",
        "\n",
        "    def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                        episode: MultiAgentEpisode, **kwargs):\n",
        "        \"\"\"\n",
        "        pole_angle = abs(episode.last_observation_for()[2])\n",
        "        raw_angle = abs(episode.last_raw_obs_for()[2])\n",
        "        assert pole_angle == raw_angle\n",
        "        episode.user_data[\"pole_angles\"].append(pole_angle)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
        "                       **kwargs):\n",
        "        #print(\"on_episode_end {}, episode.agent_rewards {}\".format(episode.episode_id, episode.agent_rewards))\n",
        "\n",
        "        player_policy = []\n",
        "        score = []\n",
        "        for k,v in episode.agent_rewards.items():\n",
        "            player_policy.append(k)\n",
        "            score.append(v)\n",
        "\n",
        "        pol_i_key = player_policy[0][1]\n",
        "        pol_j_key = player_policy[1][1]\n",
        "        _, str_i = pol_i_key.split(\"_\")\n",
        "        _, str_j = pol_j_key.split(\"_\")\n",
        "        agt_i_key = \"agt_\" + str_i\n",
        "        agt_j_key = \"agt_\" + str_j\n",
        "\n",
        "        g_helper = ray.get_actor(\"g_helper\")     \n",
        "        prev_rating_i = ray.get(g_helper.get_rating.remote(agt_i_key))\n",
        "        prev_rating_j = ray.get(g_helper.get_rating.remote(agt_j_key))\n",
        "        score_i = score[0]\n",
        "        score_j = score[1]\n",
        "        rating_i, rating_j = l_PBT_MARL.compute_rating(prev_rating_i, prev_rating_j, score_i, score_j)\n",
        "        ray.get(g_helper.update_rating.remote(agt_i_key, agt_j_key, rating_i, rating_j, score_i, score_j))\n",
        "        #print(\"on_episode_end ray.get(g_helper.get_agt_store.remote())\", ray.get(g_helper.get_agt_store.remote()))\n",
        "\n",
        "    def on_sample_end(self, worker: RolloutWorker, samples: SampleBatch,\n",
        "                      **kwargs):\n",
        "        #print(\"on_sample_end returned sample batch of size {}\".format(samples.count))\n",
        "        pass\n",
        "\n",
        "    def on_train_result(self, trainer, result: dict, **kwargs):\n",
        "        print(\"trainer.train() result: {} -> {} episodes\".format(trainer, result[\"episodes_this_iter\"]))\n",
        "        # you can mutate the result dict to add new fields to return\n",
        "        result[\"callback_ok\"] = True\n",
        "        #print(\"on_train_result result\", result)\n",
        "\n",
        "        l_PBT_MARL.PBT(trainer)     # perform PBT\n",
        "\n",
        "        g_helper = ray.get_actor(\"g_helper\")     \n",
        "        ray.get(g_helper.set_pair.remote())     # set the lastest pair\n",
        "        #print(\"on_train_result g_helper.get_pair.remote()\", ray.get(g_helper.get_pair.remote()))\n",
        "\n",
        "\n",
        "        #lr_0 = np.random.rand()\n",
        "        #lr_1 = lr_0 + 0.1\n",
        "        #for w in trainer.workers.remote_workers():\n",
        "            #w.foreach_policy.remote(lambda p, p_id: p.update_lr_schedule(i))  \n",
        "            #w.for_policy.remote(lambda p: p.update_lr_schedule(lr_0), \"p_0\")  \n",
        "            #w.for_policy.remote(lambda p: p.update_lr_schedule(lr_1), \"p_1\") \n",
        "\n",
        "\n",
        "    def on_postprocess_trajectory(\n",
        "            self, worker: RolloutWorker, episode: MultiAgentEpisode,\n",
        "            agent_id: str, policy_id: str, policies: Dict[str, Policy],\n",
        "            postprocessed_batch: SampleBatch,\n",
        "            original_batches: Dict[str, SampleBatch], **kwargs):\n",
        "        #\u0010print(\"postprocessed {}, {}, {}, {} steps\".format(episode, agent_id, policy_id, postprocessed_batch.count))              \n",
        "        \"\"\"\n",
        "        if \"num_batches\" not in episode.custom_metrics:\n",
        "            episode.custom_metrics[\"num_batches\"] = 0\n",
        "        episode.custom_metrics[\"num_batches\"] += 1\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i492GjpyrGNy",
        "colab_type": "text"
      },
      "source": [
        "#Mixin for extending policy & trainer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Ww51NXrBEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class My_Mixin:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        #self.curr_lr = lr\n",
        "                \n",
        "    def update_lr_schedule(self, lr):        \n",
        "        self.lr_schedule = ConstantSchedule(lr, framework=None)  \n",
        "        print(\"update_lr_schedule, lr={}\".format(lr))\n",
        "\n",
        "    def update_gamma(self, gamma):        \n",
        "        self.gamma = gamma\n",
        "        print(\"update_gamma, gamma={}\".format(gamma))\n",
        "\n",
        "def setup_mixins(policy, obs_space, action_space, config):\n",
        "    # Copied from PPO\n",
        "    ValueNetworkMixin.__init__(policy, obs_space, action_space, config)\n",
        "    KLCoeffMixin.__init__(policy, config)\n",
        "    EntropyCoeffSchedule.__init__(policy, config[\"entropy_coeff\"],\n",
        "                                  config[\"entropy_coeff_schedule\"])\n",
        "    LearningRateSchedule.__init__(policy, config[\"lr\"], config[\"lr_schedule\"])  \n",
        "\n",
        "CustomPolicy = PPOTorchPolicy.with_updates(\n",
        "    name=\"Custom_Policy\",\n",
        "    before_init=setup_mixins,\n",
        "    mixins=[\n",
        "        LearningRateSchedule, EntropyCoeffSchedule, KLCoeffMixin,\n",
        "        ValueNetworkMixin, \n",
        "        My_Mixin\n",
        "    ])\n",
        "\n",
        "def get_policy_class(config):\n",
        "    return CustomPolicy\n",
        "\n",
        "CustomTrainer = DDPPOTrainer.with_updates(name=\"Custom_Trainer\",\n",
        "                                          default_policy=CustomPolicy,\n",
        "                                          get_policy_class=get_policy_class,\n",
        "                                          )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpcJGyAaBbc2",
        "colab_type": "text"
      },
      "source": [
        "#Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMZ20pVCzxUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_policies(population_size, obs_space, act_space, use_lstm, hyperparameters_range):\n",
        "    \"\"\"\n",
        "    Sample hyper-parameter from the hyper-parameter distribution.\n",
        "    \"\"\"\n",
        "    policies = {}\n",
        "    for i in range(population_size):\n",
        "        pol_key = \"p_\" + str(i)\n",
        "        lr = np.random.uniform(low=hyperparameters_range[\"lr\"][0], high=hyperparameters_range[\"lr\"][1], size=None)\n",
        "        gamma = np.random.uniform(low=hyperparameters_range[\"gamma\"][0], high=hyperparameters_range[\"gamma\"][1], size=None)\n",
        "        policies[pol_key] = (None, obs_space, act_space, {\"model\": {\"use_lstm\": use_lstm},\n",
        "                                                          \"lr\": lr,\n",
        "                                                          \"gamma\": gamma})\n",
        "    return policies\n",
        "\n",
        "def train_policies(population_size):    \n",
        "    train_policies = []\n",
        "    for i in range(population_size):\n",
        "        pol_key = \"p_\" + str(i)\n",
        "        train_policies.append(pol_key)\n",
        "\n",
        "    return policies\n",
        "\n",
        "def select_policy(agent_id):\n",
        "    _, i = agent_id.split(\"_\")\n",
        "    policy = \"p_\" + str(i)\n",
        "    #print(\"select_policy {} {}\".format(agent_id , policy))\n",
        "    return policy     "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAEySBSfBS5u",
        "colab_type": "text"
      },
      "source": [
        "#Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trdlnMoHwbfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "edc93e73-b1a2-4d0d-8f61-fbb91bb945bf"
      },
      "source": [
        "population_size = 6\n",
        "K = 0.1     \n",
        "T_select = 0.77 #0.47\n",
        "binomial_n = 1\n",
        "inherit_prob = 0.5\n",
        "perturb_prob = 0.1\n",
        "perturb_val = [0.8, 1.2]\n",
        "hyperparameters_range = {\"lr\": [1e-7, 1e-1], \n",
        "                         \"gamma\": [0.9, 0.999]}\n",
        "\n",
        "register_env(\"RockPaperScissorsEnv\", lambda _: RockPaperScissorsEnv(_, population_size))     # register RockPaperScissorsEnv with RLlib     \n",
        "# get obs & act spaces from dummy CDA env\n",
        "dummy_env = RockPaperScissorsEnv(_, population_size=0)\n",
        "obs_space = dummy_env.observation_space\n",
        "act_space = dummy_env.action_space\n",
        "\n",
        "use_lstm=False\n",
        "policies = init_policies(population_size, obs_space, act_space, use_lstm, hyperparameters_range)\n",
        "train_policies = train_policies(population_size)\n",
        "\n",
        "l_PBT_MARL = PBT_MARL(population_size, \n",
        "                      K, T_select, \n",
        "                      binomial_n, inherit_prob,\n",
        "                      perturb_prob, perturb_val)\n",
        "\n",
        "ray.shutdown()\n",
        "#ray.init(ignore_reinit_error=True, log_to_driver=True, webui_host='127.0.0.1', num_cpus=2, num_gpus=1)      #start ray\n",
        "ray.init(ignore_reinit_error=True, log_to_driver=True, num_cpus=2, num_gpus=1)      #start ray\n",
        "#print(\"ray.nodes()\", ray.nodes())\n",
        "\n",
        "g_helper = Helper.options(name=\"g_helper\").remote(population_size, policies)      # this object runs on a different ray actor process\n",
        "ray.get(g_helper.set_pair.remote())\n",
        "\n",
        "num_iters = 30     # num of main training loop"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-24 15:13:56,636\tINFO resource_spec.py:250 -- Starting Ray with 7.13 GiB memory available for workers and up to 3.59 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
            "2020-08-24 15:13:57,189\tINFO services.py:1207 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyucHLoqBe5G",
        "colab_type": "text"
      },
      "source": [
        "#Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNWNnavQt9y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_config():\n",
        "    config = ddppo.DEFAULT_CONFIG.copy()\n",
        "\n",
        "    config[\"env\"] = RockPaperScissorsEnv\n",
        "    config[\"multiagent\"] = {\"policies_to_train\": train_policies,\n",
        "                            \"policies\": policies,\n",
        "                            \"policy_mapping_fn\": select_policy}        \n",
        "    config[\"num_cpus_per_worker\"] = 0.25                                \n",
        "    config[\"num_gpus_per_worker\"] = 0.125\n",
        "    config[\"num_workers\"] = 2      \n",
        "    config[\"num_envs_per_worker\"] = 3\n",
        "    config[\"rollout_fragment_length\"] = 30                  \n",
        "    #config[\"train_batch_size\"] = -1     # must be -1 for DDPPO trainer \n",
        "    config[\"sgd_minibatch_size\"] = 10                       \n",
        "    config[\"num_sgd_iter\"] = 3      # number of epochs to execute per train batch.\n",
        "    config[\"callbacks\"] = MyCallbacks\n",
        "    config[\"log_level\"] = \"WARN\"      # WARN/INFO/DEBUG \n",
        "    config[\"output\"] = log_dir\n",
        "\n",
        "    return config"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb_4cEGdBlqf",
        "colab_type": "text"
      },
      "source": [
        "#Go train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUB40TYSuDAn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "74410745-733b-41e6-9f7e-90d9e4a6d1f6"
      },
      "source": [
        "def go_train(config):     \n",
        "    #trainer = ddppo.DDPPOTrainer(config=config, env=\"RockPaperScissorsEnv\")         \n",
        "    #trainer = ppo.PPOTrainer(config=config, env=\"RockPaperScissorsEnv\")         \n",
        "    trainer = CustomTrainer(config=get_config(), env=\"RockPaperScissorsEnv\")         \n",
        "\n",
        "    if is_restore == True:\n",
        "        trainer.restore(restore_path) \n",
        "    \n",
        "    result = None\n",
        "    for i in range(num_iters):\n",
        "        result = trainer.train()       \n",
        "        print(\"training loop = {} of {}\".format(i + 1, num_iters))            \n",
        "        print(pretty_print(result))     # includes result[\"custom_metrics\"]\n",
        "\n",
        "        #p_0 = trainer.get_policy('p_0')\n",
        "        #p_0.lr_schedule = ConstantSchedule(0.3, framework=None)\n",
        "\n",
        "        if i % chkpt_freq == 0:\n",
        "            checkpoint = trainer.save(local_dir)\n",
        "            print(\"checkpoint saved at\", checkpoint)\n",
        "    \n",
        "    checkpoint = trainer.save(local_dir)\n",
        "    print(\"checkpoint saved at\", checkpoint)\n",
        "    \n",
        "\n",
        "# run everything\n",
        "go_train(get_config())    \n",
        "\n",
        "ray.shutdown()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-24 15:14:00,294\tINFO trainer.py:637 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "2020-08-24 15:14:00,346\tWARNING worker.py:413 -- ray.get_gpu_ids() will return a list of strings by default in a future version of Ray for compatibility with CUDA. To enable the forward-compatible behavior, use `ray.get_gpu_ids(as_str=True)`.\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m 2020-08-24 15:14:05,139\tWARNING worker.py:413 -- ray.get_gpu_ids() will return a list of strings by default in a future version of Ray for compatibility with CUDA. To enable the forward-compatible behavior, use `ray.get_gpu_ids(as_str=True)`.\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/torch_ops.py:149: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
            "2020-08-24 15:14:12,634\tINFO trainable.py:256 -- Trainable.setup took 12.342 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2020-08-24 15:14:12,635\tWARNING util.py:38 -- Install gputil for GPU system monitoring.\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/torch_ops.py:149: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
            "/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/torch_ops.py:149: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  tensor = torch.from_numpy(np.asarray(item))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06576497337616342\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9258009533049822\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06576497337616342\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9258009533049822\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.09903073952955821\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9818462317202308\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.09903073952955821\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9818462317202308\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.023657407418437307\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9416004212124441\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.023657407418437307\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9416004212124441\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06576497337616342\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9258009533049822\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06576497337616342\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.04221434130672636\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9258009533049822\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9292100323508685\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.04221434130672636\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9292100323508685\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07922459162364658\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9818462317202308\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07922459162364658\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9818462317202308\n",
            "training loop = 1 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-13\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 18\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_2:\n",
            "      allreduce_latency: 0.005369292365180122\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.01971450618203109\n",
            "      entropy: 0.48932846387227374\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2052558064460754\n",
            "      model: {}\n",
            "      policy_loss: 0.2639572603835\n",
            "      total_loss: 5.151276184452905\n",
            "      vf_explained_var: -0.2479480654001236\n",
            "      vf_loss: 4.646267824702793\n",
            "    p_4:\n",
            "      allreduce_latency: 0.005604267120361328\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.08252561627463184\n",
            "      entropy: 0.20745782409277227\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 5.49745171599918\n",
            "      model: {}\n",
            "      policy_loss: 0.3407280060928315\n",
            "      total_loss: 8.10174102253384\n",
            "      vf_explained_var: -0.17545372247695923\n",
            "      vf_loss: 6.661522560649448\n",
            "  num_steps_sampled: 180\n",
            "  num_steps_trained: 180\n",
            "iterations_since_restore: 1\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 88.1\n",
            "  ram_util_percent: 48.650000000000006\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_2: 7.0\n",
            "  p_4: 7.0\n",
            "policy_reward_mean:\n",
            "  p_2: -1.2777777777777777\n",
            "  p_4: 1.2777777777777777\n",
            "policy_reward_min:\n",
            "  p_2: -7.0\n",
            "  p_4: -7.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.24138342949651903\n",
            "  mean_env_wait_ms: 0.39385595629292147\n",
            "  mean_inference_ms: 4.557471121511152\n",
            "  mean_raw_obs_processing_ms: 2.8289287321029173\n",
            "time_since_restore: 1.2327754497528076\n",
            "time_this_iter_s: 1.2327754497528076\n",
            "time_total_s: 1.2327754497528076\n",
            "timers:\n",
            "  learn_time_ms: 1214.73\n",
            "timestamp: 1598282053\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 180\n",
            "training_iteration: 1\n",
            "\n",
            "checkpoint saved at /content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down/chkpt/checkpoint_1/checkpoint-1\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.037917982141169516\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9656553629369333\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.037917982141169516\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9656553629369333\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07922459162364658\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9818462317202308\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07922459162364658\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9818462317202308\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0401454559186473\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9816708669766435\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0401454559186473\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9816708669766435\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.08852163865795813\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.08852163865795813\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.03211636473491784\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9816708669766435\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.03211636473491784\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9816708669766435\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.025693091787934275\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9816708669766435\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.025693091787934275\n",
            "training loop = 2 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-15\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 36\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_2:\n",
            "      allreduce_latency: 0.004616578420003255\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.01971450618203109\n",
            "      entropy: 0.08612149705489476\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.19485787178079286\n",
            "      model: {}\n",
            "      policy_loss: 0.09414022689452395\n",
            "      total_loss: 8.003609339396158\n",
            "      vf_explained_var: 0.009334187023341656\n",
            "      vf_loss: 7.870498021443685\n",
            "    p_3:\n",
            "      allreduce_latency: 0.004516164461771647\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.0548041444801362\n",
            "      entropy: 0.15278766490519047\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 4.281496087710063\n",
            "      model: {}\n",
            "      policy_loss: 0.4073116679986318\n",
            "      total_loss: 3.291194756825765\n",
            "      vf_explained_var: -0.11257121711969376\n",
            "      vf_loss: 2.0275837779045105\n",
            "    p_4:\n",
            "      allreduce_latency: 0.003695540957980686\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.08252561627463184\n",
            "      entropy: 0.00032435174303676223\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.6982259998718897\n",
            "      model: {}\n",
            "      policy_loss: -0.00039092782470915053\n",
            "      total_loss: 5.833543088701036\n",
            "      vf_explained_var: -0.08174919337034225\n",
            "      vf_loss: 5.694288730621338\n",
            "  num_steps_sampled: 360\n",
            "  num_steps_trained: 360\n",
            "iterations_since_restore: 2\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.76666666666667\n",
            "  ram_util_percent: 48.76666666666667\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_2: 7.0\n",
            "  p_3: 4.0\n",
            "  p_4: 10.0\n",
            "policy_reward_mean:\n",
            "  p_2: -2.0416666666666665\n",
            "  p_3: -0.16666666666666666\n",
            "  p_4: 1.4166666666666667\n",
            "policy_reward_min:\n",
            "  p_2: -10.0\n",
            "  p_3: -5.0\n",
            "  p_4: -7.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.22463146685797467\n",
            "  mean_env_wait_ms: 0.31053149971238275\n",
            "  mean_inference_ms: 4.412718641259946\n",
            "  mean_raw_obs_processing_ms: 2.894455512827001\n",
            "time_since_restore: 2.292965888977051\n",
            "time_this_iter_s: 1.0601904392242432\n",
            "time_total_s: 2.292965888977051\n",
            "timers:\n",
            "  learn_time_ms: 1131.243\n",
            "timestamp: 1598282055\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 360\n",
            "training_iteration: 2\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9816708669766435\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.007141050810484645\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9150183900396838\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.007141050810484645\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.030831710145521128\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9150183900396838\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9816708669766435\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.030831710145521128\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9816708669766435\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.025693091787934275\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9816708669766435\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.025693091787934275\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9816708669766435\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0708173109263665\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0708173109263665\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.005712840648387716\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9150183900396838\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.005712840648387716\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9150183900396838\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.030831710145521128\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9816708669766435\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.030831710145521128\n",
            "training loop = 3 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-17\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 54\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_1:\n",
            "      allreduce_latency: 0.004027525583902995\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.09903073952955821\n",
            "      entropy: 0.0451666406949203\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 19.89337412516276\n",
            "      model: {}\n",
            "      policy_loss: 0.3098849014689525\n",
            "      total_loss: 19.471464157104492\n",
            "      vf_explained_var: -0.40956178307533264\n",
            "      vf_loss: 15.182904322942099\n",
            "    p_3:\n",
            "      allreduce_latency: 0.006038268407185872\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.06576497337616342\n",
            "      entropy: 1.1876622503829518e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 3.2321086724599204\n",
            "      model: {}\n",
            "      policy_loss: 0.040261310835679374\n",
            "      total_loss: 1.1649259726206462\n",
            "      vf_explained_var: -0.1373007446527481\n",
            "      vf_loss: 0.47824283440907794\n",
            "    p_4:\n",
            "      allreduce_latency: 0.004896402359008789\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.04221434130672636\n",
            "      entropy: 0.017228996381163597\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0019555973509947457\n",
            "      model: {}\n",
            "      policy_loss: -0.0013073161244392395\n",
            "      total_loss: 1.2248393495877583\n",
            "      vf_explained_var: -0.5902817845344543\n",
            "      vf_loss: 1.2257554928461711\n",
            "    p_5:\n",
            "      allreduce_latency: 0.0036855141321818032\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.07922459162364658\n",
            "      entropy: 0.4495582232872645\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 2.8675991892814636\n",
            "      model: {}\n",
            "      policy_loss: 0.40804835346837837\n",
            "      total_loss: 5.824692368507385\n",
            "      vf_explained_var: -0.6256397366523743\n",
            "      vf_loss: 4.8431241909662885\n",
            "  num_steps_sampled: 540\n",
            "  num_steps_trained: 540\n",
            "iterations_since_restore: 3\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.39999999999999\n",
            "  ram_util_percent: 48.79999999999999\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_1: 4.0\n",
            "  p_2: 7.0\n",
            "  p_3: 4.0\n",
            "  p_4: 10.0\n",
            "  p_5: 3.0\n",
            "policy_reward_mean:\n",
            "  p_1: 0.6666666666666666\n",
            "  p_2: -2.0416666666666665\n",
            "  p_3: -0.3888888888888889\n",
            "  p_4: 1.3333333333333333\n",
            "  p_5: -0.6666666666666666\n",
            "policy_reward_min:\n",
            "  p_1: -3.0\n",
            "  p_2: -10.0\n",
            "  p_3: -5.0\n",
            "  p_4: -7.0\n",
            "  p_5: -4.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.22031660404329798\n",
            "  mean_env_wait_ms: 0.2636724855168622\n",
            "  mean_inference_ms: 4.3444105884704225\n",
            "  mean_raw_obs_processing_ms: 2.9214382110668775\n",
            "time_since_restore: 3.374328374862671\n",
            "time_this_iter_s: 1.0813624858856201\n",
            "time_total_s: 3.374328374862671\n",
            "timers:\n",
            "  learn_time_ms: 1110.13\n",
            "timestamp: 1598282057\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 540\n",
            "training_iteration: 3\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9816708669766435\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0849807731116398\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0849807731116398\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.054131443815736126\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9138748572985901\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.054131443815736126\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9138748572985901\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06798461848931185\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06798461848931185\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05665384874109321\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05665384874109321\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.002597349506468398\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9198069942198617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.002597349506468398\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05438769479144948\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05438769479144948\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "training loop = 4 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-19\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 72\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_1:\n",
            "      allreduce_latency: 0.004242526160346137\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.07922459162364658\n",
            "      entropy: 0.09682837509737713\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.4053788110613823\n",
            "      model: {}\n",
            "      policy_loss: 0.143939607983662\n",
            "      total_loss: 4.154188725683424\n",
            "      vf_explained_var: -0.2680290639400482\n",
            "      vf_loss: 3.7291734880871243\n",
            "    p_4:\n",
            "      allreduce_latency: 0.005861123402913411\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.03211636473491785\n",
            "      entropy: 0.005694613792002201\n",
            "      entropy_coeff: 0.0\n",
            "      kl: .inf\n",
            "      model: {}\n",
            "      policy_loss: -0.00017911692460378012\n",
            "      total_loss: .inf\n",
            "      vf_explained_var: -0.15996195375919342\n",
            "      vf_loss: 4.296900629997253\n",
            "    p_5:\n",
            "      allreduce_latency: 0.007987022399902344\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.025693091787934275\n",
            "      entropy: 0.40789329012235004\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2303180694580078\n",
            "      model: {}\n",
            "      policy_loss: -0.07653343677520752\n",
            "      total_loss: 6.73459529876709\n",
            "      vf_explained_var: -0.27659186720848083\n",
            "      vf_loss: 6.565064589182536\n",
            "  num_steps_sampled: 720\n",
            "  num_steps_trained: 720\n",
            "iterations_since_restore: 4\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.2\n",
            "  ram_util_percent: 48.79999999999999\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_1: 6.0\n",
            "  p_2: 7.0\n",
            "  p_3: 4.0\n",
            "  p_4: 10.0\n",
            "  p_5: 6.0\n",
            "policy_reward_mean:\n",
            "  p_1: 1.4\n",
            "  p_2: -2.0416666666666665\n",
            "  p_3: -0.3888888888888889\n",
            "  p_4: 0.3148148148148148\n",
            "  p_5: -0.16666666666666666\n",
            "policy_reward_min:\n",
            "  p_1: -6.0\n",
            "  p_2: -10.0\n",
            "  p_3: -5.0\n",
            "  p_4: -7.0\n",
            "  p_5: -4.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.21551963580477096\n",
            "  mean_env_wait_ms: 0.23372059495570932\n",
            "  mean_inference_ms: 4.3079469673483715\n",
            "  mean_raw_obs_processing_ms: 2.9368241418089376\n",
            "time_since_restore: 4.447900056838989\n",
            "time_this_iter_s: 1.0735716819763184\n",
            "time_total_s: 4.447900056838989\n",
            "timers:\n",
            "  learn_time_ms: 1097.474\n",
            "timestamp: 1598282059\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 720\n",
            "training_iteration: 4\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06263560542030018\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06263560542030018\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9243650278119637\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9243650278119637\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06495773257888335\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9138748572985901\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06495773257888335\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07794927909466001\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9138748572985901\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9138748572985901\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07794927909466001\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9138748572985901\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.04532307899287457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.04532307899287457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.003116819407762078\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.003116819407762078\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05438769479144948\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05438769479144948\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "training loop = 5 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-21\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 90\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_1:\n",
            "      allreduce_latency: 0.004171133041381836\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.030831710145521125\n",
            "      entropy: 2.7559785116953663e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 5.935144751371506e-06\n",
            "      model: {}\n",
            "      policy_loss: 4.55180800903084e-06\n",
            "      total_loss: 0.23350757360458374\n",
            "      vf_explained_var: 'null'\n",
            "      vf_loss: 0.23350180437167486\n",
            "    p_3:\n",
            "      allreduce_latency: 0.0051650603612263995\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.0708173109263665\n",
            "      entropy: 0.2028112513750481\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.20674329883574197\n",
            "      model: {}\n",
            "      policy_loss: -0.020420851806799572\n",
            "      total_loss: 2.7523901065190635\n",
            "      vf_explained_var: -0.307153582572937\n",
            "      vf_loss: 2.7314622004826865\n",
            "    p_4:\n",
            "      allreduce_latency: 0.003638585408528646\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.005712840648387716\n",
            "      entropy: 0.002165487656990687\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.00013459566980600357\n",
            "      model: {}\n",
            "      policy_loss: -0.00012232661538291723\n",
            "      total_loss: 1.8268967866897583\n",
            "      vf_explained_var: -0.06732094287872314\n",
            "      vf_loss: 1.8269921938578289\n",
            "    p_5:\n",
            "      allreduce_latency: 0.0036912759145100913\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.03083171014552113\n",
            "      entropy: 0.04867893705765406\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.4127735607326031\n",
            "      model: {}\n",
            "      policy_loss: -0.07679518599373598\n",
            "      total_loss: 2.0807966689268746\n",
            "      vf_explained_var: 0.0132540762424469\n",
            "      vf_loss: 2.075037181377411\n",
            "  num_steps_sampled: 900\n",
            "  num_steps_trained: 900\n",
            "iterations_since_restore: 5\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 79.9\n",
            "  ram_util_percent: 48.8\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_1: 6.0\n",
            "  p_2: 7.0\n",
            "  p_3: 4.0\n",
            "  p_4: 10.0\n",
            "  p_5: 6.0\n",
            "policy_reward_mean:\n",
            "  p_1: 1.3333333333333333\n",
            "  p_2: -2.0416666666666665\n",
            "  p_3: -2.033333333333333\n",
            "  p_4: 0.18333333333333332\n",
            "  p_5: 1.7\n",
            "policy_reward_min:\n",
            "  p_1: -6.0\n",
            "  p_2: -10.0\n",
            "  p_3: -6.0\n",
            "  p_4: -7.0\n",
            "  p_5: -4.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.21206456045530797\n",
            "  mean_env_wait_ms: 0.21236410731141536\n",
            "  mean_inference_ms: 4.284597508938907\n",
            "  mean_raw_obs_processing_ms: 2.9422174795266396\n",
            "time_since_restore: 5.497147560119629\n",
            "time_this_iter_s: 1.0492475032806396\n",
            "time_total_s: 5.497147560119629\n",
            "timers:\n",
            "  learn_time_ms: 1085.162\n",
            "timestamp: 1598282061\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 900\n",
            "training_iteration: 5\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05438769479144948\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05438769479144948\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.051966186063106684\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9138748572985901\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.051966186063106684\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9138748572985901\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0024934555262096625\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0024934555262096625\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.08347135068364346\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9919215284037404\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.08347135068364346\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9919215284037404\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.002992146631451595\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.002992146631451595\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06526523374973937\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06526523374973937\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "training loop = 6 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-23\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 108\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_3:\n",
            "      allreduce_latency: 0.005080037646823459\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.056653848741093214\n",
            "      entropy: 0.06636887581813228\n",
            "      entropy_coeff: 0.0\n",
            "      kl: .inf\n",
            "      model: {}\n",
            "      policy_loss: -0.009706252151065402\n",
            "      total_loss: .inf\n",
            "      vf_explained_var: 0.010415269061923027\n",
            "      vf_loss: 3.6720708211263022\n",
            "    p_5:\n",
            "      allreduce_latency: 0.005436817804972331\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.05438769479144948\n",
            "      entropy: 0.07877138617925539\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.3253692504432466\n",
            "      model: {}\n",
            "      policy_loss: -0.045530855655670166\n",
            "      total_loss: 3.6515842411253185\n",
            "      vf_explained_var: 0.026555512100458145\n",
            "      vf_loss: 3.632041335105896\n",
            "  num_steps_sampled: 1080\n",
            "  num_steps_trained: 1080\n",
            "iterations_since_restore: 6\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 87.53333333333335\n",
            "  ram_util_percent: 48.833333333333336\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_1: 6.0\n",
            "  p_2: 7.0\n",
            "  p_3: 4.0\n",
            "  p_4: 10.0\n",
            "  p_5: 6.0\n",
            "policy_reward_mean:\n",
            "  p_1: 1.3333333333333333\n",
            "  p_2: -2.125\n",
            "  p_3: -3.0\n",
            "  p_4: -0.07692307692307693\n",
            "  p_5: 2.7916666666666665\n",
            "policy_reward_min:\n",
            "  p_1: -6.0\n",
            "  p_2: -10.0\n",
            "  p_3: -6.0\n",
            "  p_4: -7.0\n",
            "  p_5: -4.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2060323026793874\n",
            "  mean_env_wait_ms: 0.1783072900882\n",
            "  mean_inference_ms: 4.22109816524023\n",
            "  mean_raw_obs_processing_ms: 2.94673397366197\n",
            "time_since_restore: 6.59385085105896\n",
            "time_this_iter_s: 1.096703290939331\n",
            "time_total_s: 6.59385085105896\n",
            "timers:\n",
            "  learn_time_ms: 1085.128\n",
            "timestamp: 1598282063\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 1080\n",
            "training_iteration: 6\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0035905759577419135\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0035905759577419135\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.002872460766193531\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.002872460766193531\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0019947644209677303\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0019947644209677303\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0522121869997915\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0522121869997915\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.002393717305161276\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.002393717305161276\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.041769749599833206\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.041769749599833206\n",
            "training loop = 7 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-25\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 126\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_2:\n",
            "      allreduce_latency: 0.003557483355204264\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.07794927909466\n",
            "      entropy: 0.3378210167090098\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.359101543823878\n",
            "      model: {}\n",
            "      policy_loss: 0.0005109803751111031\n",
            "      total_loss: 6.02641765276591\n",
            "      vf_explained_var: 0.10035280138254166\n",
            "      vf_loss: 5.9540863037109375\n",
            "    p_3:\n",
            "      allreduce_latency: 0.003663778305053711\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.04532307899287457\n",
            "      entropy: 0.0003183499842028444\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.442829688390096\n",
            "      model: {}\n",
            "      policy_loss: -0.032264346877733864\n",
            "      total_loss: 3.5962599913279214\n",
            "      vf_explained_var: 0.23196345567703247\n",
            "      vf_loss: 3.539958397547404\n",
            "    p_4:\n",
            "      allreduce_latency: 0.004976868629455566\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.003116819407762078\n",
            "      entropy: 0.110831766622141\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.5055972803384066\n",
            "      model: {}\n",
            "      policy_loss: -0.04069073498249054\n",
            "      total_loss: 7.358305851618449\n",
            "      vf_explained_var: 0.09926000237464905\n",
            "      vf_loss: 7.2978769938151045\n",
            "    p_5:\n",
            "      allreduce_latency: 0.004442453384399414\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.05438769479144948\n",
            "      entropy: 0.001366810862236889\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0845400017258271\n",
            "      model: {}\n",
            "      policy_loss: -0.020805296798547108\n",
            "      total_loss: 3.2980454762776694\n",
            "      vf_explained_var: -0.048474352806806564\n",
            "      vf_loss: 3.3019426663716636\n",
            "  num_steps_sampled: 1260\n",
            "  num_steps_trained: 1260\n",
            "iterations_since_restore: 7\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 83.0\n",
            "  ram_util_percent: 48.79999999999999\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_1: 6.0\n",
            "  p_2: 1.0\n",
            "  p_3: 4.0\n",
            "  p_4: 10.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_1: 1.3333333333333333\n",
            "  p_2: -5.466666666666667\n",
            "  p_3: -3.816326530612245\n",
            "  p_4: 0.9565217391304348\n",
            "  p_5: 3.2777777777777777\n",
            "policy_reward_min:\n",
            "  p_1: -6.0\n",
            "  p_2: -10.0\n",
            "  p_3: -8.0\n",
            "  p_4: -6.0\n",
            "  p_5: -4.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2005697761401711\n",
            "  mean_env_wait_ms: 0.14071351095436874\n",
            "  mean_inference_ms: 4.192803912634681\n",
            "  mean_raw_obs_processing_ms: 2.962359869422827\n",
            "time_since_restore: 7.683156728744507\n",
            "time_this_iter_s: 1.0893058776855469\n",
            "time_total_s: 7.683156728744507\n",
            "timers:\n",
            "  learn_time_ms: 1083.94\n",
            "timestamp: 1598282065\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 1260\n",
            "training_iteration: 7\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.004308691149290296\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.004308691149290296\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9198069942198617\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0626546243997498\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0626546243997498\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07518554927969975\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07518554927969975\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.041769749599833206\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.041769749599833206\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.049689133501294114\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.049689133501294114\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0902226591356397\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0902226591356397\n",
            "training loop = 8 of 30\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-27\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 144\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_1:\n",
            "      allreduce_latency: 0.004133900006612142\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.05196618606310668\n",
            "      entropy: 0.0017116165399784222\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0063200940494425595\n",
            "      model: {}\n",
            "      policy_loss: -0.0013191352287928264\n",
            "      total_loss: 0.4477656055241823\n",
            "      vf_explained_var: 0.3259914815425873\n",
            "      vf_loss: 0.4478207404414813\n",
            "    p_2:\n",
            "      allreduce_latency: 0.004004557927449544\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.0024934555262096625\n",
            "      entropy: 0.44220519065856934\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.05625623526672522\n",
            "      model: {}\n",
            "      policy_loss: 0.13184234003225961\n",
            "      total_loss: 3.7450268268585205\n",
            "      vf_explained_var: 0.03760610148310661\n",
            "      vf_loss: 3.601933161417643\n",
            "    p_4:\n",
            "      allreduce_latency: 0.0038695335388183594\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.002992146631451595\n",
            "      entropy: 1.7449995374600274e-08\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.8299956421057383\n",
            "      model: {}\n",
            "      policy_loss: 0.11985628058513005\n",
            "      total_loss: 15.158713658650717\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 14.872859001159668\n",
            "    p_5:\n",
            "      allreduce_latency: 0.004474123318990071\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.06526523374973937\n",
            "      entropy: 0.0031743989093229175\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.00027747018126926076\n",
            "      model: {}\n",
            "      policy_loss: 0.00011763473351796468\n",
            "      total_loss: 4.167277812957764\n",
            "      vf_explained_var: -0.7682405114173889\n",
            "      vf_loss: 4.167104621728261\n",
            "  num_steps_sampled: 1440\n",
            "  num_steps_trained: 1440\n",
            "iterations_since_restore: 8\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.13333333333334\n",
            "  ram_util_percent: 48.86666666666667\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_1: 6.0\n",
            "  p_2: 2.0\n",
            "  p_3: 3.0\n",
            "  p_4: 10.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_1: 1.8372093023255813\n",
            "  p_2: -4.0\n",
            "  p_3: -4.6923076923076925\n",
            "  p_4: 0.7692307692307693\n",
            "  p_5: 2.3934426229508197\n",
            "policy_reward_min:\n",
            "  p_1: -6.0\n",
            "  p_2: -10.0\n",
            "  p_3: -8.0\n",
            "  p_4: -6.0\n",
            "  p_5: -3.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.19610547629069444\n",
            "  mean_env_wait_ms: 0.12275893129744596\n",
            "  mean_inference_ms: 4.174804869894809\n",
            "  mean_raw_obs_processing_ms: 2.959748469559133\n",
            "time_since_restore: 8.711069583892822\n",
            "time_this_iter_s: 1.0279128551483154\n",
            "time_total_s: 8.711069583892822\n",
            "timers:\n",
            "  learn_time_ms: 1075.4\n",
            "timestamp: 1598282067\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 1440\n",
            "training_iteration: 8\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0601484394237598\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0601484394237598\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.059626960201552934\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.059626960201552934\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.04811875153900785\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.04811875153900785\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.04811875153900785\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.04811875153900785\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.059626960201552934\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.059626960201552934\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07155235224186351\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07155235224186351\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "training loop = 9 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-29\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 162\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_1:\n",
            "      allreduce_latency: 0.003525336583455404\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.002872460766193531\n",
            "      entropy: 0.00023238800349645317\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.03926150128245354\n",
            "      model: {}\n",
            "      policy_loss: 0.034032622973124184\n",
            "      total_loss: 0.3768239977459113\n",
            "      vf_explained_var: 0.3052000105381012\n",
            "      vf_loss: 0.33493908246358234\n",
            "    p_3:\n",
            "      allreduce_latency: 0.0038313865661621094\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.0522121869997915\n",
            "      entropy: 3.739230540607726e-06\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.013127292273566127\n",
            "      model: {}\n",
            "      policy_loss: -0.0018229236205418904\n",
            "      total_loss: 1.2330382863680522\n",
            "      vf_explained_var: -0.5076407790184021\n",
            "      vf_loss: 1.2322357495625813\n",
            "    p_4:\n",
            "      allreduce_latency: 0.005008339881896973\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.002393717305161276\n",
            "      entropy: 1.052211173894572e-06\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 3.1802008067095106e-11\n",
            "      model: {}\n",
            "      policy_loss: 1.4280279477437338e-08\n",
            "      total_loss: 6.514943838119507\n",
            "      vf_explained_var: -0.9880793690681458\n",
            "      vf_loss: 6.514943917592366\n",
            "    p_5:\n",
            "      allreduce_latency: 0.004432519276936849\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.041769749599833206\n",
            "      entropy: 0.0021039184648543596\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 5.460445269515427e-05\n",
            "      model: {}\n",
            "      policy_loss: 6.516153613726298e-05\n",
            "      total_loss: 2.1749426126480103\n",
            "      vf_explained_var: -0.7822322845458984\n",
            "      vf_loss: 2.1748665968577066\n",
            "  num_steps_sampled: 1620\n",
            "  num_steps_trained: 1620\n",
            "iterations_since_restore: 9\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 80.25\n",
            "  ram_util_percent: 48.9\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_1: 6.0\n",
            "  p_2: 2.0\n",
            "  p_3: 3.0\n",
            "  p_4: 10.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_1: 2.3823529411764706\n",
            "  p_2: -4.0\n",
            "  p_3: -4.5\n",
            "  p_4: 1.930232558139535\n",
            "  p_5: 2.175438596491228\n",
            "policy_reward_min:\n",
            "  p_1: -1.0\n",
            "  p_2: -10.0\n",
            "  p_3: -8.0\n",
            "  p_4: -6.0\n",
            "  p_5: -4.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.19304124612126705\n",
            "  mean_env_wait_ms: 0.11155868584022531\n",
            "  mean_inference_ms: 4.157479573352716\n",
            "  mean_raw_obs_processing_ms: 2.966086893935761\n",
            "time_since_restore: 9.775259256362915\n",
            "time_this_iter_s: 1.0641896724700928\n",
            "time_total_s: 9.775259256362915\n",
            "timers:\n",
            "  learn_time_ms: 1072.656\n",
            "timestamp: 1598282069\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 1620\n",
            "training_iteration: 9\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.04811875153900785\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.04811875153900785\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9821901354854268\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.04770156816124235\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.04770156816124235\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.057241881793490816\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.057241881793490816\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.03816125452899388\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.03816125452899388\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.08586282269023622\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.08586282269023622\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.08586282269023622\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.08586282269023622\n",
            "training loop = 10 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-31\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 180\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_3:\n",
            "      allreduce_latency: 0.005549404356214736\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.041769749599833206\n",
            "      entropy: 5.699845158820457e-08\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 5.536741784803765e-08\n",
            "      model: {}\n",
            "      policy_loss: 1.6391277313232422e-07\n",
            "      total_loss: 3.2394388251834445\n",
            "      vf_explained_var: -0.15129321813583374\n",
            "      vf_loss: 3.2394387192196317\n",
            "    p_4:\n",
            "      allreduce_latency: 0.004245837529500325\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.049689133501294114\n",
            "      entropy: 1.8349949035230868e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.515222038506181e-06\n",
            "      model: {}\n",
            "      policy_loss: -6.109476089477539e-07\n",
            "      total_loss: 2.2945240338643393\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 2.294524351755778\n",
            "    p_5:\n",
            "      allreduce_latency: 0.003515481948852539\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.0902226591356397\n",
            "      entropy: 0.004906247617327608\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0006303890357154766\n",
            "      model: {}\n",
            "      policy_loss: 0.001199701800942421\n",
            "      total_loss: 10.01850994427999\n",
            "      vf_explained_var: -0.5949227213859558\n",
            "      vf_loss: 10.017184297243753\n",
            "  num_steps_sampled: 1800\n",
            "  num_steps_trained: 1800\n",
            "iterations_since_restore: 10\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.43333333333334\n",
            "  ram_util_percent: 48.9\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_1: 4.0\n",
            "  p_2: 2.0\n",
            "  p_3: 0.0\n",
            "  p_4: 10.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_1: 2.761904761904762\n",
            "  p_2: -4.0\n",
            "  p_3: -4.721311475409836\n",
            "  p_4: 3.1538461538461537\n",
            "  p_5: 2.9344262295081966\n",
            "policy_reward_min:\n",
            "  p_1: 1.0\n",
            "  p_2: -10.0\n",
            "  p_3: -8.0\n",
            "  p_4: -2.0\n",
            "  p_5: -4.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.19087535262231037\n",
            "  mean_env_wait_ms: 0.10355120934074517\n",
            "  mean_inference_ms: 4.148591035312124\n",
            "  mean_raw_obs_processing_ms: 2.971009136139738\n",
            "time_since_restore: 10.843435287475586\n",
            "time_this_iter_s: 1.068176031112671\n",
            "time_total_s: 10.843435287475586\n",
            "timers:\n",
            "  learn_time_ms: 1070.939\n",
            "timestamp: 1598282071\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 1800\n",
            "training_iteration: 10\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.008075286462033473\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9290362508888235\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.008075286462033473\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.006460229169626779\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9290362508888235\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9290362508888235\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.006460229169626779\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9290362508888235\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.08159681313744564\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.08159681313744564\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.04579350543479266\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.04579350543479266\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06869025815218897\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06869025815218897\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06869025815218897\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06869025815218897\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "training loop = 11 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-33\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 198\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.0058440764745076495\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.0601484394237598\n",
            "      entropy: 0.13158915502329668\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 7.186209162076314\n",
            "      model: {}\n",
            "      policy_loss: 0.33969920749465626\n",
            "      total_loss: 22.889018456141155\n",
            "      vf_explained_var: -0.5525213479995728\n",
            "      vf_loss: 21.11207890510559\n",
            "    p_3:\n",
            "      allreduce_latency: 0.0045566558837890625\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.04811875153900785\n",
            "      entropy: 4.2498793817458136e-08\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -2.4883262406590518e-11\n",
            "      model: {}\n",
            "      policy_loss: -3.9736429850260414e-08\n",
            "      total_loss: 3.277634064356486\n",
            "      vf_explained_var: -0.059494417160749435\n",
            "      vf_loss: 3.2776341438293457\n",
            "    p_5:\n",
            "      allreduce_latency: 0.004278765784369575\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.07155235224186351\n",
            "      entropy: 0.0025998845974552548\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0003032969220334457\n",
            "      model: {}\n",
            "      policy_loss: 0.00013314228918817308\n",
            "      total_loss: 7.092658307817247\n",
            "      vf_explained_var: -0.08314245194196701\n",
            "      vf_loss: 7.092464235093859\n",
            "  num_steps_sampled: 1980\n",
            "  num_steps_trained: 1980\n",
            "iterations_since_restore: 11\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 81.43333333333334\n",
            "  ram_util_percent: 48.9\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 3.0\n",
            "  p_1: 4.0\n",
            "  p_2: 2.0\n",
            "  p_3: 0.0\n",
            "  p_4: 10.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.16666666666666666\n",
            "  p_1: 3.0555555555555554\n",
            "  p_2: -4.0\n",
            "  p_3: -4.788461538461538\n",
            "  p_4: 3.5\n",
            "  p_5: 2.21875\n",
            "policy_reward_min:\n",
            "  p_0: -3.0\n",
            "  p_1: 3.0\n",
            "  p_2: -10.0\n",
            "  p_3: -8.0\n",
            "  p_4: -2.0\n",
            "  p_5: -4.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.1894470750284124\n",
            "  mean_env_wait_ms: 0.09765520047648976\n",
            "  mean_inference_ms: 4.140061182882182\n",
            "  mean_raw_obs_processing_ms: 2.9802183970080045\n",
            "time_since_restore: 11.937215566635132\n",
            "time_this_iter_s: 1.093780279159546\n",
            "time_total_s: 11.937215566635132\n",
            "timers:\n",
            "  learn_time_ms: 1057.515\n",
            "timestamp: 1598282073\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 1980\n",
            "training_iteration: 11\n",
            "\n",
            "checkpoint saved at /content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down/chkpt/checkpoint_11/checkpoint-11\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.09791617576493476\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.09791617576493476\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05495220652175118\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05495220652175118\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.04348240024238971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9918153956055874\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.04348240024238971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9918153956055874\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05495220652175119\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05495220652175119\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07833294061194782\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07833294061194782\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.08242830978262676\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.08242830978262676\n",
            "training loop = 12 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-35\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 216\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.004096825917561849\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.04811875153900785\n",
            "      entropy: 0.0005585331361241211\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 4.336758295694987\n",
            "      model: {}\n",
            "      policy_loss: -0.060574995974699654\n",
            "      total_loss: 5.99364447593689\n",
            "      vf_explained_var: -0.2688066363334656\n",
            "      vf_loss: 5.186867554982503\n",
            "    p_1:\n",
            "      allreduce_latency: 0.0065907637278238935\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.04770156816124235\n",
            "      entropy: 2.4774138031110244e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 8.868905903606597e-06\n",
            "      model: {}\n",
            "      policy_loss: 1.6142924626668293e-08\n",
            "      total_loss: 0.4563289557894071\n",
            "      vf_explained_var: 0.26556721329689026\n",
            "      vf_loss: 0.4563271502653758\n",
            "    p_3:\n",
            "      allreduce_latency: 0.004052639007568359\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.03816125452899388\n",
            "      entropy: 1.521844753919292e-08\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -3.4035145650886576e-11\n",
            "      model: {}\n",
            "      policy_loss: -2.0427008469899496e-07\n",
            "      total_loss: 2.312024712562561\n",
            "      vf_explained_var: -0.7112712860107422\n",
            "      vf_loss: 2.3120249211788177\n",
            "    p_5:\n",
            "      allreduce_latency: 0.0032767454783121743\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.08586282269023622\n",
            "      entropy: 0.0017499130347763032\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 7.696250433279299e-05\n",
            "      model: {}\n",
            "      policy_loss: 0.00034207602341969806\n",
            "      total_loss: 2.7734549840291343\n",
            "      vf_explained_var: -0.5150174498558044\n",
            "      vf_loss: 2.7730974356333413\n",
            "  num_steps_sampled: 2160\n",
            "  num_steps_trained: 2160\n",
            "iterations_since_restore: 12\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.06666666666666\n",
            "  ram_util_percent: 48.9\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 6.0\n",
            "  p_1: 4.0\n",
            "  p_2: 2.0\n",
            "  p_3: -3.0\n",
            "  p_4: 10.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.6111111111111112\n",
            "  p_1: 3.033333333333333\n",
            "  p_2: -3.5384615384615383\n",
            "  p_3: -4.294117647058823\n",
            "  p_4: 3.225806451612903\n",
            "  p_5: 1.105263157894737\n",
            "policy_reward_min:\n",
            "  p_0: -3.0\n",
            "  p_1: 3.0\n",
            "  p_2: -10.0\n",
            "  p_3: -8.0\n",
            "  p_4: -2.0\n",
            "  p_5: -6.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.18807565133009466\n",
            "  mean_env_wait_ms: 0.09348785628424565\n",
            "  mean_inference_ms: 4.128336086557072\n",
            "  mean_raw_obs_processing_ms: 2.9911661676900114\n",
            "time_since_restore: 13.025519609451294\n",
            "time_this_iter_s: 1.088304042816162\n",
            "time_total_s: 13.025519609451294\n",
            "timers:\n",
            "  learn_time_ms: 1059.959\n",
            "timestamp: 1598282075\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 2160\n",
            "training_iteration: 12\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.09399952873433738\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.09399952873433738\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.043961765217400944\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.043961765217400944\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.11279943448120486\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.11279943448120486\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06594264782610142\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06594264782610142\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05275411826088113\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05275411826088113\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.11279943448120486\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.11279943448120486\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "training loop = 13 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-37\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 234\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.004383206367492676\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.008075286462033473\n",
            "      entropy: 0.12926820572465658\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.11914765314819913\n",
            "      model: {}\n",
            "      policy_loss: -0.07520453678444028\n",
            "      total_loss: 1.4570973217487335\n",
            "      vf_explained_var: -0.7219879031181335\n",
            "      vf_loss: 1.50847231845061\n",
            "    p_1:\n",
            "      allreduce_latency: 0.00591127077738444\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.006460229169626779\n",
            "      entropy: 1.8037046781197812e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 3.0482666583490453e-07\n",
            "      model: {}\n",
            "      policy_loss: -2.0489096641540527e-08\n",
            "      total_loss: 0.4497649110853672\n",
            "      vf_explained_var: 0.32242676615715027\n",
            "      vf_loss: 0.44976483782132465\n",
            "    p_2:\n",
            "      allreduce_latency: 0.00391995906829834\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.08159681313744564\n",
            "      entropy: 0.0035517077000501254\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.146773676077525\n",
            "      model: {}\n",
            "      policy_loss: 0.06084660223374764\n",
            "      total_loss: 1.7550467203060787\n",
            "      vf_explained_var: -0.684825599193573\n",
            "      vf_loss: 1.464845319588979\n",
            "    p_3:\n",
            "      allreduce_latency: 0.0065424442291259766\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.04579350543479266\n",
            "      entropy: 1.5346604283668814e-08\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -3.79456766933887e-12\n",
            "      model: {}\n",
            "      policy_loss: -2.483526865641276e-08\n",
            "      total_loss: 1.2244428594907124\n",
            "      vf_explained_var: -0.9336135983467102\n",
            "      vf_loss: 1.2244428396224976\n",
            "  num_steps_sampled: 2340\n",
            "  num_steps_trained: 2340\n",
            "iterations_since_restore: 13\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 81.3\n",
            "  ram_util_percent: 48.9\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 6.0\n",
            "  p_1: 4.0\n",
            "  p_2: 2.0\n",
            "  p_3: -3.0\n",
            "  p_4: 3.0\n",
            "  p_5: 6.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.03333333333333333\n",
            "  p_1: 3.032258064516129\n",
            "  p_2: 0.6666666666666666\n",
            "  p_3: -4.0\n",
            "  p_4: 2.5714285714285716\n",
            "  p_5: 1.163265306122449\n",
            "policy_reward_min:\n",
            "  p_0: -3.0\n",
            "  p_1: 3.0\n",
            "  p_2: -3.0\n",
            "  p_3: -6.0\n",
            "  p_4: -2.0\n",
            "  p_5: -6.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.186995736650785\n",
            "  mean_env_wait_ms: 0.09020586852995706\n",
            "  mean_inference_ms: 4.124106160781412\n",
            "  mean_raw_obs_processing_ms: 3.0016954774283646\n",
            "time_since_restore: 14.078579425811768\n",
            "time_this_iter_s: 1.0530598163604736\n",
            "time_total_s: 14.078579425811768\n",
            "timers:\n",
            "  learn_time_ms: 1057.205\n",
            "timestamp: 1598282077\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 2340\n",
            "training_iteration: 13\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.11279943448120486\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.11279943448120486\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05275411826088113\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05275411826088113\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.09023954758496389\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.09023954758496389\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06330494191305736\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06330494191305736\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06330494191305736\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06330494191305736\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.09023954758496389\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.09023954758496389\n",
            "training loop = 14 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-39\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 252\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.005274534225463867\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.09791617576493476\n",
            "      entropy: 0.001524106094924112\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.012175738501052061\n",
            "      model: {}\n",
            "      policy_loss: 0.013386483925084272\n",
            "      total_loss: 0.6610206527014574\n",
            "      vf_explained_var: 0.059672653675079346\n",
            "      vf_loss: 0.6451990765829881\n",
            "    p_1:\n",
            "      allreduce_latency: 0.005154967308044434\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.05495220652175118\n",
            "      entropy: 3.240731439291267e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -1.1658478532202328e-06\n",
            "      model: {}\n",
            "      policy_loss: -3.3775965372721356e-07\n",
            "      total_loss: 12.739049434661865\n",
            "      vf_explained_var: -0.005681911949068308\n",
            "      vf_loss: 12.739049911499023\n",
            "    p_2:\n",
            "      allreduce_latency: 0.004530323876274956\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.04348240024238971\n",
            "      entropy: 0.06786735252373749\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.02003534425360461\n",
            "      model: {}\n",
            "      policy_loss: -0.016430437720070284\n",
            "      total_loss: 4.443673067622715\n",
            "      vf_explained_var: 0.4481312036514282\n",
            "      vf_loss: 4.456096635924445\n",
            "  num_steps_sampled: 2520\n",
            "  num_steps_trained: 2520\n",
            "iterations_since_restore: 14\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 83.16666666666667\n",
            "  ram_util_percent: 48.9\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 6.0\n",
            "  p_1: 3.0\n",
            "  p_2: 10.0\n",
            "  p_3: -3.0\n",
            "  p_4: 3.0\n",
            "  p_5: 6.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.1388888888888889\n",
            "  p_1: -1.7272727272727273\n",
            "  p_2: 4.533333333333333\n",
            "  p_3: -4.1020408163265305\n",
            "  p_4: 3.0\n",
            "  p_5: 2.2564102564102564\n",
            "policy_reward_min:\n",
            "  p_0: -3.0\n",
            "  p_1: -10.0\n",
            "  p_2: 0.0\n",
            "  p_3: -6.0\n",
            "  p_4: 3.0\n",
            "  p_5: -6.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.18642783282814257\n",
            "  mean_env_wait_ms: 0.08760638549350908\n",
            "  mean_inference_ms: 4.126317093116125\n",
            "  mean_raw_obs_processing_ms: 3.0064419378540053\n",
            "time_since_restore: 15.135594606399536\n",
            "time_this_iter_s: 1.0570151805877686\n",
            "time_total_s: 15.135594606399536\n",
            "timers:\n",
            "  learn_time_ms: 1055.554\n",
            "timestamp: 1598282079\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 2520\n",
            "training_iteration: 14\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.09023954758496389\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.09023954758496389\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06330494191305736\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06330494191305736\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07219163806797112\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07219163806797112\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05064395353044589\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05064395353044589\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05064395353044589\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05064395353044589\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07219163806797112\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07219163806797112\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "training loop = 15 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-41\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 270\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_1:\n",
            "      allreduce_latency: 0.005289157231648763\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.043961765217400944\n",
            "      entropy: 2.3034069575563382e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 2.1345369323929237e-08\n",
            "      model: {}\n",
            "      policy_loss: 4.718701044718424e-08\n",
            "      total_loss: 4.8695329030354815\n",
            "      vf_explained_var: 0.18781262636184692\n",
            "      vf_loss: 4.869532823562622\n",
            "    p_2:\n",
            "      allreduce_latency: 0.004854281743367513\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.11279943448120486\n",
            "      entropy: 0.100711065290347\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.006841403869227886\n",
            "      model: {}\n",
            "      policy_loss: -0.007231146925025516\n",
            "      total_loss: 2.574686302079095\n",
            "      vf_explained_var: 0.46932804584503174\n",
            "      vf_loss: 2.5805491871303983\n",
            "    p_4:\n",
            "      allreduce_latency: 0.004047950108846028\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.05275411826088113\n",
            "      entropy: 2.1397611665937195e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 2.618073627521274e-07\n",
            "      model: {}\n",
            "      policy_loss: 6.012657346824805e-07\n",
            "      total_loss: 1.6130460103352864\n",
            "      vf_explained_var: -0.3435741662979126\n",
            "      vf_loss: 1.6130454142888386\n",
            "  num_steps_sampled: 2700\n",
            "  num_steps_trained: 2700\n",
            "iterations_since_restore: 15\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.8\n",
            "  ram_util_percent: 48.9\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 6.0\n",
            "  p_1: 3.0\n",
            "  p_2: 10.0\n",
            "  p_3: -3.0\n",
            "  p_4: 3.0\n",
            "  p_5: 6.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.1388888888888889\n",
            "  p_1: -3.5\n",
            "  p_2: 4.3125\n",
            "  p_3: -4.147058823529412\n",
            "  p_4: -0.13333333333333333\n",
            "  p_5: 2.161290322580645\n",
            "policy_reward_min:\n",
            "  p_0: -3.0\n",
            "  p_1: -10.0\n",
            "  p_2: 0.0\n",
            "  p_3: -6.0\n",
            "  p_4: -1.0\n",
            "  p_5: -6.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.1861046568558008\n",
            "  mean_env_wait_ms: 0.08541115384827762\n",
            "  mean_inference_ms: 4.125250816536254\n",
            "  mean_raw_obs_processing_ms: 3.011921350305388\n",
            "time_since_restore: 16.1704318523407\n",
            "time_this_iter_s: 1.034837245941162\n",
            "time_total_s: 16.1704318523407\n",
            "timers:\n",
            "  learn_time_ms: 1054.124\n",
            "timestamp: 1598282081\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 2700\n",
            "training_iteration: 15\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06077274423653506\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06077274423653506\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07596593029566882\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07596593029566882\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.08662996568156534\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.08662996568156534\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.08662996568156534\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.08662996568156534\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0577533104543769\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0577533104543769\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0577533104543769\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0577533104543769\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "training loop = 16 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-42\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 288\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.005098303159077962\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.11279943448120487\n",
            "      entropy: 0.06615505879744887\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.022478786054610584\n",
            "      model: {}\n",
            "      policy_loss: -0.02087460574693978\n",
            "      total_loss: 0.18931217739979425\n",
            "      vf_explained_var: 0.5532608032226562\n",
            "      vf_loss: 0.2056910234193007\n",
            "    p_2:\n",
            "      allreduce_latency: 0.004613611433241103\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.09023954758496389\n",
            "      entropy: 0.2663136538532045\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.03343332693394688\n",
            "      model: {}\n",
            "      policy_loss: 0.004426725622680452\n",
            "      total_loss: 0.6379038426611159\n",
            "      vf_explained_var: 0.04390927031636238\n",
            "      vf_loss: 0.6267904424005084\n",
            "    p_4:\n",
            "      allreduce_latency: 0.004881938298543294\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.06330494191305736\n",
            "      entropy: 2.816336242698429e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -2.1114032166509212e-07\n",
            "      model: {}\n",
            "      policy_loss: -1.9868214925130207e-08\n",
            "      total_loss: 0.717050830523173\n",
            "      vf_explained_var: 0.336744099855423\n",
            "      vf_loss: 0.7170509099960327\n",
            "  num_steps_sampled: 2880\n",
            "  num_steps_trained: 2880\n",
            "iterations_since_restore: 16\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 79.5\n",
            "  ram_util_percent: 48.9\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 6.0\n",
            "  p_1: 3.0\n",
            "  p_2: 10.0\n",
            "  p_3: -3.0\n",
            "  p_4: 0.0\n",
            "  p_5: 6.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.09302325581395349\n",
            "  p_1: -3.5\n",
            "  p_2: 3.287878787878788\n",
            "  p_3: -3.4285714285714284\n",
            "  p_4: -0.9444444444444444\n",
            "  p_5: 0.125\n",
            "policy_reward_min:\n",
            "  p_0: -3.0\n",
            "  p_1: -10.0\n",
            "  p_2: 0.0\n",
            "  p_3: -6.0\n",
            "  p_4: -1.0\n",
            "  p_5: -6.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.18600146720951238\n",
            "  mean_env_wait_ms: 0.08352121430621637\n",
            "  mean_inference_ms: 4.1196705981189226\n",
            "  mean_raw_obs_processing_ms: 3.0145486956761083\n",
            "time_since_restore: 17.20370388031006\n",
            "time_this_iter_s: 1.0332720279693604\n",
            "time_total_s: 17.20370388031006\n",
            "timers:\n",
            "  learn_time_ms: 1047.653\n",
            "timestamp: 1598282082\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 2880\n",
            "training_iteration: 16\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07292729308384206\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07292729308384206\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06077274423653506\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06077274423653506\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.1039559588178784\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.1039559588178784\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06930397254525228\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06930397254525228\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.08751275170061047\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.08751275170061047\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06930397254525228\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9699469767672457\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06930397254525228\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "training loop = 17 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-44\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 306\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.004658857981363933\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.09023954758496389\n",
            "      entropy: 0.006554554041940719\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.771904706954956\n",
            "      model: {}\n",
            "      policy_loss: 0.2590069572130839\n",
            "      total_loss: 1.2918547789255779\n",
            "      vf_explained_var: 0.16773861646652222\n",
            "      vf_loss: 0.6784668366114298\n",
            "    p_2:\n",
            "      allreduce_latency: 0.0053784847259521484\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.07219163806797112\n",
            "      entropy: 0.018009972603370745\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2820839881896973\n",
            "      model: {}\n",
            "      policy_loss: 0.15328932957102856\n",
            "      total_loss: 1.0755554735660553\n",
            "      vf_explained_var: -0.03513797000050545\n",
            "      vf_loss: 0.665849357843399\n",
            "    p_3:\n",
            "      allreduce_latency: 0.004308621088663737\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.05064395353044588\n",
            "      entropy: 4.283065990762225e-08\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -1.9527578881163576e-12\n",
            "      model: {}\n",
            "      policy_loss: -3.5700698693593344e-09\n",
            "      total_loss: 2.7880955934524536\n",
            "      vf_explained_var: -0.23896783590316772\n",
            "      vf_loss: 2.7880957325299582\n",
            "    p_5:\n",
            "      allreduce_latency: 0.00405720869700114\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.07219163806797112\n",
            "      entropy: 0.04975116126540039\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.06782488117460161\n",
            "      model: {}\n",
            "      policy_loss: -0.0016853877653678258\n",
            "      total_loss: 4.3671329617500305\n",
            "      vf_explained_var: -0.7893667817115784\n",
            "      vf_loss: 4.355253318945567\n",
            "  num_steps_sampled: 3060\n",
            "  num_steps_trained: 3060\n",
            "iterations_since_restore: 17\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 83.73333333333333\n",
            "  ram_util_percent: 48.9\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 4.0\n",
            "  p_1: 3.0\n",
            "  p_2: 10.0\n",
            "  p_3: -3.0\n",
            "  p_4: 0.0\n",
            "  p_5: 6.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.05128205128205128\n",
            "  p_1: -4.548387096774194\n",
            "  p_2: 2.861111111111111\n",
            "  p_3: -4.32\n",
            "  p_4: -0.9444444444444444\n",
            "  p_5: 4.133333333333334\n",
            "policy_reward_min:\n",
            "  p_0: -1.0\n",
            "  p_1: -10.0\n",
            "  p_2: -3.0\n",
            "  p_3: -6.0\n",
            "  p_4: -1.0\n",
            "  p_5: -4.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.18572796501077704\n",
            "  mean_env_wait_ms: 0.08161571956937917\n",
            "  mean_inference_ms: 4.11875426001227\n",
            "  mean_raw_obs_processing_ms: 3.0116855291916202\n",
            "time_since_restore: 18.285850524902344\n",
            "time_this_iter_s: 1.0821466445922852\n",
            "time_total_s: 18.285850524902344\n",
            "timers:\n",
            "  learn_time_ms: 1046.926\n",
            "timestamp: 1598282084\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 3060\n",
            "training_iteration: 17\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05544317803620183\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05544317803620183\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.018873540965030654\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9801726252936649\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.018873540965030654\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.00461732245786254\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9801726252936649\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.00461732245786254\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05544317803620183\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05544317803620183\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05544317803620183\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05544317803620183\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0875705322565877\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0875705322565877\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "training loop = 18 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-46\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 324\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.0046692291895548505\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.06077274423653506\n",
            "      entropy: 0.0098247443481038\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0011990353232249618\n",
            "      model: {}\n",
            "      policy_loss: -0.0011045684417088826\n",
            "      total_loss: 0.6440756109853586\n",
            "      vf_explained_var: 0.5655373930931091\n",
            "      vf_loss: 0.6449403737982115\n",
            "    p_2:\n",
            "      allreduce_latency: 0.004342476526896159\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.08662996568156534\n",
            "      entropy: 3.6985687283201213e-07\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.5862325032552084\n",
            "      model: {}\n",
            "      policy_loss: 0.008799390556911627\n",
            "      total_loss: 0.8097631831963857\n",
            "      vf_explained_var: -0.02423895336687565\n",
            "      vf_loss: 0.6837173153956732\n",
            "    p_3:\n",
            "      allreduce_latency: 0.0034603277842203775\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.08662996568156534\n",
            "      entropy: 4.348207246115029e-08\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -2.188801892192884e-13\n",
            "      model: {}\n",
            "      policy_loss: 3.9736429850260414e-08\n",
            "      total_loss: 2.5580310424168906\n",
            "      vf_explained_var: -0.11226171255111694\n",
            "      vf_loss: 2.558030923207601\n",
            "    p_5:\n",
            "      allreduce_latency: 0.0036218961079915366\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.0577533104543769\n",
            "      entropy: 6.453345880193713e-06\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 3.855853841135589e-05\n",
            "      model: {}\n",
            "      policy_loss: 1.6898072014252345e-06\n",
            "      total_loss: 2.1169846455256143\n",
            "      vf_explained_var: 0.010804872028529644\n",
            "      vf_loss: 2.1169752875963845\n",
            "  num_steps_sampled: 3240\n",
            "  num_steps_trained: 3240\n",
            "iterations_since_restore: 18\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.26666666666667\n",
            "  ram_util_percent: 48.9\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 3.0\n",
            "  p_1: 3.0\n",
            "  p_2: 10.0\n",
            "  p_3: -3.0\n",
            "  p_4: 0.0\n",
            "  p_5: 6.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.32558139534883723\n",
            "  p_1: -8.142857142857142\n",
            "  p_2: 2.670886075949367\n",
            "  p_3: -5.428571428571429\n",
            "  p_4: -0.9444444444444444\n",
            "  p_5: 5.833333333333333\n",
            "policy_reward_min:\n",
            "  p_0: -1.0\n",
            "  p_1: -10.0\n",
            "  p_2: -3.0\n",
            "  p_3: -6.0\n",
            "  p_4: -1.0\n",
            "  p_5: 3.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.18518282676731232\n",
            "  mean_env_wait_ms: 0.07996553239879707\n",
            "  mean_inference_ms: 4.1230639037670676\n",
            "  mean_raw_obs_processing_ms: 3.012682145588985\n",
            "time_since_restore: 19.34955906867981\n",
            "time_this_iter_s: 1.0637085437774658\n",
            "time_total_s: 19.34955906867981\n",
            "timers:\n",
            "  learn_time_ms: 1050.51\n",
            "timestamp: 1598282086\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 3240\n",
            "training_iteration: 18\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.04435454242896147\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.04435454242896147\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.022648249158036784\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9801726252936649\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.022648249158036784\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9801726252936649\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.005540786949435048\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.005540786949435048\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07005642580527016\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07005642580527016\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0044326295595480385\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0044326295595480385\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.02717789898964414\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9801726252936649\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.02717789898964414\n",
            "training loop = 19 of 30\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9801726252936649\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-48\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 342\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.004061566458808051\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.07292729308384206\n",
            "      entropy: 0.001651063089310709\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0019801071676839557\n",
            "      model: {}\n",
            "      policy_loss: -0.0006450613339742025\n",
            "      total_loss: 0.2298713064649039\n",
            "      vf_explained_var: -0.09431060403585434\n",
            "      vf_loss: 0.23012033270465004\n",
            "    p_2:\n",
            "      allreduce_latency: 0.004735231399536133\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.1039559588178784\n",
            "      entropy: 6.010185643390287e-06\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 3.00790873097867e-07\n",
            "      model: {}\n",
            "      policy_loss: 1.4901161193847656e-08\n",
            "      total_loss: 0.3214145277937253\n",
            "      vf_explained_var: 0.03874770924448967\n",
            "      vf_loss: 0.32141447563966113\n",
            "    p_4:\n",
            "      allreduce_latency: 0.004105965296427409\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.08751275170061047\n",
            "      entropy: 3.164101144648157e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -2.631306680693039e-07\n",
            "      model: {}\n",
            "      policy_loss: -8.73891015847524e-08\n",
            "      total_loss: 0.787804595194757\n",
            "      vf_explained_var: 0.13623149693012238\n",
            "      vf_loss: 0.7878047153353691\n",
            "  num_steps_sampled: 3420\n",
            "  num_steps_trained: 3420\n",
            "iterations_since_restore: 19\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.63333333333334\n",
            "  ram_util_percent: 48.9\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 3.0\n",
            "  p_1: -10.0\n",
            "  p_2: 10.0\n",
            "  p_3: -3.0\n",
            "  p_4: 0.0\n",
            "  p_5: 6.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.23529411764705882\n",
            "  p_1: -10.0\n",
            "  p_2: 2.2714285714285714\n",
            "  p_3: -5.833333333333333\n",
            "  p_4: -0.5666666666666667\n",
            "  p_5: 5.833333333333333\n",
            "policy_reward_min:\n",
            "  p_0: -1.0\n",
            "  p_1: -10.0\n",
            "  p_2: -3.0\n",
            "  p_3: -6.0\n",
            "  p_4: -1.0\n",
            "  p_5: 3.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.18456493964040407\n",
            "  mean_env_wait_ms: 0.07849278896732897\n",
            "  mean_inference_ms: 4.125112475439085\n",
            "  mean_raw_obs_processing_ms: 3.0152874253049564\n",
            "time_since_restore: 20.39009189605713\n",
            "time_this_iter_s: 1.0405328273773193\n",
            "time_total_s: 20.39009189605713\n",
            "timers:\n",
            "  learn_time_ms: 1048.229\n",
            "timestamp: 1598282088\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 3420\n",
            "training_iteration: 19\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.035483633943169175\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.035483633943169175\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.02717789898964414\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9801726252936649\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.02717789898964414\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.006648944339322057\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9801726252936649\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.006648944339322057\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.08406771096632419\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.08406771096632419\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06725416877305936\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06725416877305936\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.032613478787572966\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9801726252936649\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.032613478787572966\n",
            "training loop = 20 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-50\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 360\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.004190391964382595\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.05544317803620183\n",
            "      entropy: 0.0013774834806099534\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.00016261881521333836\n",
            "      model: {}\n",
            "      policy_loss: -0.0001352807092997763\n",
            "      total_loss: 0.03496079477998945\n",
            "      vf_explained_var: -0.41791868209838867\n",
            "      vf_loss: 0.035063553250640526\n",
            "    p_1:\n",
            "      allreduce_latency: 0.004361192385355632\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.018873540965030657\n",
            "      entropy: 3.800112655105714e-11\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 8.688726143733436e-13\n",
            "      model: {}\n",
            "      policy_loss: -0.00476837158203125\n",
            "      total_loss: 5.417223314444224\n",
            "      vf_explained_var: 'null'\n",
            "      vf_loss: 5.421991686026256\n",
            "    p_4:\n",
            "      allreduce_latency: 0.00488742192586263\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.05544317803620183\n",
            "      entropy: 3.42426022446792e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 6.536336911722174e-07\n",
            "      model: {}\n",
            "      policy_loss: -1.0251688612091432e-06\n",
            "      total_loss: 0.38611432661612827\n",
            "      vf_explained_var: -0.38648319244384766\n",
            "      vf_loss: 0.38611521820227307\n",
            "  num_steps_sampled: 3600\n",
            "  num_steps_trained: 3600\n",
            "iterations_since_restore: 20\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 79.30000000000001\n",
            "  ram_util_percent: 49.0\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 3.0\n",
            "  p_1: 0.0\n",
            "  p_2: 10.0\n",
            "  p_3: -3.0\n",
            "  p_4: 0.0\n",
            "  p_5: 6.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.13636363636363635\n",
            "  p_1: -2.0\n",
            "  p_2: 0.9807692307692307\n",
            "  p_3: -5.833333333333333\n",
            "  p_4: -0.3870967741935484\n",
            "  p_5: 5.833333333333333\n",
            "policy_reward_min:\n",
            "  p_0: -1.0\n",
            "  p_1: -10.0\n",
            "  p_2: -3.0\n",
            "  p_3: -6.0\n",
            "  p_4: -1.0\n",
            "  p_5: 3.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.1841750248413495\n",
            "  mean_env_wait_ms: 0.07712729604452241\n",
            "  mean_inference_ms: 4.126269765673572\n",
            "  mean_raw_obs_processing_ms: 3.015348909047247\n",
            "time_since_restore: 21.42789387702942\n",
            "time_this_iter_s: 1.03780198097229\n",
            "time_total_s: 21.42789387702942\n",
            "timers:\n",
            "  learn_time_ms: 1045.109\n",
            "timestamp: 1598282090\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 3600\n",
            "training_iteration: 20\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9801726252936649\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.028386907154535342\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.028386907154535342\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9807898574822956\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.032613478787572966\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9801726252936649\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.032613478787572966\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9801726252936649\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.005319155471457646\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.005319155471457646\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.006382986565749175\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.006382986565749175\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05380333501844749\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05380333501844749\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.03913617454508756\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9801726252936649\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.03913617454508756\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9801726252936649\n",
            "training loop = 21 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-52\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 378\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.006243705749511719\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.04435454242896147\n",
            "      entropy: 0.001349562662653625\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.6499611679895072e-06\n",
            "      model: {}\n",
            "      policy_loss: -5.682309468587239e-06\n",
            "      total_loss: 0.5290322979498241\n",
            "      vf_explained_var: 0.043641671538352966\n",
            "      vf_loss: 0.5290376610226102\n",
            "    p_1:\n",
            "      allreduce_latency: 0.00476829210917155\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.022648249158036784\n",
            "      entropy: 9.904062162983488e-09\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -5.7170291359783425e-12\n",
            "      model: {}\n",
            "      policy_loss: -0.0002980232238769531\n",
            "      total_loss: 0.08485686043665434\n",
            "      vf_explained_var: 'null'\n",
            "      vf_loss: 0.0851548836605313\n",
            "    p_3:\n",
            "      allreduce_latency: 0.005046963691711426\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.07005642580527016\n",
            "      entropy: 5.062734572310509e-08\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 4.689883193170069e-13\n",
            "      model: {}\n",
            "      policy_loss: -3.942598899205526e-08\n",
            "      total_loss: 2.6258290807406106\n",
            "      vf_explained_var: -0.5206490755081177\n",
            "      vf_loss: 2.6258291701475778\n",
            "  num_steps_sampled: 3780\n",
            "  num_steps_trained: 3780\n",
            "iterations_since_restore: 21\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.46666666666668\n",
            "  ram_util_percent: 49.0\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 3.0\n",
            "  p_1: 0.0\n",
            "  p_2: 1.0\n",
            "  p_3: -2.0\n",
            "  p_4: 0.0\n",
            "  p_5: 6.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.20253164556962025\n",
            "  p_1: 0.0\n",
            "  p_2: 0.3235294117647059\n",
            "  p_3: -4.3\n",
            "  p_4: -0.14285714285714285\n",
            "  p_5: 5.833333333333333\n",
            "policy_reward_min:\n",
            "  p_0: -1.0\n",
            "  p_1: 0.0\n",
            "  p_2: -3.0\n",
            "  p_3: -6.0\n",
            "  p_4: -1.0\n",
            "  p_5: 3.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.1836955270067077\n",
            "  mean_env_wait_ms: 0.07590799030160489\n",
            "  mean_inference_ms: 4.128040814229021\n",
            "  mean_raw_obs_processing_ms: 3.012979991562419\n",
            "time_since_restore: 22.503746271133423\n",
            "time_this_iter_s: 1.075852394104004\n",
            "time_total_s: 22.503746271133423\n",
            "timers:\n",
            "  learn_time_ms: 1043.479\n",
            "timestamp: 1598282092\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 3780\n",
            "training_iteration: 21\n",
            "\n",
            "checkpoint saved at /content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down/chkpt/checkpoint_21/checkpoint-21\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.006382986565749175\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.006382986565749175\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.00510638925259934\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.00510638925259934\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.00510638925259934\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.00510638925259934\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.04304266801475799\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.04304266801475799\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.04304266801475799\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.04304266801475799\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05165120161770959\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05165120161770959\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "training loop = 22 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-54\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 396\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.00427442126803928\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.035483633943169175\n",
            "      entropy: 0.001119351984622578\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 2.0723232394301601e-07\n",
            "      model: {}\n",
            "      policy_loss: 4.414469003677368e-07\n",
            "      total_loss: 0.811027103000217\n",
            "      vf_explained_var: -0.13773617148399353\n",
            "      vf_loss: 0.8110265731811523\n",
            "    p_3:\n",
            "      allreduce_latency: 0.005486779742770725\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.08406771096632419\n",
            "      entropy: 5.063160912754938e-08\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -2.2194624821211432e-13\n",
            "      model: {}\n",
            "      policy_loss: -7.450580596923828e-09\n",
            "      total_loss: 1.2735854254828558\n",
            "      vf_explained_var: -0.10464856773614883\n",
            "      vf_loss: 1.2735854387283325\n",
            "  num_steps_sampled: 3960\n",
            "  num_steps_trained: 3960\n",
            "iterations_since_restore: 22\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.2\n",
            "  ram_util_percent: 49.0\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 3.0\n",
            "  p_1: 0.0\n",
            "  p_2: 1.0\n",
            "  p_3: -2.0\n",
            "  p_4: 0.0\n",
            "  p_5: 6.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.5977011494252874\n",
            "  p_1: 0.0\n",
            "  p_2: 0.38095238095238093\n",
            "  p_3: -3.13953488372093\n",
            "  p_4: 0.0\n",
            "  p_5: 5.769230769230769\n",
            "policy_reward_min:\n",
            "  p_0: -1.0\n",
            "  p_1: 0.0\n",
            "  p_2: -3.0\n",
            "  p_3: -6.0\n",
            "  p_4: 0.0\n",
            "  p_5: 3.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.18316378236559125\n",
            "  mean_env_wait_ms: 0.07475020788525671\n",
            "  mean_inference_ms: 4.125443595122107\n",
            "  mean_raw_obs_processing_ms: 3.0125773717290496\n",
            "time_since_restore: 23.5845787525177\n",
            "time_this_iter_s: 1.0808324813842773\n",
            "time_total_s: 23.5845787525177\n",
            "timers:\n",
            "  learn_time_ms: 1043.064\n",
            "timestamp: 1598282094\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 3960\n",
            "training_iteration: 22\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.03443413441180639\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.03443413441180639\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.006127667103119208\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.006127667103119208\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.060883733851551505\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9448098596271475\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.060883733851551505\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9448098596271475\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.03443413441180639\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.03443413441180639\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.004902133682495367\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.004902133682495367\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.04132096129416768\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.04132096129416768\n",
            "training loop = 23 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-56\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 414\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.005135324266221788\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.02838690715453534\n",
            "      entropy: 0.001808909763995972\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -9.578422922719278e-08\n",
            "      model: {}\n",
            "      policy_loss: -1.2810859415266248e-07\n",
            "      total_loss: 0.379627052280638\n",
            "      vf_explained_var: -0.12273846566677094\n",
            "      vf_loss: 0.37962720294793445\n",
            "    p_1:\n",
            "      allreduce_latency: 0.0047234296798706055\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.03261347878757297\n",
            "      entropy: 4.981442162469799e-08\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 3.5237907036744787e-09\n",
            "      model: {}\n",
            "      policy_loss: -4.656612873077393e-06\n",
            "      total_loss: 0.007434219752515976\n",
            "      vf_explained_var: 'null'\n",
            "      vf_loss: 0.007438875734806061\n",
            "    p_3:\n",
            "      allreduce_latency: 0.004048506418863933\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.006382986565749174\n",
            "      entropy: 5.063219710166322e-08\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -2.9082103749928506e-14\n",
            "      model: {}\n",
            "      policy_loss: -3.042320410410563e-08\n",
            "      total_loss: 0.7423727512359619\n",
            "      vf_explained_var: -0.12207835912704468\n",
            "      vf_loss: 0.7423727711041769\n",
            "  num_steps_sampled: 4140\n",
            "  num_steps_trained: 4140\n",
            "iterations_since_restore: 23\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 83.0\n",
            "  ram_util_percent: 49.0\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 2.0\n",
            "  p_1: 0.0\n",
            "  p_2: 1.0\n",
            "  p_3: -2.0\n",
            "  p_4: 0.0\n",
            "  p_5: 6.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.6185567010309279\n",
            "  p_1: 0.0\n",
            "  p_2: 0.9230769230769231\n",
            "  p_3: -2.3076923076923075\n",
            "  p_4: 0.0\n",
            "  p_5: 6.0\n",
            "policy_reward_min:\n",
            "  p_0: -1.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: -6.0\n",
            "  p_4: 0.0\n",
            "  p_5: 6.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.18266133886886557\n",
            "  mean_env_wait_ms: 0.07389145691905538\n",
            "  mean_inference_ms: 4.119169708871354\n",
            "  mean_raw_obs_processing_ms: 3.0131230066998427\n",
            "time_since_restore: 24.66041398048401\n",
            "time_this_iter_s: 1.0758352279663086\n",
            "time_total_s: 24.66041398048401\n",
            "timers:\n",
            "  learn_time_ms: 1045.371\n",
            "timestamp: 1598282096\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 4140\n",
            "training_iteration: 23\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.04132096129416767\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.04132096129416767\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.03305676903533414\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.03305676903533414\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.04870698708124121\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9448098596271475\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.04870698708124121\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9448098596271475\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.027547307529445116\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.027547307529445116\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.003921706945996294\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.003921706945996294\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.948275111317446\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07564325750375679\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07564325750375679\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "training loop = 24 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-14-58\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 432\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.004125701056586372\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.006382986565749175\n",
            "      entropy: 0.0013618143922131923\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.213362308286984e-05\n",
            "      model: {}\n",
            "      policy_loss: -2.207110325495402e-05\n",
            "      total_loss: 0.07179817691859272\n",
            "      vf_explained_var: -0.4601425528526306\n",
            "      vf_loss: 0.07181781768384907\n",
            "    p_1:\n",
            "      allreduce_latency: 0.003873586654663086\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.00510638925259934\n",
            "      entropy: 4.8543521794878565e-08\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 2.3378469877581114e-11\n",
            "      model: {}\n",
            "      policy_loss: 0.00022351741790771484\n",
            "      total_loss: 0.0030538788608585796\n",
            "      vf_explained_var: 'null'\n",
            "      vf_loss: 0.0028303614429508648\n",
            "    p_4:\n",
            "      allreduce_latency: 0.003786484400431315\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.043042668014757986\n",
            "      entropy: 3.467761477319679e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -8.599282212233087e-09\n",
            "      model: {}\n",
            "      policy_loss: 1.0834385951360066e-07\n",
            "      total_loss: 0.29921343363821507\n",
            "      vf_explained_var: -0.20453281700611115\n",
            "      vf_loss: 0.29921333119273186\n",
            "  num_steps_sampled: 4320\n",
            "  num_steps_trained: 4320\n",
            "iterations_since_restore: 24\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 79.45\n",
            "  ram_util_percent: 49.0\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 2.0\n",
            "  p_1: 0.0\n",
            "  p_2: 1.0\n",
            "  p_3: -2.0\n",
            "  p_4: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.69\n",
            "  p_1: 0.0\n",
            "  p_2: 1.0\n",
            "  p_3: -2.0\n",
            "  p_4: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: -1.0\n",
            "  p_1: 0.0\n",
            "  p_2: 1.0\n",
            "  p_3: -2.0\n",
            "  p_4: 0.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.18226439055323176\n",
            "  mean_env_wait_ms: 0.07311127674044418\n",
            "  mean_inference_ms: 4.1128572592355805\n",
            "  mean_raw_obs_processing_ms: 3.013163012026847\n",
            "time_since_restore: 25.70715570449829\n",
            "time_this_iter_s: 1.0467417240142822\n",
            "time_total_s: 25.70715570449829\n",
            "timers:\n",
            "  learn_time_ms: 1044.44\n",
            "timestamp: 1598282098\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 4320\n",
            "training_iteration: 24\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.022037846023556094\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.022037846023556094\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.017630276818844878\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.017630276818844878\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.03896558966499297\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9448098596271475\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.03896558966499297\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9448098596271475\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.03305676903533414\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.03305676903533414\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.02644541522826731\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.02644541522826731\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.09077190900450814\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.09077190900450814\n",
            "training loop = 25 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-15-00\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 450\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.004371934466891819\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.03443413441180639\n",
            "      entropy: 0.0006403669184593069\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 8.064340363490567e-06\n",
            "      model: {}\n",
            "      policy_loss: -1.3775709602567884e-05\n",
            "      total_loss: 4.1266913678911\n",
            "      vf_explained_var: 0.3443080484867096\n",
            "      vf_loss: 4.126703447765774\n",
            "    p_4:\n",
            "      allreduce_latency: 0.004773696263631185\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.004902133682495367\n",
            "      entropy: 3.383065268280916e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 5.982162676332337e-08\n",
            "      model: {}\n",
            "      policy_loss: 2.667307891594343e-07\n",
            "      total_loss: 0.17682021856307983\n",
            "      vf_explained_var: 'null'\n",
            "      vf_loss: 0.17681994289159775\n",
            "    p_5:\n",
            "      allreduce_latency: 0.005146543184916179\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.04132096129416768\n",
            "      entropy: 1.404332446478899e-06\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 4.175336935456168e-07\n",
            "      model: {}\n",
            "      policy_loss: -2.1109978357950847e-08\n",
            "      total_loss: 2.952366352081299\n",
            "      vf_explained_var: 0.26938125491142273\n",
            "      vf_loss: 2.952366213003794\n",
            "  num_steps_sampled: 4500\n",
            "  num_steps_trained: 4500\n",
            "iterations_since_restore: 25\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 81.3\n",
            "  ram_util_percent: 49.0\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 2.0\n",
            "  p_1: 0.0\n",
            "  p_3: -2.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.24\n",
            "  p_1: 0.0\n",
            "  p_3: -2.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "policy_reward_min:\n",
            "  p_0: -8.0\n",
            "  p_1: 0.0\n",
            "  p_3: -2.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.18193396171720402\n",
            "  mean_env_wait_ms: 0.07239964038392913\n",
            "  mean_inference_ms: 4.1125594663349725\n",
            "  mean_raw_obs_processing_ms: 3.0144242515425765\n",
            "time_since_restore: 26.78303337097168\n",
            "time_this_iter_s: 1.0758776664733887\n",
            "time_total_s: 26.78303337097168\n",
            "timers:\n",
            "  learn_time_ms: 1048.413\n",
            "timestamp: 1598282100\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 4500\n",
            "training_iteration: 25\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.017630276818844878\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.017630276818844878\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0919332333026551\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9079470899332867\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0919332333026551\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9079470899332867\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.041181873283398125\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9265929353603639\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.041181873283398125\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9265929353603639\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.02115633218261385\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.02115633218261385\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.014104221455075902\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.014104221455075902\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.10892629080540976\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.10892629080540976\n",
            "training loop = 26 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-15-02\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 468\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.0035239855448404946\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.04132096129416767\n",
            "      entropy: 0.000349641334550621\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 2.3748271107450275e-05\n",
            "      model: {}\n",
            "      policy_loss: -5.4811437924702964e-05\n",
            "      total_loss: 4.143097688754399\n",
            "      vf_explained_var: 0.14235171675682068\n",
            "      vf_loss: 4.143147561285231\n",
            "    p_4:\n",
            "      allreduce_latency: 0.0038983027140299478\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.003921706945996293\n",
            "      entropy: 3.2585237325596004e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -4.446366173027627e-07\n",
            "      model: {}\n",
            "      policy_loss: 5.844980477398565e-07\n",
            "      total_loss: 0.12838376561800638\n",
            "      vf_explained_var: -0.15084488689899445\n",
            "      vf_loss: 0.12838328319291273\n",
            "    p_5:\n",
            "      allreduce_latency: 0.004004637400309245\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.07564325750375679\n",
            "      entropy: 1.100516442420485e-06\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.0432901061581864e-08\n",
            "      model: {}\n",
            "      policy_loss: -9.934107462565104e-09\n",
            "      total_loss: 2.7362543741861978\n",
            "      vf_explained_var: 0.31130483746528625\n",
            "      vf_loss: 2.7362543741861978\n",
            "  num_steps_sampled: 4680\n",
            "  num_steps_trained: 4680\n",
            "iterations_since_restore: 26\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.46666666666667\n",
            "  ram_util_percent: 49.0\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 2.0\n",
            "  p_1: 0.0\n",
            "  p_3: -2.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.82\n",
            "  p_1: 0.0\n",
            "  p_3: -2.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "policy_reward_min:\n",
            "  p_0: -8.0\n",
            "  p_1: 0.0\n",
            "  p_3: -2.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.18135327614651636\n",
            "  mean_env_wait_ms: 0.0717751374186698\n",
            "  mean_inference_ms: 4.10963702351745\n",
            "  mean_raw_obs_processing_ms: 3.0194844696643184\n",
            "time_since_restore: 27.800485610961914\n",
            "time_this_iter_s: 1.0174522399902344\n",
            "time_total_s: 27.800485610961914\n",
            "timers:\n",
            "  learn_time_ms: 1046.935\n",
            "timestamp: 1598282102\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 4680\n",
            "training_iteration: 26\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.02115633218261385\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.02115633218261385\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07354658664212409\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9079470899332867\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07354658664212409\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9079470899332867\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0734904632441119\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0734904632441119\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05879237059528952\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05879237059528952\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.08714103264432782\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.08714103264432782\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.1307115489664917\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.1307115489664917\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9883105962559157training loop = 27 of 30\n",
            "\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-15-03\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 486\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.005360205968221028\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.02203784602355609\n",
            "      entropy: 0.00022903190013907272\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 9.502495762084155e-07\n",
            "      model: {}\n",
            "      policy_loss: -1.0581066211064657e-05\n",
            "      total_loss: 1.4046010772387187\n",
            "      vf_explained_var: -0.7848957180976868\n",
            "      vf_loss: 1.4046115477879841\n",
            "    p_1:\n",
            "      allreduce_latency: 0.003755807876586914\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.017630276818844878\n",
            "      entropy: 8.431227555168637e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 5.6901894822658505e-06\n",
            "      model: {}\n",
            "      policy_loss: -1.6316771507263184e-06\n",
            "      total_loss: 1.095333571235339\n",
            "      vf_explained_var: 0.2748487889766693\n",
            "      vf_loss: 1.0953340778748195\n",
            "    p_3:\n",
            "      allreduce_latency: 0.004448652267456055\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.03305676903533414\n",
            "      entropy: 1.541507869499507e-08\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 2.2444130403602485e-43\n",
            "      model: {}\n",
            "      policy_loss: 4.0978193283081055e-08\n",
            "      total_loss: 1.7218753397464752\n",
            "      vf_explained_var: -0.3015122711658478\n",
            "      vf_loss: 1.7218753695487976\n",
            "    p_4:\n",
            "      allreduce_latency: 0.004712422688802083\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.02644541522826731\n",
            "      entropy: 3.214582829969004e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 7.969776838478992e-09\n",
            "      model: {}\n",
            "      policy_loss: -3.799796092588773e-08\n",
            "      total_loss: 0.07510725532968839\n",
            "      vf_explained_var: 0.015057146549224854\n",
            "      vf_loss: 0.07510728140672047\n",
            "  num_steps_sampled: 4860\n",
            "  num_steps_trained: 4860\n",
            "iterations_since_restore: 27\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 80.9\n",
            "  ram_util_percent: 49.0\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 2.0\n",
            "  p_1: 3.0\n",
            "  p_3: -2.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_0: -1.2727272727272727\n",
            "  p_1: 1.2\n",
            "  p_3: -2.4285714285714284\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "policy_reward_min:\n",
            "  p_0: -8.0\n",
            "  p_1: 0.0\n",
            "  p_3: -3.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.1808317791009575\n",
            "  mean_env_wait_ms: 0.07118368744313056\n",
            "  mean_inference_ms: 4.109411065272147\n",
            "  mean_raw_obs_processing_ms: 3.0251264070226362\n",
            "time_since_restore: 28.872019052505493\n",
            "time_this_iter_s: 1.071533441543579\n",
            "time_total_s: 28.872019052505493\n",
            "timers:\n",
            "  learn_time_ms: 1045.888\n",
            "timestamp: 1598282103\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 4860\n",
            "training_iteration: 27\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.01692506574609108\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.01692506574609108\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.914784733341971\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06971282611546226\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06971282611546226\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.05879237059528952\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.05879237059528952\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07055084471434743\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07055084471434743\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.10456923917319338\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.10456923917319338\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.10456923917319337\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.10456923917319337\n",
            "training loop = 28 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-15-05\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 504\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.005699475606282552\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.017630276818844878\n",
            "      entropy: 0.00018237816160254446\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 2.7438779663055855e-07\n",
            "      model: {}\n",
            "      policy_loss: 2.4301310380299887e-06\n",
            "      total_loss: 0.6262904951969782\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.6262880365053812\n",
            "    p_1:\n",
            "      allreduce_latency: 0.006958484649658203\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.09193323330265508\n",
            "      entropy: 0.00011720356754570578\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 7.308295361478182e-07\n",
            "      model: {}\n",
            "      policy_loss: 1.9359091917673745e-06\n",
            "      total_loss: 0.8689441879590353\n",
            "      vf_explained_var: 0.15454687178134918\n",
            "      vf_loss: 0.8689421017964681\n",
            "    p_3:\n",
            "      allreduce_latency: 0.004871288935343425\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.02115633218261385\n",
            "      entropy: 1.541507869499507e-08\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -4.811124727515206e-44\n",
            "      model: {}\n",
            "      policy_loss: -5.0912300745646157e-08\n",
            "      total_loss: 0.8814295927683512\n",
            "      vf_explained_var: 0.014292578212916851\n",
            "      vf_loss: 0.8814296126365662\n",
            "    p_4:\n",
            "      allreduce_latency: 0.004151741663614909\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.014104221455075902\n",
            "      entropy: 3.190759222585863e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 2.165150784586937e-08\n",
            "      model: {}\n",
            "      policy_loss: -4.6938657997467694e-08\n",
            "      total_loss: 0.05873078604539236\n",
            "      vf_explained_var: 0.22999395430088043\n",
            "      vf_loss: 0.0587308444082737\n",
            "  num_steps_sampled: 5040\n",
            "  num_steps_trained: 5040\n",
            "iterations_since_restore: 28\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.03333333333333\n",
            "  ram_util_percent: 49.0\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 2.0\n",
            "  p_1: 3.0\n",
            "  p_3: -2.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_0: -1.6829268292682926\n",
            "  p_1: 1.7419354838709677\n",
            "  p_3: -2.857142857142857\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "policy_reward_min:\n",
            "  p_0: -8.0\n",
            "  p_1: 0.0\n",
            "  p_3: -3.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.18030832752465595\n",
            "  mean_env_wait_ms: 0.07048115997279564\n",
            "  mean_inference_ms: 4.107760321350185\n",
            "  mean_raw_obs_processing_ms: 3.0303117996115727\n",
            "time_since_restore: 29.949787616729736\n",
            "time_this_iter_s: 1.0777685642242432\n",
            "time_total_s: 29.949787616729736\n",
            "timers:\n",
            "  learn_time_ms: 1047.281\n",
            "timestamp: 1598282105\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 5040\n",
            "training_iteration: 28\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07257392454664202\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9140241360021734\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07257392454664202\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.08365539133855471\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9140241360021734\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.08365539133855471\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07055084471434743\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07055084471434743\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.08466101365721691\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.08466101365721691\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0319627326737742\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9785755848116598\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0319627326737742\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9785755848116598\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.12548308700783203\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.12548308700783203\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9883105962559157\n",
            "training loop = 29 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-15-07\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 522\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.004280143313937717\n",
            "      cur_kl_coeff: 0.2\n",
            "      cur_lr: 0.02115633218261385\n",
            "      entropy: 0.0046071696819530595\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.00023441925319881798\n",
            "      model: {}\n",
            "      policy_loss: -0.0002470873296260834\n",
            "      total_loss: 0.10999808144859141\n",
            "      vf_explained_var: -0.525557816028595\n",
            "      vf_loss: 0.11019829034598337\n",
            "    p_1:\n",
            "      allreduce_latency: 0.0054847002029418945\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.07354658664212409\n",
            "      entropy: 0.0014419480382154386\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.00014375527340841168\n",
            "      model: {}\n",
            "      policy_loss: 0.0\n",
            "      total_loss: 1.5053604592879613\n",
            "      vf_explained_var: 'null'\n",
            "      vf_loss: 1.5053316485136747\n",
            "    p_4:\n",
            "      allreduce_latency: 0.004617611567179362\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.08714103264432782\n",
            "      entropy: 3.167508475598879e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.6096705894123413e-08\n",
            "      model: {}\n",
            "      policy_loss: 6.953875223795573e-08\n",
            "      total_loss: 0.09136473635832469\n",
            "      vf_explained_var: -0.0387309193611145\n",
            "      vf_loss: 0.09136464695135753\n",
            "  num_steps_sampled: 5220\n",
            "  num_steps_trained: 5220\n",
            "iterations_since_restore: 29\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.86666666666667\n",
            "  ram_util_percent: 49.03333333333333\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_1: 3.0\n",
            "  p_3: -3.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_0: -1.7560975609756098\n",
            "  p_1: 1.6363636363636365\n",
            "  p_3: -3.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "policy_reward_min:\n",
            "  p_0: -8.0\n",
            "  p_1: 0.0\n",
            "  p_3: -3.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.17981444559096527\n",
            "  mean_env_wait_ms: 0.06980505606370517\n",
            "  mean_inference_ms: 4.105929180365951\n",
            "  mean_raw_obs_processing_ms: 3.035752171832902\n",
            "time_since_restore: 31.032045602798462\n",
            "time_this_iter_s: 1.0822579860687256\n",
            "time_total_s: 31.032045602798462\n",
            "timers:\n",
            "  learn_time_ms: 1051.501\n",
            "timestamp: 1598282107\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 5220\n",
            "training_iteration: 29\n",
            "\n",
            "trainer.train() result: <ray.rllib.agents.trainer_template.Custom_Trainer object at 0x7f6c38421128> -> 18 episodes\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.0010251651300613037\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9015422318789823\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.0010251651300613037\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.07796100276357777\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9015422318789823\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9043406470465906\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.07796100276357777\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9043406470465906\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06772881092577353\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06772881092577353\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.06772881092577353\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.06772881092577353\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9262822730513068\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.000820132104049043\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9015422318789823\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.000820132104049043\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9015422318789823\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_lr_schedule, lr=0.09355320331629333\n",
            "\u001b[2m\u001b[36m(pid=2008)\u001b[0m update_gamma, gamma=0.9043406470465906\n",
            "\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_lr_schedule, lr=0.09355320331629333\n",
            "training loop = 30 of 30\u001b[2m\u001b[36m(pid=2095)\u001b[0m update_gamma, gamma=0.9043406470465906\n",
            "\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-24_15-15-09\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 540\n",
            "experiment_id: f9aaf1838dcd470e8dc632af46246b00\n",
            "hostname: 2649891d73c4\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      allreduce_latency: 0.003775993982950846\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.01692506574609108\n",
            "      entropy: 0.021522797644138336\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.00037861809444924194\n",
            "      model: {}\n",
            "      policy_loss: 0.0\n",
            "      total_loss: 0.011355260047518337\n",
            "      vf_explained_var: 'null'\n",
            "      vf_loss: 0.01127953613953044\n",
            "    p_1:\n",
            "      allreduce_latency: 0.0048656463623046875\n",
            "      cur_kl_coeff: 0.20000000000000004\n",
            "      cur_lr: 0.06971282611546226\n",
            "      entropy: 0.0010319909391303856\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.8489784755123158e-06\n",
            "      model: {}\n",
            "      policy_loss: 0.0\n",
            "      total_loss: 0.5136626660823822\n",
            "      vf_explained_var: 'null'\n",
            "      vf_loss: 0.5136622687180837\n",
            "    p_2:\n",
            "      allreduce_latency: 0.005552967389424642\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.05879237059528952\n",
            "      entropy: 6.48281578226791e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 7.619119344326464e-07\n",
            "      model: {}\n",
            "      policy_loss: 1.3050933678944905e-06\n",
            "      total_loss: 0.515550342698892\n",
            "      vf_explained_var: -0.1771218627691269\n",
            "      vf_loss: 0.5155488749345144\n",
            "    p_5:\n",
            "      allreduce_latency: 0.004690011342366536\n",
            "      cur_kl_coeff: 0.19999999999999998\n",
            "      cur_lr: 0.10456923917319337\n",
            "      entropy: 2.2828155010756745e-07\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -3.148710396077304e-09\n",
            "      model: {}\n",
            "      policy_loss: 3.8370490074157715e-07\n",
            "      total_loss: 1.2383560637633007\n",
            "      vf_explained_var: -0.6535923480987549\n",
            "      vf_loss: 1.2383556986848514\n",
            "  num_steps_sampled: 5400\n",
            "  num_steps_trained: 5400\n",
            "iterations_since_restore: 30\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 82.86666666666667\n",
            "  ram_util_percent: 49.1\n",
            "pid: 1895\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_1: 3.0\n",
            "  p_2: 1.0\n",
            "  p_3: -3.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_0: -1.4857142857142858\n",
            "  p_1: 1.5\n",
            "  p_2: 1.0\n",
            "  p_3: -3.0\n",
            "  p_4: 0.0\n",
            "  p_5: 3.68\n",
            "policy_reward_min:\n",
            "  p_0: -8.0\n",
            "  p_1: 0.0\n",
            "  p_2: 1.0\n",
            "  p_3: -3.0\n",
            "  p_4: 0.0\n",
            "  p_5: -1.0\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.1794586979393177\n",
            "  mean_env_wait_ms: 0.06918936989856365\n",
            "  mean_inference_ms: 4.104942307904958\n",
            "  mean_raw_obs_processing_ms: 3.0370040738642676\n",
            "time_since_restore: 32.06201457977295\n",
            "time_this_iter_s: 1.0299689769744873\n",
            "time_total_s: 32.06201457977295\n",
            "timers:\n",
            "  learn_time_ms: 1050.857\n",
            "timestamp: 1598282109\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 5400\n",
            "training_iteration: 30\n",
            "\n",
            "checkpoint saved at /content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down/chkpt/checkpoint_30/checkpoint-30\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}