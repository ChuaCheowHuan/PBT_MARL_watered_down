{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PBT_MARL_water_down.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwD3_kI2HDbA",
        "colab_type": "text"
      },
      "source": [
        "#Setup Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyAKAl49kg7I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a12303ea-262e-4447-d67f-fc4c79a70bcd"
      },
      "source": [
        "from google.colab import drive \n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "%cd \"/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down/\"\n",
        "!pwd\n",
        "!ls -l\n",
        "\n",
        "# Install if you haven't done so.\n",
        "!pip install tensorflow==2.2.0\n",
        "!pip install ray[rllib]==0.8.6   \n",
        "#!pip show tensorflow\n",
        "#!pip show ray\n",
        "\n",
        "!rm -rf ~/ray_results/DDPPO_*\n",
        "!rm -rf ~/ray_results/PPO_*"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down\n",
            "/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down\n",
            "total 158\n",
            "drwx------ 6 root root   4096 Aug  4 07:50 chkpt\n",
            "-rw------- 1 root root   3139 Jun 11 03:59 Helper.py\n",
            "-rw------- 1 root root      1 Jun 11 03:59 __init__.py\n",
            "-rw------- 1 root root   1072 Jun 10 04:55 LICENSE\n",
            "-rw------- 1 root root   6021 Aug  4 04:44 PBT_MARL_old.py\n",
            "-rw------- 1 root root   6258 Aug  4 06:55 PBT_MARL.py\n",
            "-rw------- 1 root root   9014 Jun 10 17:23 pbt_marl_water_down_cpu_only.py\n",
            "-rw------- 1 root root 115164 Aug  4 08:00 PBT_MARL_water_down.ipynb\n",
            "drwx------ 2 root root   4096 Jul 31 11:40 __pycache__\n",
            "drwx------ 2 root root   4096 Aug  4 07:58 ray_results\n",
            "-rw------- 1 root root   3705 Jul 28 05:35 README.md\n",
            "-rw------- 1 root root   2056 Aug  4 06:41 RockPaperScissorsEnv.py\n",
            "Requirement already satisfied: tensorflow==2.2.0 in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.18.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.30.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (2.2.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (2.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (49.2.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
            "Requirement already satisfied: ray[rllib]==0.8.6 in /usr/local/lib/python3.6/dist-packages (0.8.6)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (3.0.12)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (7.1.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (1.0.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (0.4.3)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (1.30.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (2.6.0)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (2.0.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (3.6.2)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (3.13)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (0.3.3)\n",
            "Requirement already satisfied: redis<3.5.0,>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (3.4.1)\n",
            "Requirement already satisfied: scipy; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (1.4.1)\n",
            "Requirement already satisfied: gym[atari]; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (0.17.2)\n",
            "Requirement already satisfied: tabulate; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (0.8.7)\n",
            "Requirement already satisfied: opencv-python-headless; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (4.3.0.36)\n",
            "Requirement already satisfied: tensorboardX; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (2.1)\n",
            "Requirement already satisfied: pandas; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (1.0.5)\n",
            "Requirement already satisfied: dm-tree; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (0.1.5)\n",
            "Requirement already satisfied: atari-py; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (0.2.6)\n",
            "Requirement already satisfied: lz4; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]==0.8.6) (3.1.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray[rllib]==0.8.6) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray[rllib]==0.8.6) (49.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray[rllib]==0.8.6) (4.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]==0.8.6) (19.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]==0.8.6) (3.7.4.2)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]==0.8.6) (1.1.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]==0.8.6) (3.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]==0.8.6) (1.5.1)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]==0.8.6) (3.0.1)\n",
            "Requirement already satisfied: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]==0.8.6) (4.7.6)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]==0.8.6) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]==0.8.6) (1.5.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]==0.8.6) (7.0.0)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]==0.8.6) (4.1.2.30)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas; extra == \"rllib\"->ray[rllib]==0.8.6) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas; extra == \"rllib\"->ray[rllib]==0.8.6) (2018.9)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from idna-ssl>=1.0; python_version < \"3.7\"->aiohttp->ray[rllib]==0.8.6) (2.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]; extra == \"rllib\"->ray[rllib]==0.8.6) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BMjPt0fbNSf",
        "colab_type": "text"
      },
      "source": [
        "#Chkpt/restore & log path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQMyQcPpbIai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g_drive_path = \"/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down/\"\n",
        "\n",
        "local_dir = g_drive_path + \"chkpt/\"\n",
        "chkpt_freq = 10\n",
        "chkpt = 150\n",
        "restore_path = \"{}checkpoint_{}/checkpoint-{}\".format(local_dir, chkpt, chkpt)\n",
        "is_restore = False\n",
        "\n",
        "log_dir = g_drive_path + \"ray_results/\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-GBqoxsHBZV",
        "colab_type": "text"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8DRdL7tgKBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "from typing import Dict\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from gym.spaces import Discrete\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.models import ModelCatalog\n",
        "\n",
        "from ray.rllib.policy import Policy\n",
        "\n",
        "from ray.rllib.agents.ppo import ppo\n",
        "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
        "from ray.rllib.agents.ppo import appo\n",
        "from ray.rllib.agents.ppo.appo import APPOTrainer\n",
        "from ray.rllib.agents.ppo import ddppo\n",
        "from ray.rllib.agents.ppo.ddppo import DDPPOTrainer\n",
        "\n",
        "from ray.rllib.env import BaseEnv\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "\n",
        "from ray.rllib.policy.sample_batch import SampleBatch\n",
        "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
        "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
        "\n",
        "from ray.rllib.utils.schedules import ConstantSchedule\n",
        "from ray.rllib.utils import try_import_tf\n",
        "from ray.tune.logger import pretty_print\n",
        "\n",
        "tf = try_import_tf()\n",
        "\n",
        "from RockPaperScissorsEnv import RockPaperScissorsEnv\n",
        "from Helper import Helper\n",
        "from PBT_MARL import PBT_MARL"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFRqbhqAunwM",
        "colab_type": "text"
      },
      "source": [
        "#Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBp3zwiEuqM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"#Callbacks\"\"\"\n",
        "\n",
        "class MyCallbacks(DefaultCallbacks):\n",
        "    def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                         policies: Dict[str, Policy],\n",
        "                         episode: MultiAgentEpisode, **kwargs):\n",
        "        #print(\"on_episode_start {}, _agent_to_policy {}\".format(episode.episode_id, episode._agent_to_policy))\n",
        "        #episode.hist_data[\"episode_id\"] = []\n",
        "        pass\n",
        "\n",
        "    def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                        episode: MultiAgentEpisode, **kwargs):\n",
        "        \"\"\"\n",
        "        pole_angle = abs(episode.last_observation_for()[2])\n",
        "        raw_angle = abs(episode.last_raw_obs_for()[2])\n",
        "        assert pole_angle == raw_angle\n",
        "        episode.user_data[\"pole_angles\"].append(pole_angle)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
        "                       **kwargs):\n",
        "        #print(\"on_episode_end {}, episode.agent_rewards {}\".format(episode.episode_id, episode.agent_rewards))\n",
        "\n",
        "        player_policy = []\n",
        "        score = []\n",
        "        for k,v in episode.agent_rewards.items():\n",
        "            player_policy.append(k)\n",
        "            score.append(v)\n",
        "\n",
        "        pol_i_key = player_policy[0][1]\n",
        "        pol_j_key = player_policy[1][1]\n",
        "        _, str_i = pol_i_key.split(\"_\")\n",
        "        _, str_j = pol_j_key.split(\"_\")\n",
        "        agt_i_key = \"agt_\" + str_i\n",
        "        agt_j_key = \"agt_\" + str_j\n",
        "\n",
        "        g_helper = ray.get_actor(\"g_helper\")     \n",
        "        prev_rating_i = ray.get(g_helper.get_rating.remote(agt_i_key))\n",
        "        prev_rating_j = ray.get(g_helper.get_rating.remote(agt_j_key))\n",
        "        score_i = score[0]\n",
        "        score_j = score[1]\n",
        "        rating_i, rating_j = l_PBT_MARL.compute_rating(prev_rating_i, prev_rating_j, score_i, score_j)\n",
        "        ray.get(g_helper.update_rating.remote(agt_i_key, agt_j_key, rating_i, rating_j, score_i, score_j))\n",
        "        #print(\"on_episode_end ray.get(g_helper.get_agt_store.remote())\", ray.get(g_helper.get_agt_store.remote()))\n",
        "\n",
        "    def on_sample_end(self, worker: RolloutWorker, samples: SampleBatch,\n",
        "                      **kwargs):\n",
        "        #print(\"on_sample_end returned sample batch of size {}\".format(samples.count))\n",
        "        pass\n",
        "\n",
        "    def on_train_result(self, trainer, result: dict, **kwargs):\n",
        "        print(\"trainer.train() result: {} -> {} episodes\".format(trainer, result[\"episodes_this_iter\"]))\n",
        "        # you can mutate the result dict to add new fields to return\n",
        "        result[\"callback_ok\"] = True\n",
        "        #print(\"on_train_result result\", result)\n",
        "\n",
        "        l_PBT_MARL.PBT(trainer)     # perform PBT\n",
        "\n",
        "        g_helper = ray.get_actor(\"g_helper\")     \n",
        "        ray.get(g_helper.set_pair.remote())     # set the lastest pair\n",
        "        #print(\"on_train_result g_helper.get_pair.remote()\", ray.get(g_helper.get_pair.remote()))\n",
        "\n",
        "    def on_postprocess_trajectory(\n",
        "            self, worker: RolloutWorker, episode: MultiAgentEpisode,\n",
        "            agent_id: str, policy_id: str, policies: Dict[str, Policy],\n",
        "            postprocessed_batch: SampleBatch,\n",
        "            original_batches: Dict[str, SampleBatch], **kwargs):\n",
        "        #\u0010print(\"postprocessed {}, {}, {}, {} steps\".format(episode, agent_id, policy_id, postprocessed_batch.count))              \n",
        "        \"\"\"\n",
        "        if \"num_batches\" not in episode.custom_metrics:\n",
        "            episode.custom_metrics[\"num_batches\"] = 0\n",
        "        episode.custom_metrics[\"num_batches\"] += 1\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpcJGyAaBbc2",
        "colab_type": "text"
      },
      "source": [
        "#Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMZ20pVCzxUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_policies(population_size, obs_space, act_space, use_lstm, hyperparameters_range):\n",
        "    \"\"\"\n",
        "    Sample hyper-parameter from the hyper-parameter distribution.\n",
        "    \"\"\"\n",
        "    policies = {}\n",
        "    for i in range(population_size):\n",
        "        pol_key = \"p_\" + str(i)\n",
        "        lr = np.random.uniform(low=hyperparameters_range[\"lr\"][0], high=hyperparameters_range[\"lr\"][1], size=None)\n",
        "        gamma = np.random.uniform(low=hyperparameters_range[\"gamma\"][0], high=hyperparameters_range[\"gamma\"][1], size=None)\n",
        "        policies[pol_key] = (None, obs_space, act_space, {\"model\": {\"use_lstm\": use_lstm},\n",
        "                                                          \"lr\": lr,\n",
        "                                                          \"gamma\": gamma})\n",
        "    return policies\n",
        "\n",
        "def train_policies(population_size):    \n",
        "    train_policies = []\n",
        "    for i in range(population_size):\n",
        "        pol_key = \"p_\" + str(i)\n",
        "        train_policies.append(pol_key)\n",
        "\n",
        "    return policies\n",
        "\n",
        "def select_policy(agent_id):\n",
        "    _, i = agent_id.split(\"_\")\n",
        "    policy = \"p_\" + str(i)\n",
        "    #print(\"select_policy {} {}\".format(agent_id , policy))\n",
        "    return policy     "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAEySBSfBS5u",
        "colab_type": "text"
      },
      "source": [
        "#Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trdlnMoHwbfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2e5191c7-88a0-46a3-922c-ea7cbb139343"
      },
      "source": [
        "population_size = 6\n",
        "K = 0.1     \n",
        "T_select = 0.77 #0.47\n",
        "binomial_n = 1\n",
        "inherit_prob = 0.5\n",
        "perturb_prob = 0.1\n",
        "perturb_val = [0.8, 1.2]\n",
        "hyperparameters_range = {\"lr\": [1, 1], \n",
        "                         \"gamma\": [0.9, 0.999]}\n",
        "\n",
        "register_env(\"RockPaperScissorsEnv\", lambda _: RockPaperScissorsEnv(_, population_size))     # register RockPaperScissorsEnv with RLlib     \n",
        "# get obs & act spaces from dummy CDA env\n",
        "dummy_env = RockPaperScissorsEnv(_, population_size=0)\n",
        "obs_space = dummy_env.observation_space\n",
        "act_space = dummy_env.action_space\n",
        "\n",
        "use_lstm=False\n",
        "policies = init_policies(population_size, obs_space, act_space, use_lstm, hyperparameters_range)\n",
        "train_policies = train_policies(population_size)\n",
        "\n",
        "l_PBT_MARL = PBT_MARL(population_size, \n",
        "                      K, T_select, \n",
        "                      binomial_n, inherit_prob,\n",
        "                      perturb_prob, perturb_val)\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init(ignore_reinit_error=True, log_to_driver=True, webui_host='127.0.0.1', num_cpus=2, num_gpus=1)      #start ray\n",
        "#print(\"ray.nodes()\", ray.nodes())\n",
        "\n",
        "g_helper = Helper.options(name=\"g_helper\").remote(population_size, policies)      # this object runs on a different ray actor process\n",
        "ray.get(g_helper.set_pair.remote())\n",
        "\n",
        "num_iters = 30     # num of main training loop"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:01:06,156\tINFO resource_spec.py:212 -- Starting Ray with 7.28 GiB memory available for workers and up to 3.64 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
            "2020-08-04 08:01:06,410\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
            "2020-08-04 08:01:06,764\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32m127.0.0.1:8265\u001b[39m\u001b[22m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyucHLoqBe5G",
        "colab_type": "text"
      },
      "source": [
        "#Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNWNnavQt9y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_config():\n",
        "    #config = ddppo.DEFAULT_CONFIG.copy()\n",
        "    config = ppo.DEFAULT_CONFIG.copy()\n",
        "\n",
        "    config[\"env\"] = RockPaperScissorsEnv\n",
        "    config[\"multiagent\"] = {\"policies_to_train\": train_policies,\n",
        "                            \"policies\": policies,\n",
        "                            \"policy_mapping_fn\": select_policy}        \n",
        "    config[\"num_cpus_per_worker\"] = 0.25                                \n",
        "    config[\"num_gpus_per_worker\"] = 0.125\n",
        "    config[\"num_workers\"] = 2      \n",
        "    config[\"num_envs_per_worker\"] = 3\n",
        "    config[\"rollout_fragment_length\"] = 30                  \n",
        "    #config[\"train_batch_size\"] = -1     # must be -1 for DDPPO trainer \n",
        "    config[\"train_batch_size\"] = 90                                                     \n",
        "    config[\"sgd_minibatch_size\"] = 10                       \n",
        "    config[\"num_sgd_iter\"] = 3      # number of epochs to execute per train batch.\n",
        "    config[\"callbacks\"] = MyCallbacks\n",
        "    config[\"log_level\"] = \"WARN\"      # WARN/INFO/DEBUG \n",
        "    config[\"output\"] = log_dir\n",
        "\n",
        "    return config"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb_4cEGdBlqf",
        "colab_type": "text"
      },
      "source": [
        "#Go train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUB40TYSuDAn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15959100-c405-4db8-9147-0498261447ab"
      },
      "source": [
        "def go_train(config):     \n",
        "    #trainer = ddppo.DDPPOTrainer(config=config, env=\"RockPaperScissorsEnv\")         \n",
        "    trainer = ppo.PPOTrainer(config=config, env=\"RockPaperScissorsEnv\")         \n",
        "\n",
        "    if is_restore == True:\n",
        "        trainer.restore(restore_path) \n",
        "    \n",
        "    result = None\n",
        "    for i in range(num_iters):\n",
        "        result = trainer.train()       \n",
        "        print(\"training loop = {} of {}\".format(i + 1, num_iters))            \n",
        "        print(pretty_print(result))     # includes result[\"custom_metrics\"]\n",
        "\n",
        "        #p_0 = trainer.get_policy('p_0')\n",
        "        #p_0.lr_schedule = ConstantSchedule(0.3, framework=None)\n",
        "\n",
        "        if i % chkpt_freq == 0:\n",
        "            checkpoint = trainer.save(local_dir)\n",
        "            print(\"checkpoint saved at\", checkpoint)\n",
        "    \n",
        "    checkpoint = trainer.save(local_dir)\n",
        "    print(\"checkpoint saved at\", checkpoint)\n",
        "    \n",
        "\n",
        "# run everything\n",
        "go_train(get_config())    \n",
        "\n",
        "ray.shutdown()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:01:10,348\tINFO trainer.py:585 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
            "2020-08-04 08:01:10,354\tINFO trainer.py:612 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "2020-08-04 08:02:13,892\tINFO trainable.py:181 -- _setup took 63.548 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2020-08-04 08:02:13,894\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
            "2020-08-04 08:02:21,517\tWARNING ppo.py:166 -- No data for p_0, not updating kl\n",
            "2020-08-04 08:02:21,819\tWARNING ppo.py:166 -- No data for p_2, not updating kl\n",
            "2020-08-04 08:02:21,820\tWARNING ppo.py:166 -- No data for p_3, not updating kl\n",
            "2020-08-04 08:02:22,126\tWARNING ppo.py:166 -- No data for p_5, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 1 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-22\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 18\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_1:\n",
            "      cur_kl_coeff: 0.20000000298023224\n",
            "      cur_lr: 1.0\n",
            "      entropy: 0.04228334128856659\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 50.581275939941406\n",
            "      model: {}\n",
            "      policy_loss: 0.40675604343414307\n",
            "      total_loss: 150.04600524902344\n",
            "      vf_explained_var: -0.6395921111106873\n",
            "      vf_loss: 139.52297973632812\n",
            "    p_4:\n",
            "      cur_kl_coeff: 0.20000000298023224\n",
            "      cur_lr: 1.0\n",
            "      entropy: 0.01735462248325348\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 51.05513000488281\n",
            "      model: {}\n",
            "      policy_loss: 0.4643787741661072\n",
            "      total_loss: 398.4315185546875\n",
            "      vf_explained_var: -0.888289749622345\n",
            "      vf_loss: 387.756103515625\n",
            "  num_steps_sampled: 180\n",
            "  num_steps_trained: 180\n",
            "iterations_since_restore: 1\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 75.51666666666667\n",
            "  ram_util_percent: 50.20000000000001\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_1: 8.0\n",
            "  p_4: 3.0\n",
            "policy_reward_mean:\n",
            "  p_1: 0.0\n",
            "  p_4: 0.0\n",
            "policy_reward_min:\n",
            "  p_1: -3.0\n",
            "  p_4: -8.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.4695769279233871\n",
            "  mean_inference_ms: 11.05456582961544\n",
            "  mean_processing_ms: 5.348628567111108\n",
            "time_since_restore: 8.234994649887085\n",
            "time_this_iter_s: 8.234994649887085\n",
            "time_total_s: 8.234994649887085\n",
            "timers:\n",
            "  learn_throughput: 114.328\n",
            "  learn_time_ms: 1574.419\n",
            "  load_throughput: 298.069\n",
            "  load_time_ms: 603.886\n",
            "  sample_throughput: 329.394\n",
            "  sample_time_ms: 546.458\n",
            "  update_time_ms: 19.534\n",
            "timestamp: 1596528142\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 180\n",
            "training_iteration: 1\n",
            "\n",
            "checkpoint saved at /content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down/chkpt/checkpoint_1/checkpoint-1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:28,040\tWARNING ppo.py:166 -- No data for p_0, not updating kl\n",
            "2020-08-04 08:02:28,045\tWARNING ppo.py:166 -- No data for p_2, not updating kl\n",
            "2020-08-04 08:02:28,051\tWARNING ppo.py:166 -- No data for p_3, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 2 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-28\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 36\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_1:\n",
            "      cur_kl_coeff: 0.30000001192092896\n",
            "      cur_lr: 1.0\n",
            "      entropy: 1.971934552343906e-37\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 6.29649166139501e-10\n",
            "      model: {}\n",
            "      policy_loss: -5.612770905827347e-07\n",
            "      total_loss: 3.965785264968872\n",
            "      vf_explained_var: 0.21986787021160126\n",
            "      vf_loss: 3.9657859802246094\n",
            "    p_4:\n",
            "      cur_kl_coeff: 0.30000001192092896\n",
            "      cur_lr: 1.0\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.4843363302642154e-22\n",
            "      model: {}\n",
            "      policy_loss: -1.0761949553739214e-08\n",
            "      total_loss: 55.918888092041016\n",
            "      vf_explained_var: -0.4678066074848175\n",
            "      vf_loss: 55.918888092041016\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.20000000298023224\n",
            "      cur_lr: 1.0\n",
            "      entropy: 0.030603662133216858\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 69.93534851074219\n",
            "      model: {}\n",
            "      policy_loss: 0.49160799384117126\n",
            "      total_loss: 1715.4306640625\n",
            "      vf_explained_var: -0.9527872204780579\n",
            "      vf_loss: 1700.9517822265625\n",
            "  num_steps_sampled: 360\n",
            "  num_steps_trained: 360\n",
            "iterations_since_restore: 2\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 63.522222222222226\n",
            "  ram_util_percent: 50.24444444444444\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_1: 8.0\n",
            "  p_4: 3.0\n",
            "  p_5: 3.0\n",
            "policy_reward_mean:\n",
            "  p_1: 2.0\n",
            "  p_4: -1.4166666666666667\n",
            "  p_5: 0.25\n",
            "policy_reward_min:\n",
            "  p_1: -3.0\n",
            "  p_4: -8.0\n",
            "  p_5: -3.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.37307710259254495\n",
            "  mean_inference_ms: 11.407330955291538\n",
            "  mean_processing_ms: 5.262730015704773\n",
            "time_since_restore: 10.549457788467407\n",
            "time_this_iter_s: 2.3144631385803223\n",
            "time_total_s: 10.549457788467407\n",
            "timers:\n",
            "  learn_throughput: 136.57\n",
            "  learn_time_ms: 1318.002\n",
            "  load_throughput: 391.278\n",
            "  load_time_ms: 460.031\n",
            "  sample_throughput: 330.247\n",
            "  sample_time_ms: 545.047\n",
            "  update_time_ms: 17.014\n",
            "timestamp: 1596528148\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 360\n",
            "training_iteration: 2\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:30,533\tWARNING ppo.py:166 -- No data for p_0, not updating kl\n",
            "2020-08-04 08:02:30,535\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:02:30,540\tWARNING ppo.py:166 -- No data for p_2, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 3 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-30\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 54\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.20000000298023224\n",
            "      cur_lr: 1.440000057220459\n",
            "      entropy: 0.028942083939909935\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 65.32818603515625\n",
            "      model: {}\n",
            "      policy_loss: 0.537037193775177\n",
            "      total_loss: 973.3107299804688\n",
            "      vf_explained_var: -0.9384592175483704\n",
            "      vf_loss: 959.7080078125\n",
            "    p_4:\n",
            "      cur_kl_coeff: 0.15000000596046448\n",
            "      cur_lr: 1.440000057220459\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: -1.5211602644171762e-08\n",
            "      total_loss: 28.702335357666016\n",
            "      vf_explained_var: -0.5712525844573975\n",
            "      vf_loss: 28.702335357666016\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.30000001192092896\n",
            "      cur_lr: 0.800000011920929\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 3.4096681208700674e-22\n",
            "      model: {}\n",
            "      policy_loss: 5.960464477539063e-08\n",
            "      total_loss: 292.3073425292969\n",
            "      vf_explained_var: -0.8333333134651184\n",
            "      vf_loss: 292.3073425292969\n",
            "  num_steps_sampled: 540\n",
            "  num_steps_trained: 540\n",
            "iterations_since_restore: 3\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 73.15\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_1: 8.0\n",
            "  p_3: 5.0\n",
            "  p_4: 4.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_1: 2.0\n",
            "  p_3: 1.3333333333333333\n",
            "  p_4: -2.1296296296296298\n",
            "  p_5: 2.8333333333333335\n",
            "policy_reward_min:\n",
            "  p_1: -3.0\n",
            "  p_3: -4.0\n",
            "  p_4: -8.0\n",
            "  p_5: -3.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.31918298511334176\n",
            "  mean_inference_ms: 11.578865209891037\n",
            "  mean_processing_ms: 5.2757268416173195\n",
            "time_since_restore: 12.875280141830444\n",
            "time_this_iter_s: 2.325822353363037\n",
            "time_total_s: 12.875280141830444\n",
            "timers:\n",
            "  learn_throughput: 146.11\n",
            "  learn_time_ms: 1231.952\n",
            "  load_throughput: 439.297\n",
            "  load_time_ms: 409.745\n",
            "  sample_throughput: 327.878\n",
            "  sample_time_ms: 548.984\n",
            "  update_time_ms: 17.166\n",
            "timestamp: 1596528150\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 540\n",
            "training_iteration: 3\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:33,045\tWARNING ppo.py:166 -- No data for p_0, not updating kl\n",
            "2020-08-04 08:02:33,050\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:02:33,412\tWARNING ppo.py:166 -- No data for p_5, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 4 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-33\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 72\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.20000000298023224\n",
            "      cur_lr: 1.1519999504089355\n",
            "      entropy: 0.0005226968205533922\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 69.896240234375\n",
            "      model: {}\n",
            "      policy_loss: 0.43437638878822327\n",
            "      total_loss: 1631.2454833984375\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 1616.83203125\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.30000001192092896\n",
            "      cur_lr: 1.1519999504089355\n",
            "      entropy: 0.006324709393084049\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 20.446273803710938\n",
            "      model: {}\n",
            "      policy_loss: 0.1632852703332901\n",
            "      total_loss: 473.19677734375\n",
            "      vf_explained_var: -0.2763568162918091\n",
            "      vf_loss: 466.8995666503906\n",
            "    p_4:\n",
            "      cur_kl_coeff: 0.07500000298023224\n",
            "      cur_lr: 0.6399999856948853\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: -9.934107758624577e-09\n",
            "      total_loss: 17.809499740600586\n",
            "      vf_explained_var: -0.16666670143604279\n",
            "      vf_loss: 17.809497833251953\n",
            "  num_steps_sampled: 720\n",
            "  num_steps_trained: 720\n",
            "iterations_since_restore: 4\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 73.2\n",
            "  ram_util_percent: 50.29999999999999\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_1: 8.0\n",
            "  p_2: 5.0\n",
            "  p_3: 8.0\n",
            "  p_4: 4.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_1: 2.0\n",
            "  p_2: -0.5\n",
            "  p_3: 2.3\n",
            "  p_4: -2.7\n",
            "  p_5: 2.8333333333333335\n",
            "policy_reward_min:\n",
            "  p_1: -3.0\n",
            "  p_2: -6.0\n",
            "  p_3: -5.0\n",
            "  p_4: -8.0\n",
            "  p_5: -3.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.2849963681968762\n",
            "  mean_inference_ms: 11.677006739972237\n",
            "  mean_processing_ms: 5.304386939232579\n",
            "time_since_restore: 15.228480339050293\n",
            "time_this_iter_s: 2.3532001972198486\n",
            "time_total_s: 15.228480339050293\n",
            "timers:\n",
            "  learn_throughput: 151.256\n",
            "  learn_time_ms: 1190.039\n",
            "  load_throughput: 463.991\n",
            "  load_time_ms: 387.939\n",
            "  sample_throughput: 327.579\n",
            "  sample_time_ms: 549.486\n",
            "  update_time_ms: 16.877\n",
            "timestamp: 1596528153\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 720\n",
            "training_iteration: 4\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:35,951\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:02:35,956\tWARNING ppo.py:166 -- No data for p_4, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 5 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-35\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 90\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.20000000298023224\n",
            "      cur_lr: 0.40959998965263367\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: 6.457170087514896e-08\n",
            "      total_loss: 133.15428161621094\n",
            "      vf_explained_var: -0.6874828338623047\n",
            "      vf_loss: 133.15428161621094\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.30000001192092896\n",
            "      cur_lr: 0.9215999841690063\n",
            "      entropy: 4.375207605458352e-37\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 11.272784233093262\n",
            "      model: {}\n",
            "      policy_loss: -0.022838816046714783\n",
            "      total_loss: 539.1621704101562\n",
            "      vf_explained_var: -1.2914340175029793e-07\n",
            "      vf_loss: 535.8031616210938\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.44999998807907104\n",
            "      cur_lr: 1.3824000358581543\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: -9.934107758624577e-09\n",
            "      total_loss: 38.98385238647461\n",
            "      vf_explained_var: -0.4161173105239868\n",
            "      vf_loss: 38.983848571777344\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.15000000596046448\n",
            "      cur_lr: 0.40959998965263367\n",
            "      entropy: 0.0018465886823832989\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 6.009525299072266\n",
            "      model: {}\n",
            "      policy_loss: -0.01179511845111847\n",
            "      total_loss: 8.830548286437988\n",
            "      vf_explained_var: -3.973643103449831e-08\n",
            "      vf_loss: 7.940915584564209\n",
            "  num_steps_sampled: 900\n",
            "  num_steps_trained: 900\n",
            "iterations_since_restore: 5\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 75.75\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 8.0\n",
            "  p_1: 8.0\n",
            "  p_2: 5.0\n",
            "  p_3: 8.0\n",
            "  p_4: 4.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_0: 8.0\n",
            "  p_1: 2.0\n",
            "  p_2: -3.0\n",
            "  p_3: 3.25\n",
            "  p_4: -2.7\n",
            "  p_5: -1.5\n",
            "policy_reward_min:\n",
            "  p_0: 8.0\n",
            "  p_1: -3.0\n",
            "  p_2: -8.0\n",
            "  p_3: -5.0\n",
            "  p_4: -8.0\n",
            "  p_5: -8.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.2645060842073936\n",
            "  mean_inference_ms: 11.724716562563373\n",
            "  mean_processing_ms: 5.305391639901996\n",
            "time_since_restore: 17.579284191131592\n",
            "time_this_iter_s: 2.350803852081299\n",
            "time_total_s: 17.579284191131592\n",
            "timers:\n",
            "  learn_throughput: 153.63\n",
            "  learn_time_ms: 1171.644\n",
            "  load_throughput: 475.32\n",
            "  load_time_ms: 378.693\n",
            "  sample_throughput: 330.75\n",
            "  sample_time_ms: 544.218\n",
            "  update_time_ms: 16.331\n",
            "timestamp: 1596528155\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 900\n",
            "training_iteration: 5\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:37,353\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:02:37,357\tWARNING ppo.py:166 -- No data for p_2, not updating kl\n",
            "2020-08-04 08:02:37,361\tWARNING ppo.py:166 -- No data for p_4, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 6 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-37\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 108\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.10000000149011612\n",
            "      cur_lr: 0.39321601390838623\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: -5.815591919144936e-08\n",
            "      total_loss: 6.923074722290039\n",
            "      vf_explained_var: 0.07019854336977005\n",
            "      vf_loss: 6.923074722290039\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.22499999403953552\n",
            "      cur_lr: 1.1059199571609497\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: 0.0\n",
            "      total_loss: 1.063951849937439\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 1.063951849937439\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.22499999403953552\n",
            "      cur_lr: 0.4915199875831604\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 6.644400121165598e-32\n",
            "      model: {}\n",
            "      policy_loss: -2.4835268064293814e-08\n",
            "      total_loss: 12.040350914001465\n",
            "      vf_explained_var: -9.934107758624577e-09\n",
            "      vf_loss: 12.040350914001465\n",
            "  num_steps_sampled: 1080\n",
            "  num_steps_trained: 1080\n",
            "iterations_since_restore: 6\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 79.94999999999999\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 8.0\n",
            "  p_1: 8.0\n",
            "  p_2: 5.0\n",
            "  p_3: 8.0\n",
            "  p_4: 4.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_0: 4.8\n",
            "  p_1: 3.0625\n",
            "  p_2: -3.0\n",
            "  p_3: 2.4375\n",
            "  p_4: -3.1346153846153846\n",
            "  p_5: -2.5833333333333335\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: -3.0\n",
            "  p_2: -8.0\n",
            "  p_3: -5.0\n",
            "  p_4: -8.0\n",
            "  p_5: -8.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.23123601606559327\n",
            "  mean_inference_ms: 11.842256859114654\n",
            "  mean_processing_ms: 5.293360033952247\n",
            "time_since_restore: 18.812148809432983\n",
            "time_this_iter_s: 1.2328646183013916\n",
            "time_total_s: 18.812148809432983\n",
            "timers:\n",
            "  learn_throughput: 168.216\n",
            "  learn_time_ms: 1070.053\n",
            "  load_throughput: 569.409\n",
            "  load_time_ms: 316.117\n",
            "  sample_throughput: 328.356\n",
            "  sample_time_ms: 548.185\n",
            "  update_time_ms: 15.976\n",
            "timestamp: 1596528157\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 1080\n",
            "training_iteration: 6\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:38,723\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:02:38,732\tWARNING ppo.py:166 -- No data for p_4, not updating kl\n",
            "2020-08-04 08:02:38,734\tWARNING ppo.py:166 -- No data for p_5, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 7 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-38\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 126\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.05000000074505806\n",
            "      cur_lr: 0.314572811126709\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: -0.0005960464477539062\n",
            "      total_loss: 0.05720512568950653\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.05780117213726044\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.44999998807907104\n",
            "      cur_lr: 1.1059199571609497\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: 0.0\n",
            "      total_loss: 1.9716681241989136\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 1.9716681241989136\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.11249999701976776\n",
            "      cur_lr: 0.5898240208625793\n",
            "      entropy: 0.002402173588052392\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.056987594813108444\n",
            "      model: {}\n",
            "      policy_loss: -0.001172875054180622\n",
            "      total_loss: 0.8308175206184387\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.8255793452262878\n",
            "  num_steps_sampled: 1260\n",
            "  num_steps_trained: 1260\n",
            "iterations_since_restore: 7\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 88.19999999999999\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 8.0\n",
            "  p_1: 8.0\n",
            "  p_2: 5.0\n",
            "  p_3: 8.0\n",
            "  p_4: 4.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_0: 3.0\n",
            "  p_1: 8.0\n",
            "  p_2: -1.8\n",
            "  p_3: 2.1666666666666665\n",
            "  p_4: -4.117647058823529\n",
            "  p_5: -2.935483870967742\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: 8.0\n",
            "  p_2: -8.0\n",
            "  p_3: -5.0\n",
            "  p_4: -8.0\n",
            "  p_5: -8.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.18939215227473066\n",
            "  mean_inference_ms: 11.970299958017481\n",
            "  mean_processing_ms: 5.306575021192186\n",
            "time_since_restore: 19.97420883178711\n",
            "time_this_iter_s: 1.162060022354126\n",
            "time_total_s: 19.97420883178711\n",
            "timers:\n",
            "  learn_throughput: 181.018\n",
            "  learn_time_ms: 994.377\n",
            "  load_throughput: 663.036\n",
            "  load_time_ms: 271.479\n",
            "  sample_throughput: 330.102\n",
            "  sample_time_ms: 545.286\n",
            "  update_time_ms: 15.832\n",
            "timestamp: 1596528158\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 1260\n",
            "training_iteration: 7\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:40,085\tWARNING ppo.py:166 -- No data for p_3, not updating kl\n",
            "2020-08-04 08:02:40,088\tWARNING ppo.py:166 -- No data for p_4, not updating kl\n",
            "2020-08-04 08:02:40,100\tWARNING ppo.py:166 -- No data for p_5, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 8 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-40\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 144\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.02500000037252903\n",
            "      cur_lr: 0.8847360014915466\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: 0.0\n",
            "      total_loss: 1.5444592237472534\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 1.5444592237472534\n",
            "    p_1:\n",
            "      cur_kl_coeff: 0.15000000596046448\n",
            "      cur_lr: 0.8847360014915466\n",
            "      entropy: 1.411905176107453e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -1.82490155799242e-12\n",
            "      model: {}\n",
            "      policy_loss: -4.386529326438904e-07\n",
            "      total_loss: 5.63336706161499\n",
            "      vf_explained_var: 1.4901161193847656e-08\n",
            "      vf_loss: 5.633367538452148\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.22499999403953552\n",
            "      cur_lr: 0.8847360014915466\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: 6.622738357719982e-09\n",
            "      total_loss: 142.34774780273438\n",
            "      vf_explained_var: -0.15758179128170013\n",
            "      vf_loss: 142.3477325439453\n",
            "  num_steps_sampled: 1440\n",
            "  num_steps_trained: 1440\n",
            "iterations_since_restore: 8\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 87.3\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 8.0\n",
            "  p_1: -10.0\n",
            "  p_2: 10.0\n",
            "  p_3: 8.0\n",
            "  p_4: 0.0\n",
            "  p_5: 8.0\n",
            "policy_reward_mean:\n",
            "  p_0: 2.6666666666666665\n",
            "  p_1: -10.0\n",
            "  p_2: 1.375\n",
            "  p_3: 2.36734693877551\n",
            "  p_4: -5.375\n",
            "  p_5: -5.714285714285714\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: -10.0\n",
            "  p_2: -8.0\n",
            "  p_3: -5.0\n",
            "  p_4: -8.0\n",
            "  p_5: -8.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.17045890716660497\n",
            "  mean_inference_ms: 12.01192765089485\n",
            "  mean_processing_ms: 5.334622809060631\n",
            "time_since_restore: 21.13732409477234\n",
            "time_this_iter_s: 1.1631152629852295\n",
            "time_total_s: 21.13732409477234\n",
            "timers:\n",
            "  learn_throughput: 192.307\n",
            "  learn_time_ms: 936.004\n",
            "  load_throughput: 756.427\n",
            "  load_time_ms: 237.961\n",
            "  sample_throughput: 329.668\n",
            "  sample_time_ms: 546.005\n",
            "  update_time_ms: 15.571\n",
            "timestamp: 1596528160\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 1440\n",
            "training_iteration: 8\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:41,419\tWARNING ppo.py:166 -- No data for p_0, not updating kl\n",
            "2020-08-04 08:02:41,428\tWARNING ppo.py:166 -- No data for p_3, not updating kl\n",
            "2020-08-04 08:02:41,435\tWARNING ppo.py:166 -- No data for p_4, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 9 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-41\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 162\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_1:\n",
            "      cur_kl_coeff: 0.07500000298023224\n",
            "      cur_lr: 0.5662310123443604\n",
            "      entropy: 8.777326782061223e-11\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 3.09431928864623e-12\n",
            "      model: {}\n",
            "      policy_loss: 4.03573130469681e-09\n",
            "      total_loss: 8.225646018981934\n",
            "      vf_explained_var: 0.0\n",
            "      vf_loss: 8.225646018981934\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.11249999701976776\n",
            "      cur_lr: 0.7077888250350952\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: -8.982088672837563e-08\n",
            "      total_loss: 8.122262001037598\n",
            "      vf_explained_var: -0.18953675031661987\n",
            "      vf_loss: 8.122262954711914\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.11249999701976776\n",
            "      cur_lr: 0.3774873614311218\n",
            "      entropy: 5.330087515176274e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 5.918378519709222e-06\n",
            "      model: {}\n",
            "      policy_loss: -9.08970832824707e-06\n",
            "      total_loss: 5.765730381011963\n",
            "      vf_explained_var: 0.0\n",
            "      vf_loss: 5.765738010406494\n",
            "  num_steps_sampled: 1620\n",
            "  num_steps_trained: 1620\n",
            "iterations_since_restore: 9\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 87.4\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 8.0\n",
            "  p_1: -10.0\n",
            "  p_2: 10.0\n",
            "  p_3: 8.0\n",
            "  p_4: -7.0\n",
            "  p_5: -8.0\n",
            "policy_reward_mean:\n",
            "  p_0: 2.6666666666666665\n",
            "  p_1: -10.0\n",
            "  p_2: 4.081967213114754\n",
            "  p_3: 2.176470588235294\n",
            "  p_4: -7.666666666666667\n",
            "  p_5: -8.8\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: -10.0\n",
            "  p_2: -8.0\n",
            "  p_3: -2.0\n",
            "  p_4: -8.0\n",
            "  p_5: -10.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.15846183372461425\n",
            "  mean_inference_ms: 12.050237698276186\n",
            "  mean_processing_ms: 5.321376250509793\n",
            "time_since_restore: 22.294728755950928\n",
            "time_this_iter_s: 1.1574046611785889\n",
            "time_total_s: 22.294728755950928\n",
            "timers:\n",
            "  learn_throughput: 202.135\n",
            "  learn_time_ms: 890.493\n",
            "  load_throughput: 849.359\n",
            "  load_time_ms: 211.924\n",
            "  sample_throughput: 329.747\n",
            "  sample_time_ms: 545.873\n",
            "  update_time_ms: 15.546\n",
            "timestamp: 1596528161\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 1620\n",
            "training_iteration: 9\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:42,578\tWARNING ppo.py:166 -- No data for p_0, not updating kl\n",
            "2020-08-04 08:02:42,579\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:02:42,589\tWARNING ppo.py:166 -- No data for p_4, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 10 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-42\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 180\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.05624999850988388\n",
            "      cur_lr: 0.06505358964204788\n",
            "      entropy: 2.0652682998995504e-23\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 860.3184204101562\n",
            "      model: {}\n",
            "      policy_loss: 0.3050762414932251\n",
            "      total_loss: 55.64207458496094\n",
            "      vf_explained_var: 0.1780993640422821\n",
            "      vf_loss: 6.944090366363525\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.16875000298023224\n",
            "      cur_lr: 0.6794772744178772\n",
            "      entropy: 2.543644222896546e-05\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 2.172515905840555e-06\n",
            "      model: {}\n",
            "      policy_loss: -3.2186508178710938e-06\n",
            "      total_loss: 6.757432460784912\n",
            "      vf_explained_var: -1.9868215517249155e-08\n",
            "      vf_loss: 6.757434844970703\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.05624999850988388\n",
            "      cur_lr: 0.07806430757045746\n",
            "      entropy: 2.444136559809351e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 3.595024625213661e-11\n",
            "      model: {}\n",
            "      policy_loss: 9.934107758624577e-09\n",
            "      total_loss: 5.393276214599609\n",
            "      vf_explained_var: -1.9868215517249155e-08\n",
            "      vf_loss: 5.393276214599609\n",
            "  num_steps_sampled: 1800\n",
            "  num_steps_trained: 1800\n",
            "iterations_since_restore: 10\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 85.5\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 8.0\n",
            "  p_1: -10.0\n",
            "  p_2: 10.0\n",
            "  p_3: 8.0\n",
            "  p_5: -8.0\n",
            "policy_reward_mean:\n",
            "  p_0: 2.122448979591837\n",
            "  p_1: -10.0\n",
            "  p_2: 6.608695652173913\n",
            "  p_3: -2.909090909090909\n",
            "  p_5: -9.161290322580646\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: -10.0\n",
            "  p_2: -8.0\n",
            "  p_3: -10.0\n",
            "  p_5: -10.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.14704585449679097\n",
            "  mean_inference_ms: 11.966757027276532\n",
            "  mean_processing_ms: 5.303809147891626\n",
            "time_since_restore: 23.26144003868103\n",
            "time_this_iter_s: 0.9667112827301025\n",
            "time_total_s: 23.26144003868103\n",
            "timers:\n",
            "  learn_throughput: 210.741\n",
            "  learn_time_ms: 854.129\n",
            "  load_throughput: 941.97\n",
            "  load_time_ms: 191.089\n",
            "  sample_throughput: 342.57\n",
            "  sample_time_ms: 525.439\n",
            "  update_time_ms: 16.711\n",
            "timestamp: 1596528162\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 1800\n",
            "training_iteration: 10\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:43,915\tWARNING ppo.py:166 -- No data for p_0, not updating kl\n",
            "2020-08-04 08:02:43,923\tWARNING ppo.py:166 -- No data for p_4, not updating kl\n",
            "2020-08-04 08:02:43,929\tWARNING ppo.py:166 -- No data for p_5, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 11 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-43\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 198\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_1:\n",
            "      cur_kl_coeff: 0.03750000149011612\n",
            "      cur_lr: 0.36238786578178406\n",
            "      entropy: 3.2574469788215765e-09\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 5.991518020629883\n",
            "      model: {}\n",
            "      policy_loss: 0.225000262260437\n",
            "      total_loss: 199.85472106933594\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 199.40504455566406\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.08437500149011612\n",
            "      cur_lr: 0.43486544489860535\n",
            "      entropy: 0.0002436776994727552\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 8.853710174560547\n",
            "      model: {}\n",
            "      policy_loss: -0.03443639352917671\n",
            "      total_loss: 6.108030319213867\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 5.395435333251953\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.08437500149011612\n",
            "      cur_lr: 0.5435817837715149\n",
            "      entropy: 4.3576045549720277e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 5.278479147818871e-05\n",
            "      model: {}\n",
            "      policy_loss: -1.176777823275188e-05\n",
            "      total_loss: 197.34161376953125\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 197.3416290283203\n",
            "  num_steps_sampled: 1980\n",
            "  num_steps_trained: 1980\n",
            "iterations_since_restore: 11\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 86.7\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 8.0\n",
            "  p_1: 0.0\n",
            "  p_2: 10.0\n",
            "  p_3: 1.0\n",
            "  p_5: -8.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.7058823529411765\n",
            "  p_1: -6.0\n",
            "  p_2: 6.583333333333333\n",
            "  p_3: -2.6511627906976742\n",
            "  p_5: -9.714285714285714\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: -10.0\n",
            "  p_2: -1.0\n",
            "  p_3: -10.0\n",
            "  p_5: -10.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.13812435121823263\n",
            "  mean_inference_ms: 11.879484936228845\n",
            "  mean_processing_ms: 5.290434176744699\n",
            "time_since_restore: 24.432780504226685\n",
            "time_this_iter_s: 1.1713404655456543\n",
            "time_total_s: 24.432780504226685\n",
            "timers:\n",
            "  learn_throughput: 239.449\n",
            "  learn_time_ms: 751.726\n",
            "  load_throughput: 1373.446\n",
            "  load_time_ms: 131.057\n",
            "  sample_throughput: 344.451\n",
            "  sample_time_ms: 522.571\n",
            "  update_time_ms: 17.136\n",
            "timestamp: 1596528163\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 1980\n",
            "training_iteration: 11\n",
            "\n",
            "checkpoint saved at /content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down/chkpt/checkpoint_11/checkpoint-11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:45,372\tWARNING ppo.py:166 -- No data for p_2, not updating kl\n",
            "2020-08-04 08:02:45,381\tWARNING ppo.py:166 -- No data for p_4, not updating kl\n",
            "2020-08-04 08:02:45,389\tWARNING ppo.py:166 -- No data for p_5, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 12 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-45\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 216\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.012500000186264515\n",
            "      cur_lr: 0.049961157143116\n",
            "      entropy: 0.009911064058542252\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 2.348834753036499\n",
            "      model: {}\n",
            "      policy_loss: -0.08405449241399765\n",
            "      total_loss: 0.0321471206843853\n",
            "      vf_explained_var: 0.025821467861533165\n",
            "      vf_loss: 0.0868411734700203\n",
            "    p_1:\n",
            "      cur_kl_coeff: 0.05624999850988388\n",
            "      cur_lr: 0.28991028666496277\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 4.567389177591752e-33\n",
            "      model: {}\n",
            "      policy_loss: -1.0641912240316742e-06\n",
            "      total_loss: 0.0901217982172966\n",
            "      vf_explained_var: -0.217365562915802\n",
            "      vf_loss: 0.09012286365032196\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.04218750074505806\n",
            "      cur_lr: 0.11241260915994644\n",
            "      entropy: 2.2309303016054827e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 6.4172551821062296e-12\n",
            "      model: {}\n",
            "      policy_loss: 5.920728085584415e-07\n",
            "      total_loss: 68.84272003173828\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 68.84272003173828\n",
            "  num_steps_sampled: 2160\n",
            "  num_steps_trained: 2160\n",
            "iterations_since_restore: 12\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 86.94999999999999\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_1: 1.0\n",
            "  p_2: 10.0\n",
            "  p_3: 1.0\n",
            "  p_5: -10.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.42857142857142855\n",
            "  p_1: -3.5\n",
            "  p_2: 7.074626865671642\n",
            "  p_3: -2.923076923076923\n",
            "  p_5: -10.0\n",
            "policy_reward_min:\n",
            "  p_0: -1.0\n",
            "  p_1: -10.0\n",
            "  p_2: -1.0\n",
            "  p_3: -10.0\n",
            "  p_5: -10.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.13106626719602235\n",
            "  mean_inference_ms: 11.790496379192865\n",
            "  mean_processing_ms: 5.282900870668701\n",
            "time_since_restore: 25.616474151611328\n",
            "time_this_iter_s: 1.1836936473846436\n",
            "time_total_s: 25.616474151611328\n",
            "timers:\n",
            "  learn_throughput: 257.595\n",
            "  learn_time_ms: 698.772\n",
            "  load_throughput: 1804.454\n",
            "  load_time_ms: 99.753\n",
            "  sample_throughput: 343.166\n",
            "  sample_time_ms: 524.528\n",
            "  update_time_ms: 17.228\n",
            "timestamp: 1596528165\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 2160\n",
            "training_iteration: 12\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:46,717\tWARNING ppo.py:166 -- No data for p_2, not updating kl\n",
            "2020-08-04 08:02:46,719\tWARNING ppo.py:166 -- No data for p_3, not updating kl\n",
            "2020-08-04 08:02:46,725\tWARNING ppo.py:166 -- No data for p_4, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 13 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-46\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 234\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.01875000074505806\n",
            "      cur_lr: 0.059953391551971436\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 12.332501411437988\n",
            "      model: {}\n",
            "      policy_loss: 0.29999998211860657\n",
            "      total_loss: 0.5450775027275085\n",
            "      vf_explained_var: 0.6573771834373474\n",
            "      vf_loss: 0.013843039982020855\n",
            "    p_1:\n",
            "      cur_kl_coeff: 0.02812499925494194\n",
            "      cur_lr: 0.23192822933197021\n",
            "      entropy: 1.1994401305504e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: -1.655684656043377e-08\n",
            "      total_loss: 0.5685675740242004\n",
            "      vf_explained_var: -0.7040280699729919\n",
            "      vf_loss: 0.5685675740242004\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.02812499925494194\n",
            "      cur_lr: 0.10791610181331635\n",
            "      entropy: 1.671644905831826e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 2.879062656682696e-12\n",
            "      model: {}\n",
            "      policy_loss: 1.7881393432617188e-07\n",
            "      total_loss: 200.124755859375\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 200.12477111816406\n",
            "  num_steps_sampled: 2340\n",
            "  num_steps_trained: 2340\n",
            "iterations_since_restore: 13\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 87.0\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 1.0\n",
            "  p_1: 1.0\n",
            "  p_2: 10.0\n",
            "  p_3: 1.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.2857142857142857\n",
            "  p_1: -2.0327868852459017\n",
            "  p_2: 8.153846153846153\n",
            "  p_3: -3.1666666666666665\n",
            "  p_5: -6.0\n",
            "policy_reward_min:\n",
            "  p_0: -1.0\n",
            "  p_1: -10.0\n",
            "  p_2: -1.0\n",
            "  p_3: -10.0\n",
            "  p_5: -10.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.12565036395964768\n",
            "  mean_inference_ms: 11.715694208715826\n",
            "  mean_processing_ms: 5.27413878797512\n",
            "time_since_restore: 26.76875638961792\n",
            "time_this_iter_s: 1.1522822380065918\n",
            "time_total_s: 26.76875638961792\n",
            "timers:\n",
            "  learn_throughput: 280.215\n",
            "  learn_time_ms: 642.363\n",
            "  load_throughput: 2602.1\n",
            "  load_time_ms: 69.175\n",
            "  sample_throughput: 343.003\n",
            "  sample_time_ms: 524.777\n",
            "  update_time_ms: 17.014\n",
            "timestamp: 1596528166\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 2340\n",
            "training_iteration: 13\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:48,051\tWARNING ppo.py:166 -- No data for p_0, not updating kl\n",
            "2020-08-04 08:02:48,058\tWARNING ppo.py:166 -- No data for p_3, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 14 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-48\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 252\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_1:\n",
            "      cur_kl_coeff: 0.01406249962747097\n",
            "      cur_lr: 0.27831387519836426\n",
            "      entropy: 1.6568181548937133e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -1.7267082945762735e-12\n",
            "      model: {}\n",
            "      policy_loss: -2.8808912588829116e-07\n",
            "      total_loss: 6.819149017333984\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 6.819149017333984\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.12656250596046448\n",
            "      cur_lr: 0.3339766561985016\n",
            "      entropy: 0.0\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.1368214147822187e-09\n",
            "      model: {}\n",
            "      policy_loss: -3.685553622290172e-07\n",
            "      total_loss: 15.291438102722168\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 15.291439056396484\n",
            "    p_4:\n",
            "      cur_kl_coeff: 0.03750000149011612\n",
            "      cur_lr: 0.2671813368797302\n",
            "      entropy: 2.782476193896599e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -2.2578141613271985e-12\n",
            "      model: {}\n",
            "      policy_loss: 1.842776811145086e-07\n",
            "      total_loss: 200.17823791503906\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 200.17823791503906\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.01406249962747097\n",
            "      cur_lr: 0.12949931621551514\n",
            "      entropy: 1.648642888874008e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 5.27657446544854e-14\n",
            "      model: {}\n",
            "      policy_loss: -1.1920929132713809e-08\n",
            "      total_loss: 4.634167671203613\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 4.634167671203613\n",
            "  num_steps_sampled: 2520\n",
            "  num_steps_trained: 2520\n",
            "iterations_since_restore: 14\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 86.69999999999999\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 1.0\n",
            "  p_1: 1.0\n",
            "  p_2: 10.0\n",
            "  p_3: 1.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.3333333333333333\n",
            "  p_1: -0.42105263157894735\n",
            "  p_2: 5.956521739130435\n",
            "  p_3: -3.1666666666666665\n",
            "  p_4: 0.0\n",
            "  p_5: -4.193548387096774\n",
            "policy_reward_min:\n",
            "  p_0: -1.0\n",
            "  p_1: -10.0\n",
            "  p_2: -1.0\n",
            "  p_3: -10.0\n",
            "  p_4: 0.0\n",
            "  p_5: -10.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.1213159344475865\n",
            "  mean_inference_ms: 11.64326486829861\n",
            "  mean_processing_ms: 5.26130977214121\n",
            "time_since_restore: 27.908345222473145\n",
            "time_this_iter_s: 1.1395888328552246\n",
            "time_total_s: 27.908345222473145\n",
            "timers:\n",
            "  learn_throughput: 305.49\n",
            "  learn_time_ms: 589.217\n",
            "  load_throughput: 4817.219\n",
            "  load_time_ms: 37.366\n",
            "  sample_throughput: 344.921\n",
            "  sample_time_ms: 521.859\n",
            "  update_time_ms: 16.852\n",
            "timestamp: 1596528168\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 2520\n",
            "training_iteration: 14\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:49,227\tWARNING ppo.py:166 -- No data for p_0, not updating kl\n",
            "2020-08-04 08:02:49,230\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:02:49,236\tWARNING ppo.py:166 -- No data for p_3, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 15 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-49\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 270\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.06328125298023224\n",
            "      cur_lr: 0.21374505758285522\n",
            "      entropy: 3.881436838604202e-11\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: 5.01672445807344e-07\n",
            "      total_loss: 0.10581216961145401\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.1058116927742958\n",
            "    p_4:\n",
            "      cur_kl_coeff: 0.01875000074505806\n",
            "      cur_lr: 0.32061758637428284\n",
            "      entropy: 2.8217794767471105e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -8.315048682146914e-14\n",
            "      model: {}\n",
            "      policy_loss: -6.58962520105888e-08\n",
            "      total_loss: 11.780556678771973\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 11.780556678771973\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.007031249813735485\n",
            "      cur_lr: 0.32061758637428284\n",
            "      entropy: 1.651406372760178e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -1.8065550886291713e-14\n",
            "      model: {}\n",
            "      policy_loss: 2.9802322387695312e-08\n",
            "      total_loss: 0.013384084217250347\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.013384056277573109\n",
            "  num_steps_sampled: 2700\n",
            "  num_steps_trained: 2700\n",
            "iterations_since_restore: 15\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 85.25\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 1.0\n",
            "  p_1: 1.0\n",
            "  p_2: 10.0\n",
            "  p_3: 1.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.3333333333333333\n",
            "  p_1: 0.1111111111111111\n",
            "  p_2: 2.764705882352941\n",
            "  p_3: -2.064516129032258\n",
            "  p_4: 0.0\n",
            "  p_5: -0.9090909090909091\n",
            "policy_reward_min:\n",
            "  p_0: -1.0\n",
            "  p_1: -1.0\n",
            "  p_2: -1.0\n",
            "  p_3: -10.0\n",
            "  p_4: 0.0\n",
            "  p_5: -10.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.11796506159323507\n",
            "  mean_inference_ms: 11.54578695104057\n",
            "  mean_processing_ms: 5.2544824961912004\n",
            "time_since_restore: 28.889324426651\n",
            "time_this_iter_s: 0.9809792041778564\n",
            "time_total_s: 28.889324426651\n",
            "timers:\n",
            "  learn_throughput: 337.117\n",
            "  learn_time_ms: 533.939\n",
            "  load_throughput: 50965.317\n",
            "  load_time_ms: 3.532\n",
            "  sample_throughput: 356.583\n",
            "  sample_time_ms: 504.792\n",
            "  update_time_ms: 16.927\n",
            "timestamp: 1596528169\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 2700\n",
            "training_iteration: 15\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:50,408\tWARNING ppo.py:166 -- No data for p_0, not updating kl\n",
            "2020-08-04 08:02:50,419\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:02:50,423\tWARNING ppo.py:166 -- No data for p_3, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 16 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-50\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 288\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.03164062649011612\n",
            "      cur_lr: 0.062184467911720276\n",
            "      entropy: 1.5326460933717811e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -2.1625771919747683e-12\n",
            "      model: {}\n",
            "      policy_loss: -4.673997409554431e-06\n",
            "      total_loss: 0.0018283277750015259\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.0018330155871808529\n",
            "    p_4:\n",
            "      cur_kl_coeff: 0.00937500037252903\n",
            "      cur_lr: 0.25649407505989075\n",
            "      entropy: 3.5228063521852704e-11\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 2.6861176477543935e-11\n",
            "      model: {}\n",
            "      policy_loss: -1.1920928955078125e-07\n",
            "      total_loss: 0.017573349177837372\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.017573462799191475\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.0035156249068677425\n",
            "      cur_lr: 0.04145631194114685\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -2.3283914198286328e-14\n",
            "      model: {}\n",
            "      policy_loss: -9.437402326284428e-08\n",
            "      total_loss: 0.0051985979080200195\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.005198705475777388\n",
            "  num_steps_sampled: 2880\n",
            "  num_steps_trained: 2880\n",
            "iterations_since_restore: 16\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 86.05000000000001\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 1.0\n",
            "  p_1: 1.0\n",
            "  p_2: 0.0\n",
            "  p_3: 1.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.3333333333333333\n",
            "  p_1: 0.12244897959183673\n",
            "  p_2: -0.09090909090909091\n",
            "  p_3: 0.1875\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: -1.0\n",
            "  p_1: -1.0\n",
            "  p_2: -1.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.11566513177068298\n",
            "  mean_inference_ms: 11.450075211566523\n",
            "  mean_processing_ms: 5.263958061116734\n",
            "time_since_restore: 29.894164562225342\n",
            "time_this_iter_s: 1.0048401355743408\n",
            "time_total_s: 29.894164562225342\n",
            "timers:\n",
            "  learn_throughput: 339.554\n",
            "  learn_time_ms: 530.108\n",
            "  load_throughput: 50410.625\n",
            "  load_time_ms: 3.571\n",
            "  sample_throughput: 369.182\n",
            "  sample_time_ms: 487.564\n",
            "  update_time_ms: 17.969\n",
            "timestamp: 1596528170\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 2880\n",
            "training_iteration: 16\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:51,585\tWARNING ppo.py:166 -- No data for p_0, not updating kl\n",
            "2020-08-04 08:02:51,589\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:02:51,600\tWARNING ppo.py:166 -- No data for p_5, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 17 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-51\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 306\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.01582031324505806\n",
            "      cur_lr: 0.07163650542497635\n",
            "      entropy: 1.6536268188094283e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -5.383352184168411e-13\n",
            "      model: {}\n",
            "      policy_loss: 2.9802322831784522e-09\n",
            "      total_loss: 0.0005282610654830933\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.0005282434867694974\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.02109375037252903\n",
            "      cur_lr: 0.05969708785414696\n",
            "      entropy: 1.6567212879348148e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 6.885158675834502e-12\n",
            "      model: {}\n",
            "      policy_loss: 1.7881393432617188e-07\n",
            "      total_loss: 0.44678840041160583\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.4467882215976715\n",
            "    p_4:\n",
            "      cur_kl_coeff: 0.004687500186264515\n",
            "      cur_lr: 0.30779287219047546\n",
            "      entropy: 1.6564308258359972e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -2.276998034567157e-12\n",
            "      model: {}\n",
            "      policy_loss: -4.8014854314715194e-08\n",
            "      total_loss: 0.00010110603761859238\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.00010116487828781828\n",
            "  num_steps_sampled: 3060\n",
            "  num_steps_trained: 3060\n",
            "iterations_since_restore: 17\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 83.8\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 1.0\n",
            "  p_1: 1.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: -0.07692307692307693\n",
            "  p_1: 0.029411764705882353\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: -1.0\n",
            "  p_1: -1.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.11405587150221333\n",
            "  mean_inference_ms: 11.289502887284131\n",
            "  mean_processing_ms: 5.28391331565341\n",
            "time_since_restore: 30.897377014160156\n",
            "time_this_iter_s: 1.0032124519348145\n",
            "time_total_s: 30.897377014160156\n",
            "timers:\n",
            "  learn_throughput: 339.452\n",
            "  learn_time_ms: 530.266\n",
            "  load_throughput: 50622.22\n",
            "  load_time_ms: 3.556\n",
            "  sample_throughput: 380.89\n",
            "  sample_time_ms: 472.577\n",
            "  update_time_ms: 17.9\n",
            "timestamp: 1596528171\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 3060\n",
            "training_iteration: 17\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:52,941\tWARNING ppo.py:166 -- No data for p_0, not updating kl\n",
            "2020-08-04 08:02:52,949\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:02:52,957\tWARNING ppo.py:166 -- No data for p_2, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 18 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-52\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 324\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.010546875186264515\n",
            "      cur_lr: 0.07163650542497635\n",
            "      entropy: 1.6564308258359972e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 3.029543990687119e-15\n",
            "      model: {}\n",
            "      policy_loss: -1.9172827592228714e-07\n",
            "      total_loss: 0.0015051072696223855\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.0015052873641252518\n",
            "    p_4:\n",
            "      cur_kl_coeff: 0.0023437500931322575\n",
            "      cur_lr: 0.36935147643089294\n",
            "      entropy: 1.6567212879348148e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: 8.34465012644614e-08\n",
            "      total_loss: 0.023360736668109894\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.023360630497336388\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.0017578124534338713\n",
            "      cur_lr: 0.4432217478752136\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -3.6259493185752945e-07\n",
            "      total_loss: 0.0005002121324650943\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.0005005590501241386\n",
            "  num_steps_sampled: 3240\n",
            "  num_steps_trained: 3240\n",
            "iterations_since_restore: 18\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 87.85\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 1.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: 1.0\n",
            "  p_1: -0.1875\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: 1.0\n",
            "  p_1: -1.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.11271827048769442\n",
            "  mean_inference_ms: 11.12664138253713\n",
            "  mean_processing_ms: 5.311073397689879\n",
            "time_since_restore: 32.082802057266235\n",
            "time_this_iter_s: 1.185425043106079\n",
            "time_total_s: 32.082802057266235\n",
            "timers:\n",
            "  learn_throughput: 341.304\n",
            "  learn_time_ms: 527.389\n",
            "  load_throughput: 50560.857\n",
            "  load_time_ms: 3.56\n",
            "  sample_throughput: 378.908\n",
            "  sample_time_ms: 475.05\n",
            "  update_time_ms: 19.34\n",
            "timestamp: 1596528172\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 3240\n",
            "training_iteration: 18\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:54,102\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:02:54,109\tWARNING ppo.py:166 -- No data for p_4, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 19 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-54\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 342\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.02812499925494194\n",
            "      cur_lr: 0.047757670283317566\n",
            "      entropy: 4.5355094056538994e-14\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 6.04358785238901e-11\n",
            "      model: {}\n",
            "      policy_loss: -3.7302572764019715e-07\n",
            "      total_loss: 0.004277739208191633\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.004278095904737711\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.00791015662252903\n",
            "      cur_lr: 0.031838446855545044\n",
            "      entropy: 1.6567212879348148e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0\n",
            "      model: {}\n",
            "      policy_loss: -3.630916296515352e-07\n",
            "      total_loss: 0.0010381708852946758\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.0010385343339294195\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.0052734375931322575\n",
            "      cur_lr: 0.05730920657515526\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 0.0\n",
            "      total_loss: 0.00031382640008814633\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.00031382424640469253\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.0008789062267169356\n",
            "      cur_lr: 0.3545773923397064\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 1.5497207641601562e-06\n",
            "      total_loss: 0.0005598455318249762\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.0005583181628026068\n",
            "  num_steps_sampled: 3420\n",
            "  num_steps_trained: 3420\n",
            "iterations_since_restore: 19\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 85.6\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.11128187640624838\n",
            "  mean_inference_ms: 10.914079157620224\n",
            "  mean_processing_ms: 5.335015579222739\n",
            "time_since_restore: 33.03980565071106\n",
            "time_this_iter_s: 0.9570035934448242\n",
            "time_total_s: 33.03980565071106\n",
            "timers:\n",
            "  learn_throughput: 341.275\n",
            "  learn_time_ms: 527.433\n",
            "  load_throughput: 50105.505\n",
            "  load_time_ms: 3.592\n",
            "  sample_throughput: 395.202\n",
            "  sample_time_ms: 455.463\n",
            "  update_time_ms: 19.326\n",
            "timestamp: 1596528174\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 3420\n",
            "training_iteration: 19\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:55,270\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:02:55,273\tWARNING ppo.py:166 -- No data for p_3, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 20 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-55\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 360\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.01406249962747097\n",
            "      cur_lr: 0.03820613771677017\n",
            "      entropy: 1.454894676955476e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -8.802014869193579e-15\n",
            "      model: {}\n",
            "      policy_loss: 0.0\n",
            "      total_loss: 0.0005930662155151367\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.0005930500337854028\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.003955078311264515\n",
            "      cur_lr: 0.025470757856965065\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 3.0295437789288823e-15\n",
            "      model: {}\n",
            "      policy_loss: -1.41064333547547e-07\n",
            "      total_loss: 0.000318418926326558\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.00031858019065111876\n",
            "    p_4:\n",
            "      cur_kl_coeff: 0.0011718750465661287\n",
            "      cur_lr: 0.055016838014125824\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -3.6259493185752945e-07\n",
            "      total_loss: 3.485381603240967e-05\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 3.522246333886869e-05\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.0004394531133584678\n",
            "      cur_lr: 0.28366193175315857\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 0.0\n",
            "      total_loss: 7.304251630557701e-05\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 7.306048792088404e-05\n",
            "  num_steps_sampled: 3600\n",
            "  num_steps_trained: 3600\n",
            "iterations_since_restore: 20\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 86.25\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.11015684851194309\n",
            "  mean_inference_ms: 10.693085894105518\n",
            "  mean_processing_ms: 5.360461058734577\n",
            "time_since_restore: 34.01924753189087\n",
            "time_this_iter_s: 0.9794418811798096\n",
            "time_total_s: 34.01924753189087\n",
            "timers:\n",
            "  learn_throughput: 340.601\n",
            "  learn_time_ms: 528.478\n",
            "  load_throughput: 49250.758\n",
            "  load_time_ms: 3.655\n",
            "  sample_throughput: 393.631\n",
            "  sample_time_ms: 457.281\n",
            "  update_time_ms: 18.068\n",
            "timestamp: 1596528175\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 3600\n",
            "training_iteration: 20\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:56,416\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:02:56,418\tWARNING ppo.py:166 -- No data for p_2, not updating kl\n",
            "2020-08-04 08:02:56,424\tWARNING ppo.py:166 -- No data for p_3, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 21 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-56\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 378\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.007031249813735485\n",
            "      cur_lr: 0.030564909800887108\n",
            "      entropy: 1.6229823041058467e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -7.864989041983517e-13\n",
            "      model: {}\n",
            "      policy_loss: -7.947286206899662e-08\n",
            "      total_loss: 2.143035271728877e-05\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 2.149743704649154e-05\n",
            "    p_4:\n",
            "      cur_kl_coeff: 0.0005859375232830644\n",
            "      cur_lr: 0.06602020561695099\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -7.251898637150589e-07\n",
            "      total_loss: 0.00025681653642095625\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.00025752457440830767\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.0002197265566792339\n",
            "      cur_lr: 0.22692954540252686\n",
            "      entropy: 1.6564308258359972e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 1.3245476715439963e-08\n",
            "      total_loss: 3.174278390360996e-05\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 3.172614742652513e-05\n",
            "  num_steps_sampled: 3780\n",
            "  num_steps_trained: 3780\n",
            "iterations_since_restore: 21\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 83.9\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10895509148656095\n",
            "  mean_inference_ms: 10.497877306205812\n",
            "  mean_processing_ms: 5.379212129714315\n",
            "time_since_restore: 35.0021026134491\n",
            "time_this_iter_s: 0.9828550815582275\n",
            "time_total_s: 35.0021026134491\n",
            "timers:\n",
            "  learn_throughput: 340.908\n",
            "  learn_time_ms: 528.001\n",
            "  load_throughput: 48278.832\n",
            "  load_time_ms: 3.728\n",
            "  sample_throughput: 408.585\n",
            "  sample_time_ms: 440.545\n",
            "  update_time_ms: 17.198\n",
            "timestamp: 1596528176\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 3780\n",
            "training_iteration: 21\n",
            "\n",
            "checkpoint saved at /content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down/chkpt/checkpoint_21/checkpoint-21\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:57,644\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:02:57,645\tWARNING ppo.py:166 -- No data for p_2, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 22 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-57\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 396\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.0035156249068677425\n",
            "      cur_lr: 0.024451928213238716\n",
            "      entropy: 1.6556568616099554e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: -1.4731140298481632e-13\n",
            "      model: {}\n",
            "      policy_loss: 0.0\n",
            "      total_loss: 0.00043731729965656996\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.00043733150232583284\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.0026367187965661287\n",
            "      cur_lr: 0.03521077707409859\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 0.0\n",
            "      total_loss: 6.9951020122971386e-06\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 7.026638286333764e-06\n",
            "    p_4:\n",
            "      cur_kl_coeff: 0.0002929687616415322\n",
            "      cur_lr: 0.02816862054169178\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 9.189049166025143e-08\n",
            "      total_loss: 2.9007594548602356e-06\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 2.82230507764325e-06\n",
            "    p_5:\n",
            "      cur_kl_coeff: 0.00010986327833961695\n",
            "      cur_lr: 0.02816862054169178\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 5.9604645663569045e-09\n",
            "      total_loss: 4.340211489761714e-06\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 4.324117526266491e-06\n",
            "  num_steps_sampled: 3960\n",
            "  num_steps_trained: 3960\n",
            "iterations_since_restore: 22\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 84.85\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10793689937808101\n",
            "  mean_inference_ms: 10.32552925199287\n",
            "  mean_processing_ms: 5.386471769354858\n",
            "time_since_restore: 35.95792365074158\n",
            "time_this_iter_s: 0.9558210372924805\n",
            "time_total_s: 35.95792365074158\n",
            "timers:\n",
            "  learn_throughput: 342.393\n",
            "  learn_time_ms: 525.712\n",
            "  load_throughput: 45138.602\n",
            "  load_time_ms: 3.988\n",
            "  sample_throughput: 428.509\n",
            "  sample_time_ms: 420.061\n",
            "  update_time_ms: 18.333\n",
            "timestamp: 1596528177\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 3960\n",
            "training_iteration: 22\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:02:58,762\tWARNING ppo.py:166 -- No data for p_0, not updating kl\n",
            "2020-08-04 08:02:58,769\tWARNING ppo.py:166 -- No data for p_2, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 23 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-02-58\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 414\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_1:\n",
            "      cur_kl_coeff: 0.007031249813735485\n",
            "      cur_lr: 0.03521077707409859\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -3.6259493185752945e-07\n",
            "      total_loss: 0.03110264055430889\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 0.031102992594242096\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.0013183593982830644\n",
            "      cur_lr: 0.04815983027219772\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 9.934107980669182e-10\n",
            "      total_loss: 6.824731713095389e-07\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 6.576467512786621e-07\n",
            "    p_4:\n",
            "      cur_kl_coeff: 0.0001464843808207661\n",
            "      cur_lr: 0.033802345395088196\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 2.9802322831784522e-09\n",
            "      total_loss: 4.043181718316191e-07\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 3.933707830583444e-07\n",
            "    p_5:\n",
            "      cur_kl_coeff: 5.493163916980848e-05\n",
            "      cur_lr: 0.028339358046650887\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -2.3345153010723152e-07\n",
            "      total_loss: -1.7136335372924805e-07\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 5.920261969549756e-08\n",
            "  num_steps_sampled: 4140\n",
            "  num_steps_trained: 4140\n",
            "iterations_since_restore: 23\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 85.9\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10692413947150137\n",
            "  mean_inference_ms: 10.146116485134643\n",
            "  mean_processing_ms: 5.386032812775524\n",
            "time_since_restore: 36.92288303375244\n",
            "time_this_iter_s: 0.9649593830108643\n",
            "time_total_s: 36.92288303375244\n",
            "timers:\n",
            "  learn_throughput: 341.396\n",
            "  learn_time_ms: 527.247\n",
            "  load_throughput: 43697.241\n",
            "  load_time_ms: 4.119\n",
            "  sample_throughput: 448.868\n",
            "  sample_time_ms: 401.009\n",
            "  update_time_ms: 18.387\n",
            "timestamp: 1596528178\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 4140\n",
            "training_iteration: 23\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:03:00,149\tWARNING ppo.py:166 -- No data for p_2, not updating kl\n",
            "2020-08-04 08:03:00,151\tWARNING ppo.py:166 -- No data for p_3, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 24 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-03-00\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 432\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.0017578124534338713\n",
            "      cur_lr: 0.02347385138273239\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -2.2848446690204582e-07\n",
            "      total_loss: 9.98377799987793e-06\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 1.020718536892673e-05\n",
            "    p_1:\n",
            "      cur_kl_coeff: 0.0035156249068677425\n",
            "      cur_lr: 0.02816862054169178\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 3.2782554626464844e-07\n",
            "      total_loss: 8.643666660645977e-05\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 8.611558587290347e-05\n",
            "    p_4:\n",
            "      cur_kl_coeff: 7.324219041038305e-05\n",
            "      cur_lr: 0.027041876688599586\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -9.934107758624577e-09\n",
            "      total_loss: 3.973643103449831e-08\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 5.330540275849671e-08\n",
            "    p_5:\n",
            "      cur_kl_coeff: 2.746581958490424e-05\n",
            "      cur_lr: 0.02267148718237877\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -2.4835268064293814e-08\n",
            "      total_loss: 2.856055800748436e-07\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 3.1692351853962464e-07\n",
            "  num_steps_sampled: 4320\n",
            "  num_steps_trained: 4320\n",
            "iterations_since_restore: 24\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 88.4\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10602243481692764\n",
            "  mean_inference_ms: 10.00033852882902\n",
            "  mean_processing_ms: 5.383598799986257\n",
            "time_since_restore: 38.10574674606323\n",
            "time_this_iter_s: 1.182863712310791\n",
            "time_total_s: 38.10574674606323\n",
            "timers:\n",
            "  learn_throughput: 342.111\n",
            "  learn_time_ms: 526.144\n",
            "  load_throughput: 42742.321\n",
            "  load_time_ms: 4.211\n",
            "  sample_throughput: 443.494\n",
            "  sample_time_ms: 405.868\n",
            "  update_time_ms: 19.068\n",
            "timestamp: 1596528180\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 4320\n",
            "training_iteration: 24\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:03:01,280\tWARNING ppo.py:166 -- No data for p_2, not updating kl\n",
            "2020-08-04 08:03:01,282\tWARNING ppo.py:166 -- No data for p_3, not updating kl\n",
            "2020-08-04 08:03:01,294\tWARNING ppo.py:166 -- No data for p_5, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 25 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-03-01\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 450\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.0008789062267169356\n",
            "      cur_lr: 0.02163350023329258\n",
            "      entropy: 1.6564308258359972e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 8.653198062503694e-16\n",
            "      model: {}\n",
            "      policy_loss: 0.0\n",
            "      total_loss: 5.7617821624944554e-08\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 4.699584366107956e-08\n",
            "    p_1:\n",
            "      cur_kl_coeff: 0.0017578124534338713\n",
            "      cur_lr: 0.017306800931692123\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -4.967053879312289e-09\n",
            "      total_loss: 2.6101868115802063e-06\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 2.626762352520018e-06\n",
            "    p_4:\n",
            "      cur_kl_coeff: 3.662109520519152e-05\n",
            "      cur_lr: 0.013845440931618214\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 3.476937493473997e-08\n",
            "      total_loss: 1.0679165285409908e-07\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 7.233481369439687e-08\n",
            "  num_steps_sampled: 4500\n",
            "  num_steps_trained: 4500\n",
            "iterations_since_restore: 25\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 85.15\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10510667170272217\n",
            "  mean_inference_ms: 9.869280647982348\n",
            "  mean_processing_ms: 5.380865261346864\n",
            "time_since_restore: 39.06525802612305\n",
            "time_this_iter_s: 0.9595112800598145\n",
            "time_total_s: 39.06525802612305\n",
            "timers:\n",
            "  learn_throughput: 343.409\n",
            "  learn_time_ms: 524.157\n",
            "  load_throughput: 42338.197\n",
            "  load_time_ms: 4.251\n",
            "  sample_throughput: 444.985\n",
            "  sample_time_ms: 404.508\n",
            "  update_time_ms: 19.614\n",
            "timestamp: 1596528181\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 4500\n",
            "training_iteration: 25\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:03:02,424\tWARNING ppo.py:166 -- No data for p_2, not updating kl\n",
            "2020-08-04 08:03:02,432\tWARNING ppo.py:166 -- No data for p_5, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 26 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-03-02\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 468\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.0004394531133584678\n",
            "      cur_lr: 0.083354651927948\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 4.7683716530855236e-08\n",
            "      total_loss: 4.7683716530855236e-08\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 1.881837352613047e-09\n",
            "    p_1:\n",
            "      cur_kl_coeff: 0.0008789062267169356\n",
            "      cur_lr: 0.06668371707201004\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -2.1358330570819817e-07\n",
            "      total_loss: 4.2219957663292007e-07\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 6.280656634771731e-07\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.0006591796991415322\n",
            "      cur_lr: 0.0800204649567604\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -1.0430812658057675e-08\n",
            "      total_loss: -1.0430812658057675e-08\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 5.445826567473944e-10\n",
            "    p_4:\n",
            "      cur_kl_coeff: 1.831054760259576e-05\n",
            "      cur_lr: 0.016614528372883797\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 1.4901161193847656e-08\n",
            "      total_loss: 1.4901161193847656e-08\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 2.1896719992753333e-09\n",
            "  num_steps_sampled: 4680\n",
            "  num_steps_trained: 4680\n",
            "iterations_since_restore: 26\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 85.45\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.1043233498994099\n",
            "  mean_inference_ms: 9.751223579452521\n",
            "  mean_processing_ms: 5.37782610049918\n",
            "time_since_restore: 40.0295774936676\n",
            "time_this_iter_s: 0.9643194675445557\n",
            "time_total_s: 40.0295774936676\n",
            "timers:\n",
            "  learn_throughput: 343.09\n",
            "  learn_time_ms: 524.644\n",
            "  load_throughput: 41762.071\n",
            "  load_time_ms: 4.31\n",
            "  sample_throughput: 449.961\n",
            "  sample_time_ms: 400.035\n",
            "  update_time_ms: 18.654\n",
            "timestamp: 1596528182\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 4680\n",
            "training_iteration: 26\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:03:03,574\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:03:03,581\tWARNING ppo.py:166 -- No data for p_5, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 27 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-03-03\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 486\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.0002197265566792339\n",
            "      cur_lr: 0.019937435165047646\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 1.123795900781488e-08\n",
            "      total_loss: 1.123795900781488e-08\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 5.455594309644596e-11\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.0019775391556322575\n",
            "      cur_lr: 0.015949947759509087\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 3.576278473360617e-08\n",
            "      total_loss: 4.019588232040405e-06\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 3.978927907155594e-06\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.0003295898495707661\n",
            "      cur_lr: 0.08655320107936859\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -1.5522042540183634e-09\n",
            "      total_loss: -3.1044086745701804e-10\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 1.229623625143006e-09\n",
            "    p_4:\n",
            "      cur_kl_coeff: 9.15527380129788e-06\n",
            "      cur_lr: 0.013291622512042522\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -1.2417634698280722e-09\n",
            "      total_loss: -1.2417634698280722e-09\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 1.474008470836452e-09\n",
            "  num_steps_sampled: 4860\n",
            "  num_steps_trained: 4860\n",
            "iterations_since_restore: 27\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 85.5\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.1037080179970594\n",
            "  mean_inference_ms: 9.641294408732781\n",
            "  mean_processing_ms: 5.375651166110342\n",
            "time_since_restore: 41.010584592819214\n",
            "time_this_iter_s: 0.9810070991516113\n",
            "time_total_s: 41.010584592819214\n",
            "timers:\n",
            "  learn_throughput: 341.328\n",
            "  learn_time_ms: 527.352\n",
            "  load_throughput: 41372.785\n",
            "  load_time_ms: 4.351\n",
            "  sample_throughput: 455.234\n",
            "  sample_time_ms: 395.401\n",
            "  update_time_ms: 18.72\n",
            "timestamp: 1596528183\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 4860\n",
            "training_iteration: 27\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:03:04,786\tWARNING ppo.py:166 -- No data for p_1, not updating kl\n",
            "2020-08-04 08:03:04,793\tWARNING ppo.py:166 -- No data for p_4, not updating kl\n",
            "2020-08-04 08:03:04,795\tWARNING ppo.py:166 -- No data for p_5, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 28 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-03-04\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 504\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 0.00010986327833961695\n",
            "      cur_lr: 0.015949947759509087\n",
            "      entropy: 1.6564308258359972e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 3.0914734661102727e-10\n",
            "      total_loss: 3.4795244463481367e-10\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 6.803967805835143e-11\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.0009887695778161287\n",
            "      cur_lr: 0.012759958393871784\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 0.0\n",
            "      total_loss: 1.2914340175029793e-07\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 1.0940174632878552e-07\n",
            "    p_3:\n",
            "      cur_kl_coeff: 0.00016479492478538305\n",
            "      cur_lr: 0.010207965970039368\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -2.8560560583201777e-09\n",
            "      total_loss: -2.8560560583201777e-09\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 4.402475328735278e-10\n",
            "  num_steps_sampled: 5040\n",
            "  num_steps_trained: 5040\n",
            "iterations_since_restore: 28\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 85.15\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10287047390158684\n",
            "  mean_inference_ms: 9.53930465074253\n",
            "  mean_processing_ms: 5.373924238601564\n",
            "time_since_restore: 42.019267082214355\n",
            "time_this_iter_s: 1.0086824893951416\n",
            "time_total_s: 42.019267082214355\n",
            "timers:\n",
            "  learn_throughput: 336.426\n",
            "  learn_time_ms: 535.035\n",
            "  load_throughput: 41500.369\n",
            "  load_time_ms: 4.337\n",
            "  sample_throughput: 482.157\n",
            "  sample_time_ms: 373.322\n",
            "  update_time_ms: 17.374\n",
            "timestamp: 1596528184\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 5040\n",
            "training_iteration: 28\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:03:05,956\tWARNING ppo.py:166 -- No data for p_2, not updating kl\n",
            "2020-08-04 08:03:05,962\tWARNING ppo.py:166 -- No data for p_4, not updating kl\n",
            "2020-08-04 08:03:05,963\tWARNING ppo.py:166 -- No data for p_5, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 29 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-03-05\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 522\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_0:\n",
            "      cur_kl_coeff: 5.493163916980848e-05\n",
            "      cur_lr: 0.012759958393871784\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -9.313225746154785e-10\n",
            "      total_loss: 6.208817349140361e-10\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 1.6975367778115924e-09\n",
            "    p_1:\n",
            "      cur_kl_coeff: 0.0004394531133584678\n",
            "      cur_lr: 0.010207965970039368\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 3.750125543433569e-08\n",
            "      total_loss: 5.960464477539063e-08\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 2.4957733657515746e-08\n",
            "    p_3:\n",
            "      cur_kl_coeff: 8.239746239269152e-05\n",
            "      cur_lr: 0.008166372776031494\n",
            "      entropy: 1.6564308258359972e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 2.1730859334212482e-09\n",
            "      total_loss: 2.1730859334212482e-09\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 9.050798305265673e-11\n",
            "  num_steps_sampled: 5220\n",
            "  num_steps_trained: 5220\n",
            "iterations_since_restore: 29\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 87.05000000000001\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "  p_5: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10224123281879895\n",
            "  mean_inference_ms: 9.423758246765093\n",
            "  mean_processing_ms: 5.375914521675063\n",
            "time_since_restore: 43.01871418952942\n",
            "time_this_iter_s: 0.9994471073150635\n",
            "time_total_s: 43.01871418952942\n",
            "timers:\n",
            "  learn_throughput: 336.502\n",
            "  learn_time_ms: 534.915\n",
            "  load_throughput: 38181.717\n",
            "  load_time_ms: 4.714\n",
            "  sample_throughput: 477.34\n",
            "  sample_time_ms: 377.09\n",
            "  update_time_ms: 17.28\n",
            "timestamp: 1596528185\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 5220\n",
            "training_iteration: 29\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-04 08:03:07,114\tWARNING ppo.py:166 -- No data for p_0, not updating kl\n",
            "2020-08-04 08:03:07,124\tWARNING ppo.py:166 -- No data for p_4, not updating kl\n",
            "2020-08-04 08:03:07,130\tWARNING ppo.py:166 -- No data for p_5, not updating kl\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainer.train() result: <ray.rllib.agents.trainer_template.PPO object at 0x7fe2d6a6b550> -> 18 episodes\n",
            "training loop = 30 of 30\n",
            "callback_ok: true\n",
            "custom_metrics: {}\n",
            "date: 2020-08-04_08-03-07\n",
            "done: false\n",
            "episode_len_mean: 10.0\n",
            "episode_reward_max: 0.0\n",
            "episode_reward_mean: 0.0\n",
            "episode_reward_min: 0.0\n",
            "episodes_this_iter: 18\n",
            "episodes_total: 540\n",
            "experiment_id: 143681295c0a4ea89f1fc8c8dfcce63f\n",
            "hostname: 34fd3023fdc6\n",
            "info:\n",
            "  learner:\n",
            "    p_1:\n",
            "      cur_kl_coeff: 0.0002197265566792339\n",
            "      cur_lr: 0.012249560095369816\n",
            "      entropy: 1.6564308258359972e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 2.1523899107478428e-08\n",
            "      total_loss: 2.1523899107478428e-08\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 6.314014200370366e-09\n",
            "    p_2:\n",
            "      cur_kl_coeff: 0.0004943847889080644\n",
            "      cur_lr: 0.018374338746070862\n",
            "      entropy: 1.6564309646138753e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: -2.4835269396561444e-09\n",
            "      total_loss: 1.2417634032146907e-08\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 1.5345406723099586e-08\n",
            "    p_3:\n",
            "      cur_kl_coeff: 4.119873119634576e-05\n",
            "      cur_lr: 0.04287692904472351\n",
            "      entropy: 1.6564306870581191e-10\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 1.2980590393050205e-15\n",
            "      model: {}\n",
            "      policy_loss: 0.0\n",
            "      total_loss: 8.692343733684993e-09\n",
            "      vf_explained_var: -1.0\n",
            "      vf_loss: 9.556949898126277e-09\n",
            "  num_steps_sampled: 5400\n",
            "  num_steps_trained: 5400\n",
            "iterations_since_restore: 30\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 85.3\n",
            "  ram_util_percent: 50.3\n",
            "pid: 1279\n",
            "policy_reward_max:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "policy_reward_mean:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "policy_reward_min:\n",
            "  p_0: 0.0\n",
            "  p_1: 0.0\n",
            "  p_2: 0.0\n",
            "  p_3: 0.0\n",
            "  p_4: 0.0\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10165667777036712\n",
            "  mean_inference_ms: 9.287988100770328\n",
            "  mean_processing_ms: 5.381150862883759\n",
            "time_since_restore: 43.982290744781494\n",
            "time_this_iter_s: 0.9635765552520752\n",
            "time_total_s: 43.982290744781494\n",
            "timers:\n",
            "  learn_throughput: 338.231\n",
            "  learn_time_ms: 532.181\n",
            "  load_throughput: 38872.141\n",
            "  load_time_ms: 4.631\n",
            "  sample_throughput: 476.683\n",
            "  sample_time_ms: 377.61\n",
            "  update_time_ms: 17.369\n",
            "timestamp: 1596528187\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 5400\n",
            "training_iteration: 30\n",
            "\n",
            "checkpoint saved at /content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down/chkpt/checkpoint_30/checkpoint-30\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}