{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PBT_MARL_water_down.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WwD3_kI2HDbA","colab_type":"text"},"source":["#Setup Colab"]},{"cell_type":"code","metadata":{"id":"IyAKAl49kg7I","colab_type":"code","colab":{}},"source":["from google.colab import drive \n","\n","drive.mount('/content/gdrive')\n","%cd \"/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down/\"\n","!pwd\n","!ls -l\n","\n","# Install if you haven't done so.\n","!pip install tensorflow==2.2.0\n","!pip install ray[rllib]==0.8.5   \n","#!pip show tensorflow\n","#!pip show ray"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2BMjPt0fbNSf","colab_type":"text"},"source":["#Chkpt/restore & log path"]},{"cell_type":"code","metadata":{"id":"zQMyQcPpbIai","colab_type":"code","colab":{}},"source":["g_drive_path = \"/content/gdrive/My Drive/Colab Notebooks/PBT_MARL_watered_down/\"\n","\n","local_dir = g_drive_path + \"chkpt/\"\n","chkpt_freq = 10\n","chkpt = 150\n","restore_path = \"{}checkpoint_{}/checkpoint-{}\".format(local_dir, chkpt, chkpt)\n","is_restore = False\n","\n","log_dir = g_drive_path + \"ray_results/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I-GBqoxsHBZV","colab_type":"text"},"source":["#Imports"]},{"cell_type":"code","metadata":{"id":"p8DRdL7tgKBr","colab_type":"code","colab":{}},"source":["from collections import defaultdict\n","from typing import Dict\n","import random\n","import numpy as np\n","\n","from gym.spaces import Discrete\n","\n","import ray\n","from ray import tune\n","\n","from ray.tune.registry import register_env\n","from ray.rllib.models import ModelCatalog\n","\n","from ray.rllib.policy import Policy\n","\n","from ray.rllib.agents.ppo import ppo\n","from ray.rllib.agents.ppo.ppo import PPOTrainer\n","from ray.rllib.agents.ppo import appo\n","from ray.rllib.agents.ppo.appo import APPOTrainer\n","from ray.rllib.agents.ppo import ddppo\n","from ray.rllib.agents.ppo.ddppo import DDPPOTrainer\n","\n","from ray.rllib.env import BaseEnv\n","from ray.rllib.env.multi_agent_env import MultiAgentEnv\n","\n","from ray.rllib.policy.sample_batch import SampleBatch\n","from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n","from ray.rllib.agents.callbacks import DefaultCallbacks\n","\n","from ray.rllib.utils import try_import_tf\n","from ray.tune.logger import pretty_print\n","\n","tf = try_import_tf()\n","\n","from RockPaperScissorsEnv import RockPaperScissorsEnv\n","from Helper import Helper\n","from PBT_MARL import PBT_MARL"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bFRqbhqAunwM","colab_type":"text"},"source":["#Callbacks"]},{"cell_type":"code","metadata":{"id":"CBp3zwiEuqM7","colab_type":"code","colab":{}},"source":["\"\"\"#Callbacks\"\"\"\n","\n","class MyCallbacks(DefaultCallbacks):\n","    def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n","                         policies: Dict[str, Policy],\n","                         episode: MultiAgentEpisode, **kwargs):\n","        print(\"on_episode_start {}, _agent_to_policy {}\".format(episode.episode_id, episode._agent_to_policy))\n","        episode.hist_data[\"episode_id\"] = []\n","\n","    def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n","                        episode: MultiAgentEpisode, **kwargs):\n","          \"\"\"\n","          pole_angle = abs(episode.last_observation_for()[2])\n","          raw_angle = abs(episode.last_raw_obs_for()[2])\n","          assert pole_angle == raw_angle\n","          episode.user_data[\"pole_angles\"].append(pole_angle)\n","          \"\"\"\n","          pass\n","\n","    def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n","                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n","                       **kwargs):\n","        print(\"on_episode_end {}, episode.agent_rewards {}\".format(episode.episode_id, episode.agent_rewards))\n","\n","        player_policy = []\n","        score = []\n","        for k,v in episode.agent_rewards.items():\n","            player_policy.append(k)\n","            score.append(v)\n","\n","        pol_i_key = player_policy[0][1]\n","        pol_j_key = player_policy[1][1]\n","        _, str_i = pol_i_key.split(\"_\")\n","        _, str_j = pol_j_key.split(\"_\")\n","        agt_i_key = \"agt_\" + str_i\n","        agt_j_key = \"agt_\" + str_j\n","\n","        g_helper = ray.util.get_actor(\"g_helper\")     \n","        prev_rating_i = ray.get(g_helper.get_rating.remote(agt_i_key))\n","        prev_rating_j = ray.get(g_helper.get_rating.remote(agt_j_key))\n","        score_i = score[0]\n","        score_j = score[1]\n","        rating_i, rating_j = l_PBT_MARL.compute_rating(prev_rating_i, prev_rating_j, score_i, score_j)\n","        ray.get(g_helper.update_rating.remote(agt_i_key, agt_j_key, rating_i, rating_j, score_i, score_j))\n","        print(\"on_episode_end ray.get(g_helper.get_agt_store.remote())\", ray.get(g_helper.get_agt_store.remote()))\n","\n","    def on_sample_end(self, worker: RolloutWorker, samples: SampleBatch,\n","                      **kwargs):\n","        print(\"on_sample_end returned sample batch of size {}\".format(samples.count))\n","\n","    def on_train_result(self, trainer, result: dict, **kwargs):\n","        print(\"trainer.train() result: {} -> {} episodes\".format(trainer, result[\"episodes_this_iter\"]))\n","        # you can mutate the result dict to add new fields to return\n","        result[\"callback_ok\"] = True\n","        print(\"on_train_result result\", result)\n","\n","        l_PBT_MARL.PBT(trainer)     # perform PBT\n","\n","        g_helper = ray.util.get_actor(\"g_helper\")     \n","        ray.get(g_helper.set_pair.remote())     # set the lastest pair\n","        print(\"on_train_result g_helper.get_pair.remote()\", ray.get(g_helper.get_pair.remote()))\n","\n","    def on_postprocess_trajectory(\n","            self, worker: RolloutWorker, episode: MultiAgentEpisode,\n","            agent_id: str, policy_id: str, policies: Dict[str, Policy],\n","            postprocessed_batch: SampleBatch,\n","            original_batches: Dict[str, SampleBatch], **kwargs):\n","        print(\"postprocessed {}, {}, {}, {} steps\".format(episode, agent_id, policy_id, postprocessed_batch.count))\n","        \"\"\"\n","        if \"num_batches\" not in episode.custom_metrics:\n","            episode.custom_metrics[\"num_batches\"] = 0\n","        episode.custom_metrics[\"num_batches\"] += 1\n","        \"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DpcJGyAaBbc2","colab_type":"text"},"source":["#Policy"]},{"cell_type":"code","metadata":{"id":"iMZ20pVCzxUN","colab_type":"code","colab":{}},"source":["def init_policies(population_size, obs_space, act_space, use_lstm, hyperparameters_range):\n","    \"\"\"\n","    Sample hyper-parameter from the hyper-parameter distribution.\n","    \"\"\"\n","    policies = {}\n","    for i in range(population_size):\n","        pol_key = \"p_\" + str(i)\n","        lr = np.random.uniform(low=hyperparameters_range[\"lr\"][0], high=hyperparameters_range[\"lr\"][1], size=None)\n","        gamma = np.random.uniform(low=hyperparameters_range[\"gamma\"][0], high=hyperparameters_range[\"gamma\"][1], size=None)\n","        policies[pol_key] = (None, obs_space, act_space, {\"model\": {\"use_lstm\": use_lstm},\n","                                                          \"lr\": lr,\n","                                                          \"gamma\": gamma})\n","    return policies\n","\n","def train_policies(population_size):    \n","    train_policies = []\n","    for i in range(population_size):\n","        pol_key = \"p_\" + str(i)\n","        train_policies.append(pol_key)\n","\n","    return policies\n","\n","def select_policy(agent_id):\n","    _, i = agent_id.split(\"_\")\n","    policy = \"p_\" + str(i)\n","    print(\"select_policy {} {}\".format(agent_id , policy))\n","    return policy     "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oAEySBSfBS5u","colab_type":"text"},"source":["#Variables"]},{"cell_type":"code","metadata":{"id":"trdlnMoHwbfT","colab_type":"code","outputId":"593538ec-bb1a-4da5-a0f8-d36560c07724","executionInfo":{"status":"ok","timestamp":1591862238670,"user_tz":-480,"elapsed":79217,"user":{"displayName":"H C","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUv_2vYj1_WlrNYmPQibhDRVGFq8_ZtRHRBbZ_=s64","userId":"02161151882970450665"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["population_size = 8\n","K = 0.1     \n","T_select = 0.77 #0.47\n","binomial_n = 1\n","inherit_prob = 0.5\n","perturb_prob = 0.1\n","perturb_val = [0.8, 1.2]\n","hyperparameters_range = {\"lr\": [0.00001, 0.01], \n","                         \"gamma\": [0.9, 0.999]}\n","\n","register_env(\"RockPaperScissorsEnv\", lambda _: RockPaperScissorsEnv(_, population_size))     # register RockPaperScissorsEnv with RLlib     \n","# get obs & act spaces from dummy CDA env\n","dummy_env = RockPaperScissorsEnv(_, population_size=0)\n","obs_space = dummy_env.observation_space\n","act_space = dummy_env.action_space\n","\n","use_lstm=False\n","policies = init_policies(population_size, obs_space, act_space, use_lstm, hyperparameters_range)\n","train_policies = train_policies(population_size)\n","\n","l_PBT_MARL = PBT_MARL(population_size, \n","                      K, T_select, \n","                      binomial_n, inherit_prob,\n","                      perturb_prob, perturb_val)\n","\n","ray.shutdown()\n","ray.init(ignore_reinit_error=True, log_to_driver=True, webui_host='127.0.0.1', num_cpus=2, num_gpus=1)      #start ray\n","print(\"ray.nodes()\", ray.nodes())\n","\n","g_helper = Helper.options(name=\"g_helper\").remote(population_size, policies)      # this object runs on a different ray actor process\n","ray.get(g_helper.set_pair.remote())\n","\n","num_iters = 15     # num of main training loop"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-06-11 07:57:14,116\tINFO resource_spec.py:212 -- Starting Ray with 7.18 GiB memory available for workers and up to 3.59 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n","2020-06-11 07:57:14,580\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32m127.0.0.1:8265\u001b[39m\u001b[22m\n"],"name":"stderr"},{"output_type":"stream","text":["ray.nodes() [{'NodeID': 'e41d10539bf08b56039e2431a5f75a431d43206a', 'Alive': True, 'NodeManagerAddress': '172.28.0.2', 'NodeManagerHostname': '571b538b671a', 'NodeManagerPort': 53651, 'ObjectManagerPort': 43717, 'ObjectStoreSocketName': '/tmp/ray/session_2020-06-11_07-57-14_115178_125/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2020-06-11_07-57-14_115178_125/sockets/raylet', 'Resources': {'node:172.28.0.2': 1.0, 'GPU': 1.0, 'object_store_memory': 50.0, 'memory': 147.0, 'CPU': 2.0}, 'alive': True}]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xyucHLoqBe5G","colab_type":"text"},"source":["#Config"]},{"cell_type":"code","metadata":{"id":"KNWNnavQt9y0","colab_type":"code","colab":{}},"source":["def get_config():\n","    config = ddppo.DEFAULT_CONFIG.copy()\n","\n","    config[\"env\"] = RockPaperScissorsEnv\n","    config[\"multiagent\"] = {\"policies_to_train\": train_policies,\n","                            \"policies\": policies,\n","                            \"policy_mapping_fn\": select_policy}        \n","    config[\"num_cpus_per_worker\"] = 0.25                                \n","    config[\"num_gpus_per_worker\"] = 0.125\n","    config[\"num_workers\"] = 2      \n","    config[\"num_envs_per_worker\"] = 3\n","    config[\"rollout_fragment_length\"] = 30                  \n","    config[\"train_batch_size\"] = -1     # must be -1 for DDPPO trainer                                                            \n","    config[\"sgd_minibatch_size\"] = 10                       \n","    config[\"num_sgd_iter\"] = 3      # number of epochs to execute per train batch.\n","    config[\"callbacks\"] = MyCallbacks\n","    config[\"log_level\"] = \"WARN\"      # WARN/INFO/DEBUG \n","    config[\"output\"] = log_dir\n","\n","    return config"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vb_4cEGdBlqf","colab_type":"text"},"source":["#Go train"]},{"cell_type":"code","metadata":{"id":"kUB40TYSuDAn","colab_type":"code","outputId":"7e4d9918-bcb0-4d24-9c83-92abd4df3181","executionInfo":{"status":"ok","timestamp":1591862292128,"user_tz":-480,"elapsed":132662,"user":{"displayName":"H C","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUv_2vYj1_WlrNYmPQibhDRVGFq8_ZtRHRBbZ_=s64","userId":"02161151882970450665"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1AqwSkhwdxFOqYMTrgfR2dtBPvcKGAanf"}},"source":["def go_train(config):     \n","    trainer = ddppo.DDPPOTrainer(config=config, env=\"RockPaperScissorsEnv\")         \n","\n","    if is_restore == True:\n","        trainer.restore(restore_path) \n","    \n","    result = None\n","    for i in range(num_iters):\n","        result = trainer.train()       \n","        print(\"training loop = {} of {}\".format(i + 1, num_iters))            \n","        print(pretty_print(result))     # includes result[\"custom_metrics\"]\n","    \n","        if i % chkpt_freq == 0:\n","            checkpoint = trainer.save(local_dir)\n","            print(\"checkpoint saved at\", checkpoint)\n","    \n","    checkpoint = trainer.save(local_dir)\n","    print(\"checkpoint saved at\", checkpoint)\n","    \n","\n","# run everything\n","go_train(get_config())    \n","\n","ray.shutdown()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}